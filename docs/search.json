[
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#what-was-easier-to-find",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#what-was-easier-to-find",
    "title": "Good and Bad Visualizations",
    "section": "What was easier to find?",
    "text": "What was easier to find?\n\ngood üëè visualizations\nbad üò° visualizations"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#microbiome-stacked-bar-plots-at-it-again",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#microbiome-stacked-bar-plots-at-it-again",
    "title": "Good and Bad Visualizations",
    "section": "Microbiome stacked bar plots at it again",
    "text": "Microbiome stacked bar plots at it again\n\nFigure source: Niu, B. et al.¬†(2017). Simplified and representative bacterial community of maize roots. PNAS, 114 (12). https://doi.org/10.1073/pnas.1616148114"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#bad-covid-is-bad",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#bad-covid-is-bad",
    "title": "Good and Bad Visualizations",
    "section": "Bad COVID is bad",
    "text": "Bad COVID is bad"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#barbarplots",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#barbarplots",
    "title": "Good and Bad Visualizations",
    "section": "#barbarplots",
    "text": "#barbarplots"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#ggplot-can-still-be-bad",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#ggplot-can-still-be-bad",
    "title": "Good and Bad Visualizations",
    "section": "ggplot can still be bad",
    "text": "ggplot can still be bad"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#the-winner-of-bad-visualizations",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#the-winner-of-bad-visualizations",
    "title": "Good and Bad Visualizations",
    "section": "The winner of bad visualizations",
    "text": "The winner of bad visualizations\nüèÜüèÖ Shiva Jahanbakhshi"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#cell-expression",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#cell-expression",
    "title": "Good and Bad Visualizations",
    "section": "Cell expression",
    "text": "Cell expression\n Werner, Y., Mass, E., Ashok Kumar, P., Ulas, T., H√§ndler, K., Horne, A., ‚Ä¶ & Stumm, R. (2020). Cxcr4 distinguishes HSC-derived monocytes from microglia and reveals monocyte immune responses to experimental stroke. Nature neuroscience, 23(3), 351-362."
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#sports",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#sports",
    "title": "Good and Bad Visualizations",
    "section": "Sports",
    "text": "Sports"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#weight-loss-rct",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#weight-loss-rct",
    "title": "Good and Bad Visualizations",
    "section": "Weight loss RCT",
    "text": "Weight loss RCT\n\nMok J, Adeleke MO, Brown A, et al.¬†Safety and Efficacy of Liraglutide, 3.0 mg, Once Daily vs Placebo in Patients With Poor Weight Loss Following Metabolic Surgery: The BARI-OPTIMISE Randomized Clinical Trial. JAMA Surg. Published online July 26, 2023. doi:10.1001/jamasurg.2023.2930"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#world-population",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#world-population",
    "title": "Good and Bad Visualizations",
    "section": "World population",
    "text": "World population\n\nhttps://www.visualcapitalist.com/visualized-the-worlds-population-at-8-billion/"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#volcano-plot",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#volcano-plot",
    "title": "Good and Bad Visualizations",
    "section": "Volcano plot",
    "text": "Volcano plot"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#land-doesnt-vote",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#land-doesnt-vote",
    "title": "Good and Bad Visualizations",
    "section": "Land doesn‚Äôt vote",
    "text": "Land doesn‚Äôt vote"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#the-winner-of-good-visualizations",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#the-winner-of-good-visualizations",
    "title": "Good and Bad Visualizations",
    "section": "The winner of good visualizations",
    "text": "The winner of good visualizations\nJeff Eiseman üèÜüèÖ\n\nhttps://www.nytimes.com/2022/04/28/learning/whats-going-on-in-this-graph-may-4-2022.html"
  },
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#the-winner-of-good-visualizations-1",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#the-winner-of-good-visualizations-1",
    "title": "Good and Bad Visualizations",
    "section": "The winner of good visualizations",
    "text": "The winner of good visualizations\nCarl Engstrom üèÜüèÖ"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd_recitation.html",
    "href": "modules/module2/03_rmarkdown/03_Rmd_recitation.html",
    "title": "R Markdown for Reproducible Research Recitation",
    "section": "",
    "text": "Today you will be playing around with modifying the following within R Markdown:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd_recitation.html#r-markdown-recitation",
    "href": "modules/module2/03_rmarkdown/03_Rmd_recitation.html#r-markdown-recitation",
    "title": "R Markdown for Reproducible Research Recitation",
    "section": "",
    "text": "Today you will be playing around with modifying the following within R Markdown:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "We are going to practice using ggplot today, focusing on the data, aesthetic, and geom layers. We are going to use data from the TidyTuesday project. For this recitation, we are going to use the Giant Pumpkins data which is collected from the Great Pumpkin Commonwealth.\nAt the end of of this module you will create of of this descriptive plots\n\n\n\n\n\n\n\n\n\n\nQuestion: How can we replicate this plot?\n\n\n\nWork with real world data\n\nImport data from github\nModify variables types\nSelect observations with certain values\nWrangle some more\nPractice plotting\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\n\n\nWhen you open the github page you will see a file called pumpkins.csv. You also are introduced about the details of the data (i.e., variables, variable types, descriptions), as well as how to import the it.\nFirst thing first, we are going to import the data by reading the csv file with the Github link provided. You can also read the data in by downloading it manually, saving it, and then loading it.\n# load libraries\nlibrary(tidyverse)\n\n# Import giant pumpkins data\npumpkins_raw &lt;- readr::read_csv('WHAT-GOES-HERE??')\nOnce we have imported our data, how can you check it out?\n\nglimpse(pumpkins_raw)\n\nRows: 28,065\nColumns: 14\n$ id                &lt;chr&gt; \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2‚Ä¶\n$ place             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"‚Ä¶\n$ weight_lbs        &lt;chr&gt; \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1‚Ä¶\n$ grower_name       &lt;chr&gt; \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen‚Ä¶\n$ city              &lt;chr&gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo‚Ä¶\n$ state_prov        &lt;chr&gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc‚Ä¶\n$ country           &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"‚Ä¶\n$ gpc_site          &lt;chr&gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum‚Ä¶\n$ seed_mother       &lt;chr&gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar‚Ä¶\n$ pollinator_father &lt;chr&gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open‚Ä¶\n$ ott               &lt;chr&gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", ‚Ä¶\n$ est_weight        &lt;chr&gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141‚Ä¶\n$ pct_chart         &lt;chr&gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0‚Ä¶\n$ variety           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nDo some of these variables contain more than one piece of information?\n\nWhat is embedded within the variable id?\nWhat type of info does id contain?\nWhat types of variables are place and weight_lbs? Are there any limitations to plotting these variable types?"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#goals-of-this-recitation",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#goals-of-this-recitation",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "Work with real world data\n\nImport data from github\nModify variables types\nSelect observations with certain values\nWrangle some more\nPractice plotting\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\n\n\nWhen you open the github page you will see a file called pumpkins.csv. You also are introduced about the details of the data (i.e., variables, variable types, descriptions), as well as how to import the it.\nFirst thing first, we are going to import the data by reading the csv file with the Github link provided. You can also read the data in by downloading it manually, saving it, and then loading it.\n# load libraries\nlibrary(tidyverse)\n\n# Import giant pumpkins data\npumpkins_raw &lt;- readr::read_csv('WHAT-GOES-HERE??')\nOnce we have imported our data, how can you check it out?\n\nglimpse(pumpkins_raw)\n\nRows: 28,065\nColumns: 14\n$ id                &lt;chr&gt; \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2‚Ä¶\n$ place             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"‚Ä¶\n$ weight_lbs        &lt;chr&gt; \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1‚Ä¶\n$ grower_name       &lt;chr&gt; \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen‚Ä¶\n$ city              &lt;chr&gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo‚Ä¶\n$ state_prov        &lt;chr&gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc‚Ä¶\n$ country           &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"‚Ä¶\n$ gpc_site          &lt;chr&gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum‚Ä¶\n$ seed_mother       &lt;chr&gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar‚Ä¶\n$ pollinator_father &lt;chr&gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open‚Ä¶\n$ ott               &lt;chr&gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", ‚Ä¶\n$ est_weight        &lt;chr&gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141‚Ä¶\n$ pct_chart         &lt;chr&gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0‚Ä¶\n$ variety           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nDo some of these variables contain more than one piece of information?\n\nWhat is embedded within the variable id?\nWhat type of info does id contain?\nWhat types of variables are place and weight_lbs? Are there any limitations to plotting these variable types?"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#turn-one-character-column-into-two",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#turn-one-character-column-into-two",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Turn one character column into two ‚úÇÔ∏è",
    "text": "Turn one character column into two ‚úÇÔ∏è\nFrom both looking at the data, and reading about the variable id on the documentation page, you see that it contains two type of observations. To use them separately, we need to separate this column into two columns such like year and type.\nTry doing this with the function separate() from the tidyr package to do this. And you will obtain the following data\npumpkins_raw %&gt;%\n   separate(WHAT-GOES-HERE)"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#select-observations-by-their-values",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#select-observations-by-their-values",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Select observations by their values üéÉ",
    "text": "Select observations by their values üéÉ\nNow that you separated the year and crop type, keep only the data for Giant Pumpkins. Hint, you can use the filter() function from the dplyr package.\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\npumpkins_raw %&gt;%\n   filter(...predicate/condition...) \nNow that you are familiar with the filter(), retain only the observations that were the winners or those in the first place.\npumpkins_raw %&gt;%\n   filter(...predicate/condition...) %&gt;%\n   filter(...predicate/condition...)"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#remove-pesky-strings",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#remove-pesky-strings",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Remove pesky strings üòë",
    "text": "Remove pesky strings üòë\nIf we were to try and plot our data as it is now we would not get our desired outcome. But try it anyway.\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  code-to-plot\nWhat is weird about this y-axis?\n\n\n\n\n\nIf you take a look at the variables of the weight_lbs column, it contains commas as thousand separator. However, R does not recognize this as a number (and instead views it as a character) so and it has to be removed prior changing the column type.\nFor this purpose, we are going to remove this annoying character. You can use str_remove() function from the base and stringr package respectively. Here is an example of how both functions work.\n\nwrong_number &lt;- \"700,057.58\"\nwrong_number\n\n[1] \"700,057.58\"\n\n\nUsing str_remove\n\nstringr::str_remove(string = wrong_number, pattern = \",\")\n\n[1] \"700057.58\"\n\n\nRemember, we don‚Äôt want to just remove the thousands place comma in one number, we want to edit the dataset to remove the comma.\nIn this case, you can embed str_remove() within the mutate() function, which can create new variables or modify existing ones. In our case, we want to modify the weight_lbs variable.\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) \nCommas, gone! üëèüëèüëè"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#convert-character-to-numeric",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#convert-character-to-numeric",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Convert character to numeric üî¢",
    "text": "Convert character to numeric üî¢\nNow the comma is gone, you can simply change the variable weight_lbs from a character to numeric, so it can be plotted like a number., to change the column type, we are going to use the as.numeric() function. Here‚Äôs some example about how to use as.numeric().\n\nright_number_chr &lt;- stringr::str_remove(string = wrong_number, pattern = \",\")\n\nright_number_number &lt;- as.numeric(right_number_chr)\nclass(right_number_number)\n\n[1] \"numeric\"\n\n\nLet‚Äôs add this to our growing pipe.\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) %&gt;%\n  mutate(variable = as.numeric(arguments-here))"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#where-are-the-lines",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#where-are-the-lines",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Where are the lines?",
    "text": "Where are the lines?\nWhy do you think the lines aren‚Äôt showing up? Hint - look at what variable type year is.\nHow can you fix this? Hint, you can change year to either numeric or a date. Here are some packages that allow you to deal with dates specifically.\n\ndateFixR\nlubridate\n\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) %&gt;%\n  mutate(variable = as.numeric(arguments-here)) %&gt;%\n  mutate(do-something-with-your-date) %&gt;%\n  code-to-plot"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#playing-around",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#playing-around",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Playing around",
    "text": "Playing around\nTry using different geoms besides geom_point() and geom_line(). Which might make sense in this situation?\nCan you color all the lines blue?\nCan you color the data based on year?\nCan you color and change shape based on country?\nCan you make a plot showing the distribution of weights of all giant pumpkins entered in 2021?\nCan you make a boxplot showing the distribution of weights of all giant pumpkins across all years? Also can you add all the datapoints on top of the boxplot? Is this a good idea? Might there be a better geom to use than a boxplot?"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html",
    "title": "ggplot 101 (and üçÖ)",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#what-is-the-tidyverse",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#what-is-the-tidyverse",
    "title": "ggplot 101 (and üçÖ)",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe package ggplot2 is a part of a larger collection of packages called ‚Äúthe tidyverse‚Äù that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier.\nWe can install just ggplot2 or install all of the packages in the core tidyverse (which is what I‚Äôd recommend since we will use the others too), which include:\n\ndplyr: for data manipulation\nggplot2: a ‚Äúgrammar of graphics‚Äù for creating beautiful plots\nreadr: for reading in rectangular data (i.e., Excel-style formatting)\ntibble: using tibbles as modern/better dataframes\nstringr: handling strings (i.e., text or stuff in quotes)\nforcats: for handling categorical variables (i.e., factors) (meow!)\ntidyr: to make ‚Äútidy data‚Äù\npurrr: for enhancing functional programming (also meow!)\n\nWe will be using many of these other packages in this course, but will talk about them as we go. There are more tidyverse packages outside of these core eight, and we will talk about some of them another time.\n\ntl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are required, but I think you‚Äôll find many tidyverse approaches easier and more intuitive than using base R.\n\nYou can find here some examples of comparing tidyverse and base R syntax."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#installing-ggplot-tidyverse",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#installing-ggplot-tidyverse",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Installing ggplot & tidyverse",
    "text": "Installing ggplot & tidyverse\nTo install packages in R that are on the Comprehensive R Archive Network (CRAN), you can use the function install.packages().\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\nWe only need to install packages once. But, every time we want to use them, we need to ‚Äúload‚Äù them, and can do this using the function library().\n\ntl:dr install.packages() once, library() every time."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#data",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#data",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Data",
    "text": "Data\nThe first argument passed to your plot is the data. How did I know that? It‚Äôs in the documentation.\n\n?ggplot()\n\nThe simplest ggplot code you can write, just using the ggplot() function and indicating the data we want to use. Because data is the default first argument, you can actually omit the data = part of this code and it will work just the same.\n\nggplot(data = garden_harvest)\n\n\n\n\nWhy do we not see a plot? Well we haven‚Äôt told R what to plot! We are getting the first ‚Äúbase‚Äù layer of the plot.\nYou can also pipe |&gt; or %&gt;%, the data to the ggplot function. When reading code, you can interpret the pipe as ‚Äúand then.‚Äù Here, take the garden_harvest_tomato data, and then, run ggplot(). Writing code in this way is my preference so I tend to code like this. We talked in more detail about the pipe last week, so you can go back there and read more if you like.\n\ngarden_harvest_tomato %&gt;% \n  ggplot()\n\n\n\n\nStill nothing. Well that‚Äôs what we would expect."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#aesthetic-mappings-aes",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#aesthetic-mappings-aes",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Aesthetic mappings aes()",
    "text": "Aesthetic mappings aes()\nNow that we‚Äôve indicated our data, we can add aesthetics mapping so we can work towards actually see a plot. We want to make a line plot where on the x-axis we have the date (date), and on the y-axis we have how much tomato was harvested (weight).\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight))\n\n\n\n\nSo we have progressed from a blank plot, but we still do not have a plot by basically anyone‚Äôs defintion. Why not?\nEven though we have indicated to R our data and aesthetic mappings, we have not indicated what precisely to do with our data. We have said what we want on x and y (and now we can see those labelled appearing) but we have not indicated what type of plot we want. And, we can do that in the next step, by adding a geom_."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#geoms-geom_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#geoms-geom_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Geoms geom_",
    "text": "Geoms geom_\nNow let‚Äôs indicate what type of plot we want. In this example, we are going to make a line plot, and to do that we will use geom_line()\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line()\n\n\n\n\nWe have a plot! It‚Äôs not a really good plot, but its a plot and we can work from here.\nYou can see that what R has done is take each date, and plotted the total weight of tomatoes harvested on that day. What we can see from this part is that in the beginning of the season, there is little tomato production (this is not surprising to anyone who knows about horticulture or has grown tomatoes before), and production increases as the season progresses. We don‚Äôt see any harvest after mid-October which makes sense because Dr.¬†Lendway lives in Minnesota and probably there was a frost that killed the plants (hence no more üçÖ).\nA note about aesthetic mappings now that we have introduced geoms -aes() can go in two places:\n\nin the ggplot() call, and this means they will inherit for every layer of the plot\nin a specific geom_, and those aesthetics will only be for that specific geom.\n\nSo we can make the same plot we saw above by mapping aesthetics within geom_line().\n\ngarden_harvest_tomato %&gt;%\n  ggplot() +\n  geom_line(aes(x = date, y = weight))\n\n\n\n\nLet‚Äôs say we wanted to see how the harvest of different varieties looks over the summer? We can take the variable variety and map it to the aesthetic color.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line()\n\n\n\n\nThis is till not a beautiful plot, but you are able to see now how you can map a variable of the data (here, variety) to an aesthetic (color).\nAnother important thing to notice here is that now the data is grouped by variety. We are seeing 12 lines instead of 1. This happens automatically under the hood. This ‚Äògrouping‚Äô will be maintained across additional geoms because it is in the global aesthetic mappings for the plot.\nWe can also add more than one geom. Let‚Äôs try adding geom_point() so we can better see exactly which times were sampled in this dataset.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point()\n\n\n\n\nTo more fully make the point about global vs aesthetic mappings, let‚Äôs look at an example.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(color = variety)) +\n  geom_point()\n\n\n\n\nHere, we can see that how the line layer is being grouped by variety, while the points are not. This is because the aesthetic mappings for one geom don‚Äôt inherit to the next one. If we want to also color points by variety, we need to either 1) set this as the global aesthetic mapping or 2) also set aes(color = variety) in geom_point() too.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(color = variety)) +\n  geom_point(aes(color = variety))\n\n\n\n\n\nMapping vs.¬†‚Äòsetting‚Äô\nIf you want to map a variable to an aesthetic, it MUST be within the aes() statement. If you just want to change the color to ‚Äúblue‚Äù for example, it should be outside the aes() statement. Look at the difference.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(color = \"blue\")\n\n\n\n\nLook what happens if we put color = \"blue\" inside the aes() statement.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = \"blue\")) +\n  geom_line()\n\n\n\n\n\ntl:dr if mapping a variable to an aesthetic, inside aes(), if not, then outside.\n\n\n\nMore about aesthetics\nIt‚Äôs hard to talk about how to map to aesthetics before you add a geom, which is why this content is in this section.\nSo far we have talked about mapping aesthetics to x, y, and color. Below is a list of other aesthetics you can map to:\n\ncolor (or colour if that suits you better) and fill\nIn general color controls the outside/line, and fill controls the inside of a shape. Some geoms will work only with color or fill, and work with both. There are a millions ways to control the color, including by using the R color names (don‚Äôt forget to put them in quotes), or hex codes (e.g., ‚ÄúFF0000‚Äù for red). There are a ton of different color palettes in R like color brewer and I‚Äôd recommend you to think about using colors that are color blind friendly like viridis. Picking a color palette allows continuity across your presentation/manuscript, and can help in the interpretation of your data (e.g., having a divergent scale where darker means more abundant, or pairing colors like light and medium blue to indicate which samples have some kind of relationship).\n\n\nlinetype\nYou can change the style of a line based on a variable\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(linetype = variety))\n\n\n\n\nWow this is a disaster but you can see the point about mapping variables to linetype.\nHere are some different linetypes you can select from:\n\n\n\n\n\nFigure from ggplot documentation\n\n\n\n\n\n\nsize\nYou can also map variables to size. This could be useful if you wanted to say make your points bigger when a fold change is bigger, or bigger when a value is more significant. Below is an example of mapping weight to size in our example dataset.\n\ngarden_harvest_tomato %&gt;%\n  filter(variety %in% c(\"Mortgage Lifter\", \"Brandywine\")) %&gt;%\n  ggplot(aes(x = variety, y = date, size = weight)) +\n  geom_point()\n\n\n\n\n\n\nshape\nYou can also map variables to shape. This could be useful if you want points of one treatment on a scatterplot to be circles, a second treatment triangles, etc. You can combine mapping to shape and color together which is good for those who are concerned about black and white printability, but also easy differentiability when viewed on a computer.\n\n\n\n\n\nFigure from sthda\n\n\n\n\nShapes 0-20 accept only a color aesthetics. Shapes 21-25 accept both a color and fill aesthetic, where color controls the color of the outside of the shape, and fill controls the color of the inside of the shape. I basically always use shapes 21-25 (actually I almost always just use 21).\n\n\nalpha\nSetting alpha allows you to map a variable to the transparency of a part of your plot. So for example, if you had a correlation plot, you could make a strong correlation really dark, while a weak relationship lighter. Alpha can range between 0 and 1, where 0 is totally transparent, and 1 is completely opaque.\nThe next sections we will go over in more detail next week but I want to introduce the idea of these additional layers very briefly."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#scales-scale_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#scales-scale_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Scales scale_",
    "text": "Scales scale_\nUsing scales allows you to control how the data are linked to the visual properties of your plot.\nScales allow you to pick colors, shapes, alphas, lines, transformations (e.g.¬†scaling your axes to a log scale), and others. You can also use scales to set the limits of your plots."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#facets-facet_wrap-and-facet_grid",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#facets-facet_wrap-and-facet_grid",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Facets facet_wrap() and facet_grid()",
    "text": "Facets facet_wrap() and facet_grid()\nFaceting allows you to look at your plots using small multiples, to compare plots that might be otherwise crowded or hard to interpret.\nFaceting can be done using facet_wrap() or facet_grid()."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#coordinates-coord_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#coordinates-coord_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Coordinates coord_",
    "text": "Coordinates coord_\nOften the coordinate system used for your plot will be a simple Cartesian system using x and y. But sometimes, like for making maps or other specialized plots, you will want to change how x and y map to your coordinate system."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#labels-labs",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#labels-labs",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Labels labs()",
    "text": "Labels labs()\nHaving good labels helps your reader (and you, when you come back to the plot in the future) understand what its all about.\nIn the labs() function, you can indicate:\n\nx for the x-axis label\ny for the y-axis label\ntitle for a title\nsubtitle for a subtitle underneath your title\ncaption for a caption\n\nIn theme() you can change characteristics of these labels like their size, fonts, justfication, etc."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#themes-theme-and-theme_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#themes-theme-and-theme_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Themes theme() and theme_",
    "text": "Themes theme() and theme_\nThemes will control all the non-data parts of your plot. There are some pre-set ‚Äúcomplete‚Äù themes that you can recognize as they‚Äôll be called theme_XXX(), and you can adjust any theme parameters by setting parameters within theme(). There are probably 50 parameters you can set within theme() and they include text size, axis label orientation, the presence of a legend, and many others."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Introduction",
    "text": "Introduction\nWe will will build upon our last lesson on ggplot101 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, and geoms. Today we will focus on some of the other more commonly adjusted layers:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data.\n\nlibrary(tidyverse)\nlibrary(gardenR)\n\nAnd let‚Äôs remember whats in garden_harvest.\n\nglimpse(garden_harvest)\n\nRows: 781\nColumns: 5\n$ vegetable &lt;chr&gt; \"lettuce\", \"radish\", \"lettuce\", \"lettuce\", \"radish\", \"lettuc‚Ä¶\n$ variety   &lt;chr&gt; \"reseed\", \"Garden Party Mix\", \"reseed\", \"reseed\", \"Garden Pa‚Ä¶\n$ date      &lt;date&gt; 2020-06-06, 2020-06-06, 2020-06-08, 2020-06-09, 2020-06-11,‚Ä¶\n$ weight    &lt;dbl&gt; 20, 36, 15, 10, 67, 12, 9, 8, 53, 19, 14, 10, 48, 58, 8, 121‚Ä¶\n$ units     &lt;chr&gt; \"grams\", \"grams\", \"grams\", \"grams\", \"grams\", \"grams\", \"grams‚Ä¶"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#facets",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#facets",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Facets",
    "text": "Facets\nFaceting allows to create small multiples of plots, enabling the easy comparison across the entirety of your data. A benefit of plots like this is they are all structured the same way, so once you understand one, you can begin to look at trends across groups/treatments/conditions simply and easily.\nHere is a more infographic example of using small multiples.\n\n\n\n\n\nFigure from Five Thirty Eight\n\n\n\n\nSo we can easily see that states with more of a maroon color have a lower than average life expectancy, while those that are higher than average are orange. We also can see easily where each state is on the map, so we can begin to understand how geography is related to life expectancy. We can also see which states have gotten better (i.e.¬†their people live longer) with time, and those that haven‚Äôt. And this is all with a quick glance!\nIf we look back to the plot we were using as our example last week, can see how we have a plot faceted by tomato variety.\nFirst lets select only the data for tomatoes.\n\n# filter data to include only tomatoes \n# filter() is a useful function from dplyr (part of tidyverse)\n# it allows us to select observations based on their values\ngarden_harvest_tomato &lt;- garden_harvest %&gt;%\n  filter(vegetable == \"tomatoes\")\n\nLet‚Äôs remember what our base plot is currently looking like.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) \n\n\n\n\nSee how crowded this is? I think faceting might help us better see our data by variety.\nThere are two functions that allow you to facet:\n\nfacet_wrap: allows to lay out your facets in a wrapped type. You can use facet_wrap if you have 1 variable you‚Äôd like to facet on.\nfacet_grid: allows you to lay out your facets in a grid. You can use facet_grid if you have 1 or 2 variables you‚Äôd like to facet on.\n\nThere are a few different sets of syntax that work for faceting, but I think this is the most intuitive.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety))\n\n\n\n\nWe will get a very reasonably different looking plot with facet_grid with the default settings.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_grid(vars(variety))\n\n\n\n\nNote because you have provided only one variable, ggplot has put that facet in one row.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_grid(cols = vars(variety))\n\n\n\n\nWe can make the faceting go by column, but this is also looks bad in this case\nHowever, you might be thinking now that if you have two variables, and you want to facet by the combination of them, you could do that with facet_grid. Here is an example with the mpg dataset from the tidyverse (since there isn‚Äôt really good data to demonstrate this from garden_harevst).\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy)) + # city and highway gas mileage\n  geom_point() +\n  facet_grid(cols = vars(class), # category of car\n             rows = vars(drv)) # type of drive train, 4 wheel, front, rear\n\n\n\n\nThe default in both facet_wrap and facet_grid are for the x and y-axis to be fixed and constant among all the plots. This is often what you want to take advance of the comparisons between small multiples, but this is something you can change if you want. You can adjust the scales within facets to:\n\nscales = \"fixed\": both the x- and y-axes are fixed for all plots to be the same (this is the default)\nscales = \"free\": both the x- and y-axes are set per plot\nscales = \"free_x\": the x-axis scales are free between plots\nscales = \"free_y\": the y-axis scales are free between plots\n\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety), scales = \"free\")\n\n\n\n\nDo note how this affects how easy it is to compare among the facets now. Also note that in this case, since we have all the same x-axis labels between the plots, when we set scales = \"free\" it really only changes the y, making it functionally equivalent to scales = \"free_y\". This will not hold true in other situations."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#scales",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#scales",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Scales",
    "text": "Scales\nUsing scales allows you to control how the data are linked to the visual properties of your plot. Some books will include labels as a part of scales but I‚Äôm going to cover them separately.\nScales allow you to pick colors, shapes, alphas, lines, transformations (e.g.¬†scaling your axes to a log scale), and others. You can also use scales to set the limits of your plots.\nScales functions start with scale_.\nHere are some common things you might do with the scale_ functions.\n\nPosition scales\nYou can set position scales for dates/times (like we have here), x and y data, binned data, continuous data, and for discrete data.\nHere is one example for date/time data, which is what we have here. The date-time POSIX standards are listed here.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_x_date(date_labels = \"%m/%y\")\n\n\n\n\nHere is another example (which isn‚Äôt very good) about how you can also use scales to log transform your axes. Remember you are not actually transforming your data, you are just transforming the axis labels.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_y_log10()\n\n\n\n\n\n\nColor scales\nYou can set color scales for continuous and binned colour data, sequential, diverging and qualitative data using ColorBrewer, and perceptually uniform scales using viridis from viridisLite\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_color_brewer(palette = \"Set3\")\n\n\n\n\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_color_viridis_d()\n\n\n\n\nYou can play around with scales to see all you can do with it."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#labels",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#labels",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Labels",
    "text": "Labels\nHaving good labels helps your reader (and you, when you come back to the plot in the future) understand what its all about.\nIn the labs() function, you can indicate:\n\nx for the x-axis label\ny for the y-axis label\ntitle for a title\nsubtitle for a subtitle underneath your title\ncaption for a caption\n\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) + \n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\",\n       alt = \"A plot showing 12 varieties of tomatoes and how much of each of them Dr. Lisa Lendway harvested in her home garden in 2022. The biggest producers were amish paste and better boy, which had earlier season peaks, and mortgage lifter, old german, and volunteer plants were more productive towards the end of the season.\")\n\n\n\n\nYou can also use get_alt_text() to pull the alt-text for an image. This will come back with an empty string if there is no alt-text provided.\nIn theme() you can change characteristics of these labels like their size, fonts, justification, etc."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#themes",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#themes",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Themes",
    "text": "Themes\nThemes will control all the non-data parts of your plot. There are some pre-set ‚Äúcomplete‚Äù themes that you can recognize as they‚Äôll be called theme_*(), and you can adjust any theme parameters by setting parameters within theme(). There are probably 50 parameters you can set within theme() and they include text size, axis label orientation, the presence of a legend, and many others.\n\nComplete themes from ggplot\nThere are some pre-set complete themes that control the look of the non-data displays. Below are some examples. theme_grey() is the default ggplot2 theme.\n\ntheme_minimal()\nThis is the one I use the most.\n\nbase_plot &lt;- garden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) + \n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\nbase_plot + theme_minimal()\n\n\n\n\n\n\ntheme_classic()\nThis is another nice lightweight theme.\n\nbase_plot + theme_classic()\n\n\n\n\n\n\ntheme_bw()\nIn black and white.\n\nbase_plot + theme_bw()\n\n\n\n\n\n\ntheme_dark()\nFor dark-mode aficionados.\n\nbase_plot + theme_dark()\n\n\n\n\n\n\ntheme_void()\nHere is a theme with very little if you really want only the bare bones.\n\nbase_plot + theme_void()\n\n\n\n\n\n\n\nComplete themes from other packages\nThe packages ggthemes and hrbrthemes have some nice themes you might be interested in.\n\nlibrary(ggthemes)\nlibrary(hrbrthemes)\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n\n\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n\n\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\n\ntheme_tufte()\nAnother lightweight theme\n\nbase_plot + theme_tufte()\n\n\n\n\n\n\ntheme_excel()\nIn case you find yourself wishing your plots looked more Excel 2005.\n\nbase_plot + theme_excel()\n\n\n\n\n\n\ntheme_ipsum()\nYou need to have Roboto Condensed for this.\n\nbase_plot + theme_ipsum()\n\n\n\n\n\n\n\nModify components of a theme\nIf there is a part of the non-data components of your plot you want to change, chances are you do this using theme(). You can also start with a complete theme and then modify from there. This is what I do most of the time.\nThere are more than 40 unique theme elements that can be modified to control the appearance of a plot.\nYou can find the complete list of theme elements in the ggplot2 documentation. Let‚Äôs play around a little bit.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\n\n\n\nThe legend here is duplicative, let‚Äôs remove it.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\n\n\n\nWhat if we wanted to make the strip text background black, and the strip text white?\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        strip.text = element_text(color = \"white\"),\n        strip.background = element_rect(fill = \"black\"))\n\n\n\n\nRemember that ggplot works on layers and these layers are added in the order you indicate. That means if you write something code that negates or edits something that comes above, the lower code will prevail.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(color = \"white\"),\n        strip.background = element_rect(fill = \"black\")) +\n  theme_classic() \n\n\n\n\n\n\nSetting an active theme\nIf you know you want to use one theme for all your plots, you can set all the parameters for that theme using theme_set() and theme_update() and then your theme will carry for all the plots you make going forward.\n\nmy_theme &lt;- theme_set(theme_classic())\n\nbase_plot"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#saving-your-plots",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#saving-your-plots",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Saving your plots",
    "text": "Saving your plots\nYou probably won‚Äôt want to save all your plots, but you definitely will want to save some of them. The function ggsave() makes this each. I like to save images as .svg as these are vectorized and have unlimited resolution. You could also adjust the file extension to save it in the format you like.\n\nggsave(plot = base_plot,\n       filename = \"img/my_plot.svg\",\n       width = 9,\n       height = 6)\n\nI like to set the code chunk options for my chunks where I am saving plots to eval = FALSE this way I don‚Äôt accidentally save over figures I don‚Äôt intend to. If I want to save the plot, I can do so manually."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#useful-resources",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#useful-resources",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Useful resources",
    "text": "Useful resources\n\nggplot2 cheatsheet\nggplot2 documentation\nggplot2: elegant graphics for data analysis by Hadley Wickham\nA really compehensive list of resources compiled by Erik Gahner Larsen\nPast ggplot Code Clubs:\n\nVisualizing Data by Michael Broe\nggplot round 2 by me\nFaceting, multi-plots, and animating\nVisualizing Data by Michael Broe a second one\nggplot round 2 a second one by me"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html",
    "href": "modules/module2/04_wrangling/04_wrangling.html",
    "title": "Wrangling your data ü§†, the basics",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling.html#introduction",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Introduction",
    "text": "Introduction\nThis is a new lecture from the previous delivery of this course. In the last offering, I found that the process of wrangling data was by far the thing that people had the most trouble with. In recitations, and for module assignments, I would provide data in a way that would need some adjustment before visualization can be made - and if I‚Äôm being honest, I heard a lot of rumblings about this.\nStill, I am going to leave in the course activities that required data to the wrangled before visualization. I am doing this because real data is mostly not structured precisely how it needs to be to make the visualizations you want. I want to provide you all some practice to get comfortable with using your data lassos. This is something you need to get comfortable with on your coding journey.\nBut, I have added in this extra lecture to explicitly go over what I think are the most useful wrangling functions and tools you can use in R. I hope this introduces you to some of what is possible with R, so it will trigger your memory later when you need to use it. You can also always come back to this page during the course."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#what-is-the-tidyverse",
    "href": "modules/module2/04_wrangling/04_wrangling.html#what-is-the-tidyverse",
    "title": "Wrangling your data ü§†, the basics",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\n‚ÄúThe tidyverse‚Äù is a collection of packages called that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier. We will be using mostly tidyverse functions in this class, with some base R syntax scattered throughout.\nThe ‚Äúcore tidyverse‚Äù contains the 8 packages below:\n\ndplyr: for data manipulation\nggplot2: a ‚Äúgrammar of graphics‚Äù for creating beautiful plots\nreadr: for reading in rectangular data (i.e., Excel-style formatting)\ntibble: using tibbles as modern/better dataframes\nstringr: handling strings (i.e., text or stuff in quotes)\nforcats: for handling categorical variables (i.e., factors) (meow!)\ntidyr: to make ‚Äútidy data‚Äù\npurrr: for enhancing functional programming (also meow!)\n\nWe will be using many of these other packages in this course, but will talk about them as we go. There are more tidyverse packages outside of these core eight, and we will talk about some of them another time.\n\ntl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are required, but I think you‚Äôll find many tidyverse approaches easier and more intuitive than using base R.\n\nYou can find here some examples of comparing tidyverse and base R syntax.\nToday we will be mostly talking through functions that live within the dplyr package."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#installing-ggplot-tidyverse",
    "href": "modules/module2/04_wrangling/04_wrangling.html#installing-ggplot-tidyverse",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Installing ggplot & tidyverse",
    "text": "Installing ggplot & tidyverse\nTo install packages in R that are on the Comprehensive R Archive Network (CRAN), you can use the function install.packages().\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\nWe only need to install packages once. But, every time we want to use them, we need to ‚Äúload‚Äù them, and can do this using the function library().\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.4     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIt‚Äôs a good habit to not ignore warnings/messages that R gives you.\n\ntl:dr install.packages() once, library() every time."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#loading-data",
    "href": "modules/module2/04_wrangling/04_wrangling.html#loading-data",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Loading data",
    "text": "Loading data\nIn class, we will use a combination of data embedded within R (or packages in R), from the internet, or data you import yourself. I am going to quickly go over ways to import common data types.\n\n.csv\nFiles saved as comma separated values are the most common data type I tend to import. The function read_csv() which is a part of the tidyverse package readr allows you to do this easily as it has a special function for this file type, as it is so common.\nMake sure that your file is within your working directory (or you have its relative or complete path), and you can install it (and save it) like this:\n\nsample_csv_data &lt;- read_csv(file = \"my-file-name.csv\")\n\n\n\n.xlsx\nThe second most common file type I import are those made in Excel. These files can either be converted to a .csv and then read in like we just went over, or you can load the package readxl and read files in directly. If you don‚Äôt already have readxl you can download it using install.packages().\n\nlibrary(readxl)\nsample_excel_data &lt;- read_excel(file = \"my-file-name.xlsx\",\n                                sheet = \"Sheet1\")\n\nHere you can find the readr cheatsheet."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#the-pipe",
    "href": "modules/module2/04_wrangling/04_wrangling.html#the-pipe",
    "title": "Wrangling your data ü§†, the basics",
    "section": "The pipe |>",
    "text": "The pipe |&gt;\nThe pipe |&gt; (which used to be written %&gt;%, and you will see this widely when googling/troubleshooting and sometimes see me default to this older syntax) is a tool that allows you to take the output of one function, and send it to the next function.\nYou can read the pipe as ‚Äúand then‚Äù - here is a theoretical example.\n\ntake_this_data |&gt;\n  then_this_function() |&gt;\n  then_another_function() |&gt; \n  finally_a_last_function()\n\nThe easiest way to see how the pipe works is with an example. We are going to use the dataset diamonds which comes pre-loaded when you load the tidyverse.\nWhat is in the dataset diamonds? We can get a ‚Äúglimpse‚Äù of it with the function glimpse, which is sort of like the tidyverse version of str().\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\n\nWhat if we want to see what is the average price of a diamond where cut = \"Premium\". There are a few ways we can do this.\n\n# one way\n# filter for only the premium diamonds\ndiamonds_premium &lt;- filter(diamonds, cut == \"Premium\")\n\n# calculate the mean using summarize\nsummarize(diamonds_premium, mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      4584.\n\n# or calculate mean using mean\n# the function mean() requires a vector\nmean(diamonds_premium$price)\n\n[1] 4584.258\n\n\nOr, we can use the pipe |&gt;. We are going to talk about summarize() in a minute.\n\ndiamonds |&gt;\n  filter(cut == \"Premium\") |&gt;\n  summarize(mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      4584.\n\n# if we want to use the function mean() we need to supply a vector\ndiamonds |&gt; \n  filter(cut == \"Premium\") |&gt;\n  pull(price) |&gt; # pulls out price as a vector\n  mean()\n\n[1] 4584.258\n\n\nSome reasons I like the pipe:\n\nits easier to read (and doesn‚Äôt have a lot of nested parentheses)\nit doesn‚Äôt require you to create lots of interim objects which you won‚Äôt use again\nits easy to troubleshoot\n\n\nThe keyboard shortcut for |&gt; is Ctrl/Cmd + Shift + M\n\nOf course you can assign the output of a pipe to something using the assignment operator &lt;- and then use it for other things.\nSsome functions are not ‚Äúpipe friendly‚Äù meaning they will not work using pipes. This is often because the data is not the first argument passed to the function. All tidyverse functions work with piping."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#selecting-columns-with-select",
    "href": "modules/module2/04_wrangling/04_wrangling.html#selecting-columns-with-select",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nOften you will want to pick only certain columns in your dataframe, and you can do this with the function select(). You can pick columns by:\n\ntheir names\ntheir position (i.e., index)\ncharacteristics of that column\n\nLet‚Äôs select first by name.\n\ndiamonds |&gt; \n  select(carat, cut, price)\n\n# A tibble: 53,940 √ó 3\n   carat cut       price\n   &lt;dbl&gt; &lt;ord&gt;     &lt;int&gt;\n 1  0.23 Ideal       326\n 2  0.21 Premium     326\n 3  0.23 Good        327\n 4  0.29 Premium     334\n 5  0.31 Good        335\n 6  0.24 Very Good   336\n 7  0.24 Very Good   336\n 8  0.26 Very Good   337\n 9  0.22 Fair        337\n10  0.23 Very Good   338\n# ‚Ñπ 53,930 more rows\n\n\nNote that when you use the pipe, the potential column names will autofill for you after you type 3 letters. You can also hit tab to scroll through all the potential objects to select.\nWe can also select by index. In general I would recommend against this because its really hard to remember which column indices are which variables today, nevermind returning back to old code 1 year from now.\n\ndiamonds |&gt; \n  select(c(1, 2, 7)) # you could also use the colon syntax if your columns are sequential\n\n# A tibble: 53,940 √ó 3\n   carat cut       price\n   &lt;dbl&gt; &lt;ord&gt;     &lt;int&gt;\n 1  0.23 Ideal       326\n 2  0.21 Premium     326\n 3  0.23 Good        327\n 4  0.29 Premium     334\n 5  0.31 Good        335\n 6  0.24 Very Good   336\n 7  0.24 Very Good   336\n 8  0.26 Very Good   337\n 9  0.22 Fair        337\n10  0.23 Very Good   338\n# ‚Ñπ 53,930 more rows\n\n\nYou can also select using selection helpers like:\n\neverything(): picks all variables\nstarts_with(): starts with some prefix\ncontains(): contains a specific string\nwhere(): selects columns where the statement given in the argument is TRUE\n\nHere is an example of using where() to select only the columns that are numeric.\n\ndiamonds |&gt; \n  select(where(is.numeric))\n\n# A tibble: 53,940 √ó 7\n   carat depth table price     x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23  61.5    55   326  3.95  3.98  2.43\n 2  0.21  59.8    61   326  3.89  3.84  2.31\n 3  0.23  56.9    65   327  4.05  4.07  2.31\n 4  0.29  62.4    58   334  4.2   4.23  2.63\n 5  0.31  63.3    58   335  4.34  4.35  2.75\n 6  0.24  62.8    57   336  3.94  3.96  2.48\n 7  0.24  62.3    57   336  3.95  3.98  2.47\n 8  0.26  61.9    55   337  4.07  4.11  2.53\n 9  0.22  65.1    61   337  3.87  3.78  2.49\n10  0.23  59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nYou can find more helpers here.\nUsing select() will also set the order of your columns. More about this later."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#choosing-observations-with-filter",
    "href": "modules/module2/04_wrangling/04_wrangling.html#choosing-observations-with-filter",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Choosing observations with filter()",
    "text": "Choosing observations with filter()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nSometimes you want to select observations (rows) based on values. To do this you use filter(). Try not to confuse this with select().\n\nselect() picks columns, while filter() picks rows.\n\nThe function filter() will keep only observations that meet your filtering criteria.\nLet‚Äôs say we want to only keep the diamonds that are bigger than 3 carats.\n\ndiamonds |&gt; \n  filter(carat &gt; 3)\n\n# A tibble: 32 √ó 10\n   carat cut     color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.01 Premium I     I1       62.7    58  8040  9.1   8.97  5.67\n 2  3.11 Fair    J     I1       65.9    57  9823  9.15  9.02  5.98\n 3  3.01 Premium F     I1       62.2    56  9925  9.24  9.13  5.73\n 4  3.05 Premium E     I1       60.9    58 10453  9.26  9.25  5.66\n 5  3.02 Fair    I     I1       65.2    56 10577  9.11  9.02  5.91\n 6  3.01 Fair    H     I1       56.1    62 10761  9.54  9.38  5.31\n 7  3.65 Fair    H     I1       67.1    53 11668  9.53  9.48  6.38\n 8  3.24 Premium H     I1       62.1    58 12300  9.44  9.4   5.85\n 9  3.22 Ideal   I     I1       62.6    55 12545  9.49  9.42  5.92\n10  3.5  Ideal   H     I1       62.8    57 12587  9.65  9.59  6.03\n# ‚Ñπ 22 more rows\n\n\nHere I made use of the greater than &gt; sign, and there are other operators you could also use to help you filter.\n\n==: equal to (I usually read this as exactly equal to, and is different than using an equal sign in an equation)\n&lt;, &gt;: less than or greater than\n&lt;=, &gt;=: less than or equal to, great than or equal to\n&: and\n|: or\n!: not equal\nis.na: is NA\n\nYou can also layer your filtering.\n\ndiamonds |&gt; \n  filter(carat &gt; 3 & cut == \"Premium\")\n\n# A tibble: 13 √ó 10\n   carat cut     color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.01 Premium I     I1       62.7    58  8040  9.1   8.97  5.67\n 2  3.01 Premium F     I1       62.2    56  9925  9.24  9.13  5.73\n 3  3.05 Premium E     I1       60.9    58 10453  9.26  9.25  5.66\n 4  3.24 Premium H     I1       62.1    58 12300  9.44  9.4   5.85\n 5  3.01 Premium G     SI2      59.8    58 14220  9.44  9.37  5.62\n 6  4.01 Premium I     I1       61      61 15223 10.1  10.1   6.17\n 7  4.01 Premium J     I1       62.5    62 15223 10.0   9.94  6.24\n 8  3.67 Premium I     I1       62.4    56 16193  9.86  9.81  6.13\n 9  3.01 Premium I     SI2      60.2    59 18242  9.36  9.31  5.62\n10  3.04 Premium I     SI2      59.3    60 18559  9.51  9.46  5.62\n11  3.51 Premium J     VS2      62.5    59 18701  9.66  9.63  6.03\n12  3.01 Premium J     SI2      60.7    59 18710  9.35  9.22  5.64\n13  3.01 Premium J     SI2      59.7    58 18710  9.41  9.32  5.59"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#make-new-columns-with-mutate",
    "href": "modules/module2/04_wrangling/04_wrangling.html#make-new-columns-with-mutate",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Make new columns with mutate()",
    "text": "Make new columns with mutate()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nSometimes you want to make new columns based on existing variables and you can do this with mutate().\nFor example, we might want to create a new column called ‚Äúprice_per_carat‚Äù which we calculate by taking price and divide it by carat. Keep in mind this would be an easy way to log transform data.\n\ndiamonds |&gt; \n  mutate(price_per_carat = price/carat)\n\n# A tibble: 53,940 √ó 11\n   carat cut   color clarity depth table price     x     y     z price_per_carat\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43           1417.\n 2  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31           1552.\n 3  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31           1422.\n 4  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63           1152.\n 5  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75           1081.\n 6  0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48           1400 \n 7  0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47           1400 \n 8  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53           1296.\n 9  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49           1532.\n10  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39           1470.\n# ‚Ñπ 53,930 more rows\n\n\nMutated columns are by default put at the end of the dataframe. We can reorder simply using select().\n\ndiamonds |&gt; \n  mutate(price_per_carat = price/carat) |&gt; \n  select(price_per_carat, everything()) # put new column first, then everything\n\n# A tibble: 53,940 √ó 11\n   price_per_carat carat cut   color clarity depth table price     x     y     z\n             &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1           1417.  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2           1552.  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31\n 3           1422.  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 4           1152.  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63\n 5           1081.  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n 6           1400   0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7           1400   0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8           1296.  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53\n 9           1532.  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\n10           1470.  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nYou can also make new columns using conditional statements. For example, what if we want to create a new column that tells us if a diamond is more than $1000 called ‚Äúat_least_1000‚Äù. We will do this using if_else().\n\ndiamonds |&gt; \n  mutate(at_least_1000 = if_else(condition = price &gt;= 1000,\n                                  true = \"$1000 or more\",\n                                  false = \"less than $1000\")) |&gt; \n  select(at_least_1000, everything()) # move to front so we can see it\n\n# A tibble: 53,940 √ó 11\n   at_least_1000   carat cut   color clarity depth table price     x     y     z\n   &lt;chr&gt;           &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 less than $1000  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2 less than $1000  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31\n 3 less than $1000  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 4 less than $1000  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63\n 5 less than $1000  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n 6 less than $1000  0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7 less than $1000  0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8 less than $1000  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53\n 9 less than $1000  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\n10 less than $1000  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nIf you have more than two conditions, you can use case_when().\nIf you use mutate() to create a new column that has the same name as an existing column, it will override that current column.\nYou can find other mutate() helpers here."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#sorting-with-arrange",
    "href": "modules/module2/04_wrangling/04_wrangling.html#sorting-with-arrange",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Sorting with arrange()",
    "text": "Sorting with arrange()\nSometimes you just want to see a dataframe ordered by a particular column. We can do that easily with arrange().\n\ndiamonds |&gt; \n  arrange(price)\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nBy default, arrange() sorts from smallest to largest. We can change that if that‚Äôs what we want.\n\n# these are the same\ndiamonds |&gt; \n  arrange(-price)\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ‚Ñπ 53,930 more rows\n\ndiamonds |&gt; \n  arrange(desc(price))\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ‚Ñπ 53,930 more rows"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#computing-summaries-with-summarize",
    "href": "modules/module2/04_wrangling/04_wrangling.html#computing-summaries-with-summarize",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Computing summaries with summarize()",
    "text": "Computing summaries with summarize()\nThe function summarize() calculates summary information based on the functions you provide as arguments. This function creates a wholly new dataframe, providing one row for each grouping variable. If there is no grouping, the resulting dataframe will have one row.\nLet‚Äôs look at an example. We can use summarize() The syntax is new_column_name = function().\n\ndiamonds |&gt; \n  summarize(mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      3933.\n\n\nWe can also provide multiple items for summary.\n\ndiamonds |&gt; \n  summarize(mean_price = mean(price),\n            sd_price = sd(price),\n            count = n())\n\n# A tibble: 1 √ó 3\n  mean_price sd_price count\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1      3933.    3989. 53940\n\n\nHere are some examples of functions you can use within summarize():\n\nmean() and median(): calculate mean and median\nsd() and IQR(): calculate standard deviation and interquartile range\nmin() and max(): calculate min and max\nn() and n_distinct(): calculate how many observations there are, and how many distinct observations there are\n\nYou can also use the function across() combined with where() to calculate summary data ‚Äúacross‚Äù different columns.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nFor example, like we see in the illustration above, we might want to calculate the mean ‚Äúacross‚Äù all columns ‚Äúwhere‚Äù if we asked if that column contains numeric data, we would get TRUE.\n\ndiamonds |&gt; \n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 1 √ó 7\n  carat depth table price     x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.798  61.7  57.5 3933.  5.73  5.73  3.54\n\n\nI hope you can start to see now how combining lots of these different functions together will help you achieve what you want with your coding."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#operations-by-group-with-group_by",
    "href": "modules/module2/04_wrangling/04_wrangling.html#operations-by-group-with-group_by",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Operations by group with group_by()",
    "text": "Operations by group with group_by()\nSometimes you might want to group your data together to perform operations group-wise. You can do this with group_by(). The way to ungroup is to use ungroup().\nFor example, say we want to calculate the average price of a diamond for each cut type.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(mean_price = mean(price))\n\n# A tibble: 5 √ó 2\n  cut       mean_price\n  &lt;ord&gt;          &lt;dbl&gt;\n1 Fair           4359.\n2 Good           3929.\n3 Very Good      3982.\n4 Premium        4584.\n5 Ideal          3458.\n\n\nNow instead of getting one row for the mean price, we are getting a mean price for each cut.\nNote that when you use group_by(), the groupings are now embedded within your data. Let me show you what I mean.\n\ndiamonds_cut &lt;- diamonds |&gt; \n  group_by(cut)\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\nglimpse(diamonds_cut)\n\nRows: 53,940\nColumns: 10\nGroups: cut [5]\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\n\nAgain we can combine these different functions together to summarize for the mean value across all columns that are numeric, but this time grouped by cut.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 5 √ó 8\n  cut       carat depth table price     x     y     z\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40\n\n\nWe can also use summarize() to add how many observations there are for each category.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(across(where(is.numeric), mean), n = n())\n\n# A tibble: 5 √ó 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\n\nHere is a helpful blogpost by Hadley Wickham for working across columns."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#pivoting-with-pivot_longer-and-pivot_wider",
    "href": "modules/module2/04_wrangling/04_wrangling.html#pivoting-with-pivot_longer-and-pivot_wider",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Pivoting with pivot_longer() and pivot_wider()",
    "text": "Pivoting with pivot_longer() and pivot_wider()\nThe function pivot_longer() will often let you make your data in ‚Äútidy‚Äù format, and pivot_wider() allow you to make it untidy (but often still useful) again. Let me explain more what I mean.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nThis is easier to ‚Äúsee‚Äù üëÄ than to explain. Here is an example of non-tidy data, where there is data embedded in column names, and one variable (the rank of a song) is spread across many columns:\n\nbillboard\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nHere is an example of the same exact data, in a tidy format, where those data that used to be column names, are now values coded for a particular variable.\n\nbillboard_long &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\nbillboard_long\n\n# A tibble: 24,092 √ó 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ‚Ñπ 24,082 more rows\n\n\nWe can go back from our new longer dataframe with pivot_wider().\n\nbillboard_long |&gt; \n  pivot_wider(names_from = week,\n              values_from = rank)\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nIn recap:\npivot_longer() pulls data that is embedded in column names, and reshapes your dataframe such this information is now embedded within the values. Or put differently, it collects variables that are spread across multiple columns into a single column. This makes your dataframes longer, i.e., increases the number of rows. Typically, we use pivot_longer() to make an untidy dataset tidy.\npivot_wider() takes data that is embedded in the values of your dataframe, and puts this information in variable names. Or put differently, it spreads a variable across multiple columns. This makes your dataframe ‚Äúwider‚Äù, i.e., increases the number of columns. Typically, pivot_wider() will make a dataset untidy. This can be useful for certain calculations, or if you want to use a for loop to do something iteratively across columns."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#joining-data-together",
    "href": "modules/module2/04_wrangling/04_wrangling.html#joining-data-together",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Joining data together",
    "text": "Joining data together\nOften you will have two separate dataframes that you want to join together. You can do this in two main ways:\n\nby matching something between them (i.e., using _join())\nby smushing them together in their existing order by columns bind_cols() or rows bind_rows().\n\n\n*_join()\nWe can join two dataframes, let‚Äôs call them x and y, together based on a key that we provide. This is one of the first things I did using R that I felt like wow this is really a lot easier than be doing this manually.\nThere are four types of joins:\n\ninner_join(): keeps observations in x that are also present in y\nleft_join(): keeps observations in x\nright_join()`: keeps observations in y\nfull_join(): keeps observations in both x and y\n\nWe will use the datasets band_members and band_instruments which are pre-loaded with the tidyverse to show how this works. You can also see these examples on the mutating joins documentation page.\n\nglimpse(band_members)\n\nRows: 3\nColumns: 2\n$ name &lt;chr&gt; \"Mick\", \"John\", \"Paul\"\n$ band &lt;chr&gt; \"Stones\", \"Beatles\", \"Beatles\"\n\nglimpse(band_instruments)\n\nRows: 3\nColumns: 2\n$ name  &lt;chr&gt; \"John\", \"Paul\", \"Keith\"\n$ plays &lt;chr&gt; \"guitar\", \"bass\", \"guitar\"\n\n\nR will make its best guess as to what you want to ‚Äújoin‚Äù based on, and that works a lot of the time, but I always like to be exclicit and indicate the column key for the join with by =.\nAn inner join: we will only get the observations that are present in both dataframes.\n\ninner_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 2 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nAn left join: we will only get the observations that are present in band_members. Note the appearance of NA for Mick.\n\nleft_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 3 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nAn right join: we will only get the observations that are present in band_instruments. Note the appearance of NA for band for Keith (Richards, who is in the Rolling Stones). You could also switch the order of the dataframes in your argument instead of using left vs right.\n\nright_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 3 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n3 Keith &lt;NA&gt;    guitar\n\n\nAn full join: we get all observations of what is present in band_members and band_instruments.\n\nfull_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 4 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nThere is nuances to what happens in different joining situations, so do this cautiously and always check that it went the way you expected it to.\n\n\nbind_rows() and bind_cols()\nGenerally it would be preferrable to use a _join() over bind_cols() or bind_rows() since in the latter, the binding happens in the order that observations appear. This might make your data not meaningful without you knowing.\nLet‚Äôs get to the examples.\n\ndata1 &lt;- tibble(x = 1:5)\ndata2 &lt;- tibble(y = 6:10)\n\nbind_cols(data1, data2)\n\n# A tibble: 5 √ó 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     6\n2     2     7\n3     3     8\n4     4     9\n5     5    10\n\n\n\nbind_rows(data1, data2)\n\n# A tibble: 10 √ó 2\n       x     y\n   &lt;int&gt; &lt;int&gt;\n 1     1    NA\n 2     2    NA\n 3     3    NA\n 4     4    NA\n 5     5    NA\n 6    NA     6\n 7    NA     7\n 8    NA     8\n 9    NA     9\n10    NA    10"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#handling-strings",
    "href": "modules/module2/04_wrangling/04_wrangling.html#handling-strings",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Handling strings",
    "text": "Handling strings\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nHandling strings (i.e., ‚Äústrings‚Äù of characters) could be multiple whole lessons, so my goal is to introduce you here to how to handle them. The tidyverse package to manage strings is called stringr. Sometimes you might want to automate extraction of only part of a value present in a column to use, remove some values, or split strings apart. This is valuable especially when the way that data is coded/recorded is different than the way you want it to be when you analyze it. Instead of manually recoding in excel, you can reproducibly and tracibly recode in R. You can read about all the functions within stringr here.\nYou can use regular expressions within stringr functions, but I‚Äôm not going to explicitly go over that (check out these code clubs regex1, regex2 if you want to learn more).\nI‚Äôm going to create some sample data to play with.\n\nstrings &lt;- tibble(\n  sample = c(rep.int(\"Treatment_Level1\", 3), \n             rep.int(\"Treatment_Level2\", 3),\n             rep.int(\"Treatment_Level3\", 3),\n             rep.int(\"Control_Level1\", 3),\n             rep.int(\"Control_Level2\", 3),\n             rep.int(\"Control_Level3\", 3)))\n\nLet‚Äôs first ask how many times do we have the string ‚Äú3‚Äù in our dataframe? Note that these functions accept a vector, so you need to provide data in that form. The function str_detect() gives a logical vector as the output, the same length as the vector provided, and indicates FALSE when the pattern is not met, and TRUE when it is.\n\nstr_detect(strings$sample, pattern = \"3\")\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n# using sum will count how many times the logical is evaluated to be TRUE\nsum(str_detect(strings$sample, pattern = \"3\"))\n\n[1] 6\n\n\nYou might want to re-code your data so that Level1 becomes that actual level used. Let‚Äôs say that Level1 is 100mg, Level2 is 300 mg, and Level3 is 500mg. We can do this with str_replace() to replace the first match only or str_replace_all() to replace all matches (which is what we want here).\n\n(strings$sample &lt;- strings |&gt; \n  select(sample) |&gt; \n  pull(sample) |&gt; # make a vector so can pass to next fxn\n  str_replace_all(pattern = \"Level1\", replacement = \"100mg\") |&gt; \n  str_replace_all(pattern = \"Level2\", replacement = \"300mg\") |&gt; \n  str_replace_all(pattern = \"Level3\", replacement = \"500mg\"))   \n\n [1] \"Treatment_100mg\" \"Treatment_100mg\" \"Treatment_100mg\" \"Treatment_300mg\"\n [5] \"Treatment_300mg\" \"Treatment_300mg\" \"Treatment_500mg\" \"Treatment_500mg\"\n [9] \"Treatment_500mg\" \"Control_100mg\"   \"Control_100mg\"   \"Control_100mg\"  \n[13] \"Control_300mg\"   \"Control_300mg\"   \"Control_300mg\"   \"Control_500mg\"  \n[17] \"Control_500mg\"   \"Control_500mg\"  \n\n\nWe might not want to have both Treatment/Control and Level nested in the same cell, we can split them apart using separate_*() functions. Here we are using separate_wider_delim() to seprate the column ‚Äúsample‚Äù into two new columns called ‚Äútreatment‚Äù and ‚Äúdose.\n\n(strings_separated &lt;- strings |&gt; \n  separate_wider_delim(cols = sample,\n                       delim = \"_\", # what is the delimiter\n                       names = c(\"treatment\", \"dose\")))\n\n# A tibble: 18 √ó 2\n   treatment dose \n   &lt;chr&gt;     &lt;chr&gt;\n 1 Treatment 100mg\n 2 Treatment 100mg\n 3 Treatment 100mg\n 4 Treatment 300mg\n 5 Treatment 300mg\n 6 Treatment 300mg\n 7 Treatment 500mg\n 8 Treatment 500mg\n 9 Treatment 500mg\n10 Control   100mg\n11 Control   100mg\n12 Control   100mg\n13 Control   300mg\n14 Control   300mg\n15 Control   300mg\n16 Control   500mg\n17 Control   500mg\n18 Control   500mg\n\n\nThe opposite function for separate_() is unite().\nIf we wanted to extract just the number part out of ‚Äúdose‚Äù we could use readr::parse_number() to do that. Note I‚Äôve embedded parse_number() within a mutate() function to change the values in the dataset.\n\nstrings_separated |&gt; \n  mutate(dose = parse_number(dose))\n\n# A tibble: 18 √ó 2\n   treatment  dose\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Treatment   100\n 2 Treatment   100\n 3 Treatment   100\n 4 Treatment   300\n 5 Treatment   300\n 6 Treatment   300\n 7 Treatment   500\n 8 Treatment   500\n 9 Treatment   500\n10 Control     100\n11 Control     100\n12 Control     100\n13 Control     300\n14 Control     300\n15 Control     300\n16 Control     500\n17 Control     500\n18 Control     500"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#cleaning-up-column-names-with-clean_names",
    "href": "modules/module2/04_wrangling/04_wrangling.html#cleaning-up-column-names-with-clean_names",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Cleaning up column names with clean_names()",
    "text": "Cleaning up column names with clean_names()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nI really like the package janitor which has some nice functions for cleaning up üßπ ‚Äúmessy‚Äù data. I use clean_names() a lot which converts untidy column names into only characters (default all in lower case) and connects words or terms with underscores.\nI am making up some messy names so you can see how this works.\n\n# make messy data\nmessy_data &lt;- tibble(\n  \"Sample Name\" = 1:5,\n  \"THE NEXT VARIABLE\" = 6:10,\n  \"ThisIsChaos\" = 11:15\n)\n\n# print column names\ncolnames(messy_data)\n\n[1] \"Sample Name\"       \"THE NEXT VARIABLE\" \"ThisIsChaos\"      \n\n\n\n# install and load janitor\n# install.packages(\"janitor)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# clean up column names\nclean_names(messy_data)\n\n# A tibble: 5 √ó 3\n  sample_name the_next_variable this_is_chaos\n        &lt;int&gt;             &lt;int&gt;         &lt;int&gt;\n1           1                 6            11\n2           2                 7            12\n3           3                 8            13\n4           4                 9            14\n5           5                10            15"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html",
    "title": "Wrangling your data ü§† Recitation",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class. When you go to the links below, click on the Download Raw File icon (the down arrow over a turned open bracket) at the top right of the file\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#introduction",
    "title": "Wrangling your data ü§† Recitation",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class. When you go to the links below, click on the Download Raw File icon (the down arrow over a turned open bracket) at the top right of the file\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#explore-your-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#explore-your-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Explore your data",
    "text": "Explore your data\nWrite some code that lets you explore that is in these two datasets.\nHow many observations there in each dataset?\nWhat years do the data contain information for?"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#modifying-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#modifying-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Modifying data",
    "text": "Modifying data\nCreate a new dataset for life_expectancy that only includes observed data (i.e., remove the projected data after 2022)."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#calculating-summaries",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#calculating-summaries",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Calculating summaries",
    "text": "Calculating summaries\nWhat country has the highest average happiness index in 2022?\nWhat about overall average highest index?\nHow many countries had an average life expectancy over 80 years in 2022?\nWhat countries are in the top 10 percentile for happiness? What about the bottom? What about for life expectancy? You can calculate this for the most recent data, for the mean, or really for whatever you want. Remember there are lots of ways to do this.\nClick the button Show on the right if you need a hint\n\n# Hint - try using the functions in the `slice_()` family.\n\nWhich country has had their happiness index increase the most from 2012 to 2022? Which dropped the most?"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#joining-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#joining-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Joining data",
    "text": "Joining data\nTry joining the happiness and life_expectancy datasets together and use the different *_join() functions so you can see how they differ. Check their dimensions and look at them. Think about how you might want to do different joins in different situations.\nIf you wanted to create a plot that allowed you to see the correlation between happiness score and life expectancy in 2022, which joined dataset would you use and why?"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html",
    "href": "modules/module3/08_correlations/08_correlations.html",
    "title": "Visualizing Correlations",
    "section": "",
    "text": "Figure from XKCD"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations.html#introduction",
    "title": "Visualizing Correlations",
    "section": "Introduction",
    "text": "Introduction\nWe will will building on our lessons on ggplot101 and ggplot102 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, geoms, facets, scales, labels, and themes. Today we are going to apply what we learned towards trying to better understanding and visualize correlations within our data. To do this we will also use some ggplot extension packages.\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\n\nlibrary(tidyverse)\n\nToday we are going to continue to use the same real research data from my group from last week. We will be reading in the supplementary data from a paper written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 1. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloids &lt;- readxl::read_excel(\"tpg220192-sup-0002-supmat.xlsx\",\n                                sheet = \"S1 Raw Data Diversity Panel\")\n\n\nknitr::kable(head(alkaloids))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nYear\nEnvironment\nBlock\nGenotype\nPlot_Source\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nLatitude\nLongitude\nDehydrotomatidine\nTomatidine\nDehydrotomatine1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\n\n\n\n\n7805\n2018\nFreEarly18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.000000\n5.726010\n0.350331\n6.076341\n172.66244\n1.079190\n86.72742\n17.831892\n9.142607\n114.78111\n18.902399\n56.307182\n1.890053\n77.099634\n5.125904\n10.277325\n336.8893\n347.1666\n3.787979\n0.924195\n3.943230\n8.655404\n731.5675\n\n\n7898\n2017\nFre17\n2\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.169068\n0.000000\n0.000000\n0.000000\n55.47329\n0.000000\n53.32292\n13.630697\n4.841762\n71.79538\n3.557348\n4.107289\n0.000000\n7.664637\n2.905500\n5.548102\n199.6694\n205.2175\n8.978931\n1.897850\n6.794690\n17.671471\n360.8969\n\n\n7523\n2018\nFreLate18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.135675\n0.680554\n5.073552\n0.000000\n5.073552\n123.85835\n0.000000\n50.90989\n6.503939\n1.368847\n58.78268\n3.931461\n4.123222\n0.623340\n8.678023\n2.185082\n5.104115\n259.0177\n264.1218\n4.049145\n0.000000\n6.749386\n10.798531\n474.3143\n\n\n7724\n2017\nFre17\n1\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.054300\n0.497261\n19.419087\n0.000000\n19.419087\n239.01264\n0.000000\n36.02318\n8.557673\n7.483933\n52.06478\n3.341048\n16.415426\n1.057100\n20.813574\n0.000000\n0.000000\n203.0061\n203.0061\n1.678210\n0.000000\n2.349633\n4.027843\n538.8955\n\n\n7427\n2018\nFreLate18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.139454\n0.553801\n0.000000\n0.000000\n0.000000\n64.31783\n0.879435\n39.91027\n7.228388\n3.015298\n51.03339\n0.000000\n3.131685\n0.000000\n3.131685\n0.000000\n4.054211\n299.5687\n303.6229\n10.146857\n0.000000\n4.882339\n15.029197\n437.8283\n\n\n7854\n2018\nFreEarly18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.049700\n0.262174\n3.737579\n0.000000\n3.737579\n68.44913\n0.000000\n23.86864\n13.506299\n1.456982\n38.83192\n4.657902\n4.259007\n0.605729\n9.522638\n9.832149\n11.595595\n459.5205\n471.1161\n6.839930\n0.486236\n5.595751\n12.921917\n614.7233\n\n\n\n\n\nThis dataset has 605 observations, with data about different steroidal alkaloids in the fruits of different tomato germplasm grown in 3 locations across 2 years. There is also some other metadata too.\nFor those who are chemistry minded, here is a little pathway context for the compounds we are investigating today.\n\n\n\nFigure from Syzma≈Ñsky et al., Nature Genetics 2020"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#scatterplots",
    "href": "modules/module3/08_correlations/08_correlations.html#scatterplots",
    "title": "Visualizing Correlations",
    "section": "Scatterplots",
    "text": "Scatterplots\nA very simple first pass way to understand if you have relationships within your data is to make scatterplots of the variables you think might be correlated. Let‚Äôs start by investigating how the different alkaloid concentrations are correlated to each other. First we will see how alpha-tomatine content (Tomatine) is related to total steroidal alkaloid content (Total).\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total, y = Tomatine)) +\n  geom_point() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g)\",\n       y = \"Alpha-Tomatine (¬µg/100 g)\")\n\n\n\n\nIt seems like there are two separate groups here - the points with a steeper slope, and the points with a less steep slope. We can color our points based on what Class of tomato the data comes from, maybe that will reveal something. In the meanwhile let‚Äôs make this plot look a bit nicer. The package scales has some nice functions that help you control the scaling of your plots, in this case, making each of the axes have numbers in comma_format(). I also am using the hex codes for a color-blind friendly qualitative color scheme developed by Paul Tol.\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point(alpha = 0.8) +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")\n\n\n\n\nAll of the tomatoes in the two extremes of this plot are from the Class Wild Cherry. What would this look like if we removed these fruits? Note, I adjusted the color scale to remove the hex code associated with Wild Cherry but keeping the other colors the same.\n\nalkaloids %&gt;%\n  filter(Class != \"Wild Cherry\") %&gt;%\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point() +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#adding-geom_smooth",
    "href": "modules/module3/08_correlations/08_correlations.html#adding-geom_smooth",
    "title": "Visualizing Correlations",
    "section": "Adding geom_smooth()",
    "text": "Adding geom_smooth()\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point(alpha = 0.8) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nFaceted scatterplots\nWe may be able to see trends by tomato class more easily if we facet our scatterplots. I also am demonstrating here how within the ggplot function you can make alter the aesthetics you plot - here I am turning data that is present as ¬µg/100 g to mg/100 g by dividing by 1000 and changing the axis labels accordingly.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total/1000, y = Tomatine/1000, color = Class)) +\n  geom_point(alpha = 0.8) +\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  facet_wrap(vars(Class), scales = \"free\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total Steroidal Alkaloids (mg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (mg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#correlation-matrix-with-cor",
    "href": "modules/module3/08_correlations/08_correlations.html#correlation-matrix-with-cor",
    "title": "Visualizing Correlations",
    "section": "Correlation matrix with cor()",
    "text": "Correlation matrix with cor()\ncor() is a function from base R that will allow you to create a correlation matrix.\nBefore we use cor() we will clean up our dataset to include only the variables we want to correlate.\n\ncolnames(alkaloids)\n\n [1] \"ID\"                                      \n [2] \"Year\"                                    \n [3] \"Environment\"                             \n [4] \"Block\"                                   \n [5] \"Genotype\"                                \n [6] \"Plot_Source\"                             \n [7] \"Class\"                                   \n [8] \"Origin\"                                  \n [9] \"Provence\"                                \n[10] \"Blanca_Cluster1\"                         \n[11] \"Blanca_Cluster2\"                         \n[12] \"Passport_Species\"                        \n[13] \"Passport_Classification\"                 \n[14] \"Sim_Grouping\"                            \n[15] \"Latitude\"                                \n[16] \"Longitude\"                               \n[17] \"Dehydrotomatidine\"                       \n[18] \"Tomatidine\"                              \n[19] \"Dehydrotomatine1\"                        \n[20] \"Dehydrotomatine2\"                        \n[21] \"TotalDehydrotomatine\"                    \n[22] \"Tomatine\"                                \n[23] \"Hydroxytomatine1\"                        \n[24] \"Hydroxytomatine2\"                        \n[25] \"Hydroxytomatine3\"                        \n[26] \"Hydroxytomatine4\"                        \n[27] \"TotalHydroxytomatine\"                    \n[28] \"Acetoxytomatine1\"                        \n[29] \"Acetoxytomatine2\"                        \n[30] \"Acetoxytomatine3\"                        \n[31] \"TotalAcetoxytomatine\"                    \n[32] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[33] \"LycoperosideFGEsculeosideA1\"             \n[34] \"LycoperosideFGEsculeosideA2\"             \n[35] \"TotalLycoperosideFGEsculeosideA\"         \n[36] \"EsculeosideB1\"                           \n[37] \"EsculeosideB2\"                           \n[38] \"EsculeosideB3\"                           \n[39] \"TotalEsculeosideB\"                       \n[40] \"Total\"                                   \n\n\nFrom looking at the colnames and reading the supplemental information, we can see that some columns are composites of others. For example, the column TotalAcetoxytomatine = Acetoxytomatine1 + Acetoxytomatine2 + Acetoxytomatine3. So we want to pull only the columns that represent the total for any given alkaloids. There should be 10 columns.\n\n# create a vector of the names we want to keep\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\n# make a new df including some metadata and the alkaloid_total_names\nalkaloids_totals &lt;- alkaloids %&gt;%\n  select(ID, Year, Environment, Block, Genotype, Class, all_of(alkaloid_total_names))\n\n# did it work? look at colnames()\ncolnames(alkaloids_totals)\n\n [1] \"ID\"                                      \n [2] \"Year\"                                    \n [3] \"Environment\"                             \n [4] \"Block\"                                   \n [5] \"Genotype\"                                \n [6] \"Class\"                                   \n [7] \"Dehydrotomatidine\"                       \n [8] \"Tomatidine\"                              \n [9] \"TotalDehydrotomatine\"                    \n[10] \"Tomatine\"                                \n[11] \"TotalHydroxytomatine\"                    \n[12] \"TotalAcetoxytomatine\"                    \n[13] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[14] \"TotalLycoperosideFGEsculeosideA\"         \n[15] \"TotalEsculeosideB\"                       \n[16] \"Total\"                                   \n\n\nNow we can create a correlation matrix to see how each of our 10 alkaloids is correlated to the concentration of each other alkaloid (including the compile metric of Total which sums all the alkaloids). The default for cor() is to use Pearson‚Äôs correlation coefficient, but you can set to use Spearman method = \"spearman\" or Kendall method = \"kendall\" if you prefer. Check the documentation for cor() for more information.\n\nalkaloids_cor &lt;- alkaloids_totals %&gt;%\n  select(all_of(alkaloid_total_names)) %&gt;%\n  cor()\n\n# look at our correlation matrix\nknitr::kable(alkaloids_cor) # kable makes a nicely formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDehydrotomatidine\nTomatidine\nTotalDehydrotomatine\nTomatine\nTotalHydroxytomatine\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nTotalLycoperosideFGEsculeosideA\nTotalEsculeosideB\nTotal\n\n\n\n\nDehydrotomatidine\n1.0000000\n0.2974462\n0.0324918\n0.0238230\n0.0099126\n0.0322029\n0.0305049\n0.0761907\n0.0282219\n0.0708252\n\n\nTomatidine\n0.2974462\n1.0000000\n0.3744672\n0.3736949\n0.1003558\n0.0382981\n-0.0059964\n0.0373649\n0.0126724\n0.2044979\n\n\nTotalDehydrotomatine\n0.0324918\n0.3744672\n1.0000000\n0.9214859\n0.2290192\n0.4011257\n-0.0820469\n-0.1149682\n-0.1217560\n0.5636969\n\n\nTomatine\n0.0238230\n0.3736949\n0.9214859\n1.0000000\n0.0995212\n0.1220596\n-0.1140360\n-0.1357819\n-0.1260377\n0.3756155\n\n\nTotalHydroxytomatine\n0.0099126\n0.1003558\n0.2290192\n0.0995212\n1.0000000\n0.3563506\n0.0330078\n0.0284887\n0.0134806\n0.4774036\n\n\nTotalAcetoxytomatine\n0.0322029\n0.0382981\n0.4011257\n0.1220596\n0.3563506\n1.0000000\n-0.0865506\n-0.1106212\n-0.0947254\n0.6782337\n\n\nDehydrolycoperosideFGdehydroesculeosideA\n0.0305049\n-0.0059964\n-0.0820469\n-0.1140360\n0.0330078\n-0.0865506\n1.0000000\n0.8862982\n0.7401116\n0.4792108\n\n\nTotalLycoperosideFGEsculeosideA\n0.0761907\n0.0373649\n-0.1149682\n-0.1357819\n0.0284887\n-0.1106212\n0.8862982\n1.0000000\n0.7779405\n0.5222751\n\n\nTotalEsculeosideB\n0.0282219\n0.0126724\n-0.1217560\n-0.1260377\n0.0134806\n-0.0947254\n0.7401116\n0.7779405\n1.0000000\n0.4059824\n\n\nTotal\n0.0708252\n0.2044979\n0.5636969\n0.3756155\n0.4774036\n0.6782337\n0.4792108\n0.5222751\n0.4059824\n1.0000000\n\n\n\n\n\nNote the diagonal is all composed of 1s. This makes sense because the correlation of each alkaloid with itself is 1."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#using-ggcorrplot-from-ggcorrplot",
    "href": "modules/module3/08_correlations/08_correlations.html#using-ggcorrplot-from-ggcorrplot",
    "title": "Visualizing Correlations",
    "section": "Using ggcorrplot() from ggcorrplot",
    "text": "Using ggcorrplot() from ggcorrplot\nUse the function ggcorrplot() without any additional arguments besides the correlation matrix alkaloids_cor. In general, I think if you want to make a bunch of correlation plots quickly, and don‚Äôt intend to publish them, `ggcorrplot() works well, but the visuals of the plot are quite difficult to customize.\n\nlibrary(ggcorrplot)\n\nggcorrplot(alkaloids_cor)\n\n\n\n\nThis is not a perfect plot but its a good starting point. Correlation matrices are inherently symmetric, meaning if we display only the top or bottom triangle, we do not lose any information. We will work on editing this plot in different ways to show more information and make it more beautiful.\nWe could also make the plot circles instead of squares at the same time.\n\nggcorrplot(alkaloids_cor, \n           method = \"circle\",\n           type = \"lower\")\n\n\n\n\nIn general, I think if you want to make a bunch of correlation plots quickly, and don‚Äôt intend to publish them, `ggcorrplot() works well, but the visuals of the plot are quite difficult to customize."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#using-corrplot-from-corrplot",
    "href": "modules/module3/08_correlations/08_correlations.html#using-corrplot-from-corrplot",
    "title": "Visualizing Correlations",
    "section": "Using corrplot() from corrplot",
    "text": "Using corrplot() from corrplot\nSimilarly, you can use a base R plotting based package corrplot() to make correlation plots. The customization syntax here is quite different from what we‚Äôve been working with in ggplot, but I wanted you to feel familiar with some base R tools.\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\ncorrplot(alkaloids_cor, type = \"lower\")\n\n\n\n\nI have used corrplot() in publications before and felt like I couldn‚Äôt customize the plots as much as I wanted. In the process of putting together this content, I learned some news ways to customize these plots that are actually very nice. Here are some parameters you can modify in R. You can also order your variables by hierarchical clustering.\nFirst we will start (as we always do) by wrangling.\n\n# create matrix for correlation\nalkaloids_to_cor &lt;- alkaloids_totals %&gt;%\n  select(all_of(alkaloid_total_names)) %&gt;%\n  as.matrix() # rcorr() needs a matrix\n\nlibrary(Hmisc) # does cor() but also computes significance levels\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n# create a matrix of pvalues for the correlations\nalkaloids_rcorr = rcorr(alkaloids_to_cor, type = \"pearson\")\n\n# create a vector of the alkaloid names for labeling\nalkaloid_labels &lt;- c(\"Dehydrotomatidine\",\n                     \"Tomatidine\",\n                     \"Dehydrotomatine\",\n                     \"Alpha-Tomatine\",\n                     \"Hydroxytomatine\",\n                     \"Acetoxytomatine\",\n                     \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\",\n                     \"Lycoperoside F, G, \\nor Escueloside A\",\n                     \"Escueloside B\",\n                     \"Total Steroidal Alkaloids\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(alkaloids_rcorr$r) &lt;- alkaloid_labels\nrownames(alkaloids_rcorr$r) &lt;- alkaloid_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(alkaloids_rcorr$P) &lt;- alkaloid_labels\nrownames(alkaloids_rcorr$P) &lt;- alkaloid_labels\n\nNow we are ready to plot\n\ncorrplot(alkaloids_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = alkaloids_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"black\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 0.6) # size of correlation font"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#ggally",
    "href": "modules/module3/08_correlations/08_correlations.html#ggally",
    "title": "Visualizing Correlations",
    "section": "GGally",
    "text": "GGally\n\nggcorr()\nAnother ggplot extension package ggally has the function ggcorr() which also allows the creation of correlation plots, but ones that are more easily customizable. ggcorr() objects are moderately customizable. They make work for some of you so I‚Äôm sharing how to make them.\nNote, GGally::ggcorr() does not take a correlation matrix, but instead takes the data you want to make a correlation matrix for. You can specific the method of correlation in the arguments. The default is Pearson‚Äôs correlation.\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nto_corr &lt;- alkaloids_totals %&gt;%\n  select(all_of(alkaloid_total_names)) \n\nggcorr(to_corr)\n\n\n\n\nThere is only one labeled axis - this is because there is no diagonal in these plots, like we saw with ggcorrplot() and corrplot().\nWe can now spend some time improving the aesthetics of our plot.\n\nggcorr(to_corr, # data for correlation\n       low = \"#f1a340\", # -1 correlation color\n       mid = \"#f7f7f7\", # 0 correlation color\n       high = \"#998ec3\") # 1 correlation color\n\n\n\nggcorr(to_corr,\n       low = \"#f1a340\", mid = \"#f7f7f7\", high = \"#998ec3\",\n       geom = \"circle\",\n       label = TRUE, \n       label_size = 2, \n       label_round = 2,\n       layout.exp = 3)\n\n\n\n\nFor this example, we have very long label names which are really difficult to wrap, but if your labels are more reasonable this may work well for you.\n\n\nggpairs()\nWe can also use the function GGally::ggpairs() to make a matrix of correlation related plots.\n\nalkaloids_totals %&gt;%\n  ggpairs(columns = c(\"Tomatine\", \"TotalLycoperosideFGEsculeosideA\", \"Total\"), # pick variables\n          aes(color = Class))\n\n\n\n\nLet‚Äôs customized a bit.\n\n# remove zeroes since they don't log transform\n# make log transformed columns\nalkaloids_totals_log &lt;- alkaloids_totals %&gt;%\n  filter(Tomatine != 0, \n         TotalLycoperosideFGEsculeosideA != 0,\n         Total != 0) %&gt;%\n  mutate(log10_tomatine = log10(Tomatine),\n         log10_FGA = log10(TotalLycoperosideFGEsculeosideA),\n         log10_total = log10(Total))\n\nalkaloids_totals_log %&gt;%\n    ggpairs(columns = c(\"log10_tomatine\", \"log10_FGA\", \"log10_total\"),\n          aes(color = Class, alpha = 0.5), # note alpha inside aes which is weird idk why\n          columnLabels = c(\"Alpha-Tomatine\", \"Lycoperoside F/G\\n Escueloside A\", \"Total Alkaloids\"))"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#manually-making-correlation-plots-with-reshapemelt-and-ggplot",
    "href": "modules/module3/08_correlations/08_correlations.html#manually-making-correlation-plots-with-reshapemelt-and-ggplot",
    "title": "Visualizing Correlations",
    "section": "Manually making correlation plots with reshape::melt() and ggplot",
    "text": "Manually making correlation plots with reshape::melt() and ggplot\nBecause some of the correlation specific packages are hard to customize, I am going to show you how to make your own plots by reshaping your data with reshape2::melt() and some base R functions, and plotting using the standard ggplot syntax.\n\nlibrary(reshape2) # contains melt()\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# take cor matrix and convert to df with 3 columns: Var1, Var2, and value\nmelted_alkaloids_cor &lt;- melt(alkaloids_cor)\n\n# what does it look like?\nhead(melted_alkaloids_cor)\n\n                  Var1              Var2       value\n1    Dehydrotomatidine Dehydrotomatidine 1.000000000\n2           Tomatidine Dehydrotomatidine 0.297446153\n3 TotalDehydrotomatine Dehydrotomatidine 0.032491778\n4             Tomatine Dehydrotomatidine 0.023823011\n5 TotalHydroxytomatine Dehydrotomatidine 0.009912624\n6 TotalAcetoxytomatine Dehydrotomatidine 0.032202892\n\n\nFirst pass minimalist plotting\n\nmelted_alkaloids_cor %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile()\n\n\n\n\nLots to fix! What if we want only the upper or lower triangle, again since this plot is symmetric.\n\nUpper triangle\nKeep only the upper triangle.\n\n# \"save as\"\nalkaloids_upper &lt;- alkaloids_cor\n\n# use function lower.tri() and set the lower triangle all to NA\n# then we can keep only the upper triangle\nalkaloids_upper[lower.tri(alkaloids_upper)] &lt;- NA\n\n# melt to go back to long format\nmelted_alkaloids_upper &lt;- melt(alkaloids_upper, na.rm = TRUE)\n\n# did it work?\nhead(melted_alkaloids_upper) # yup\n\n                   Var1                 Var2      value\n1     Dehydrotomatidine    Dehydrotomatidine 1.00000000\n11    Dehydrotomatidine           Tomatidine 0.29744615\n12           Tomatidine           Tomatidine 1.00000000\n21    Dehydrotomatidine TotalDehydrotomatine 0.03249178\n22           Tomatidine TotalDehydrotomatine 0.37446722\n23 TotalDehydrotomatine TotalDehydrotomatine 1.00000000\n\n\n\n\nLower triangle\nCreate a lower triangle object to plot.\n\n# \"save as\"\nalkaloids_lower &lt;- alkaloids_cor\n\n# use function upper.tri() and set the upper triangle all to NA\n# then we can keep only the lower triangle\nalkaloids_lower[upper.tri(alkaloids_lower)] &lt;- NA\n\n# melt to go back to long format\nmelted_alkaloids_lower &lt;- melt(alkaloids_lower, na.rm = TRUE)\n\n# did it work?\nhead(melted_alkaloids_lower) # yup\n\n                  Var1              Var2       value\n1    Dehydrotomatidine Dehydrotomatidine 1.000000000\n2           Tomatidine Dehydrotomatidine 0.297446153\n3 TotalDehydrotomatine Dehydrotomatidine 0.032491778\n4             Tomatine Dehydrotomatidine 0.023823011\n5 TotalHydroxytomatine Dehydrotomatidine 0.009912624\n6 TotalAcetoxytomatine Dehydrotomatidine 0.032202892\n\n\nPlot\n\n# remember we made alkaloid_labels\nprint(alkaloid_labels)\n\n [1] \"Dehydrotomatidine\"                                 \n [2] \"Tomatidine\"                                        \n [3] \"Dehydrotomatine\"                                   \n [4] \"Alpha-Tomatine\"                                    \n [5] \"Hydroxytomatine\"                                   \n [6] \"Acetoxytomatine\"                                   \n [7] \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\"\n [8] \"Lycoperoside F, G, \\nor Escueloside A\"             \n [9] \"Escueloside B\"                                     \n[10] \"Total Steroidal Alkaloids\"                         \n\nmelted_alkaloids_lower %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#f1a340\",\n                       mid = \"#f7f7f7\",\n                       high = \"#998ec3\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = alkaloid_labels) +\n  scale_y_discrete(labels = alkaloid_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y =\"\",\n       title = \"Correlation between steroidal alkaloids using \\nPearson's correlation coefficient\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#useful-resources",
    "href": "modules/module3/08_correlations/08_correlations.html#useful-resources",
    "title": "Visualizing Correlations",
    "section": "Useful resources",
    "text": "Useful resources\n\ncor()\nHmisc::rcorr()\nggcorrplot::ggcorrplot()\ncorrplot::corrplot()\nGGally\nGGally:ggcorr()\nGGally:ggpairs()"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(???)"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#introduction",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(???)"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#total-cupping-score-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#total-cupping-score-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "Total cupping score in arabica and robusta",
    "text": "Total cupping score in arabica and robusta\nMake 3 different visualizations that shows the distribution of total cupping score across arabica and robusta beans. Make the plots so you think they look good."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "Individual characteristic cupping scores in arabica and robusta",
    "text": "Individual characteristic cupping scores in arabica and robusta\nMake 3 different visualizations that show the distribution of all the individual contributors to total cupping score across arabica and robusta in one plot."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html",
    "href": "modules/module3/07_distributions/07_distributions.html",
    "title": "Understanding Data Distributions",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions.html#introduction",
    "title": "Understanding Data Distributions",
    "section": "Introduction",
    "text": "Introduction\nWe will will building on our lesson on ggplot101 and ggplot102 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, geoms, facets, scales, labels, and themes. Today we are going to apply what we learned towards trying to better understanding our underlying data distributions.\nOften, we think about figure generation as the last part of the scientific process, something you do as you prepare a manuscript for publication. I hope to convince you that exploring your data, and making exploratory plots is a critical part of the data analysis and interpretation process.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\n\nlibrary(tidyverse)\n\nToday we are using real research data from my group. We will be reading in the supplementary data from a paper written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 1. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloids &lt;- readxl::read_excel(\"tpg220192-sup-0002-supmat.xlsx\",\n                                sheet = \"S1 Raw Data Diversity Panel\")\n\n\nknitr::kable(head(alkaloids))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nYear\nEnvironment\nBlock\nGenotype\nPlot_Source\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nLatitude\nLongitude\nDehydrotomatidine\nTomatidine\nDehydrotomatine1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\n\n\n\n\n7805\n2018\nFreEarly18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.000000\n5.726010\n0.350331\n6.076341\n172.66244\n1.079190\n86.72742\n17.831892\n9.142607\n114.78111\n18.902399\n56.307182\n1.890053\n77.099634\n5.125904\n10.277325\n336.8893\n347.1666\n3.787979\n0.924195\n3.943230\n8.655404\n731.5675\n\n\n7898\n2017\nFre17\n2\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.169068\n0.000000\n0.000000\n0.000000\n55.47329\n0.000000\n53.32292\n13.630697\n4.841762\n71.79538\n3.557348\n4.107289\n0.000000\n7.664637\n2.905500\n5.548102\n199.6694\n205.2175\n8.978931\n1.897850\n6.794690\n17.671471\n360.8969\n\n\n7523\n2018\nFreLate18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.135675\n0.680554\n5.073552\n0.000000\n5.073552\n123.85835\n0.000000\n50.90989\n6.503939\n1.368847\n58.78268\n3.931461\n4.123222\n0.623340\n8.678023\n2.185082\n5.104115\n259.0177\n264.1218\n4.049145\n0.000000\n6.749386\n10.798531\n474.3143\n\n\n7724\n2017\nFre17\n1\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.054300\n0.497261\n19.419087\n0.000000\n19.419087\n239.01264\n0.000000\n36.02318\n8.557673\n7.483933\n52.06478\n3.341048\n16.415426\n1.057100\n20.813574\n0.000000\n0.000000\n203.0061\n203.0061\n1.678210\n0.000000\n2.349633\n4.027843\n538.8955\n\n\n7427\n2018\nFreLate18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.139454\n0.553801\n0.000000\n0.000000\n0.000000\n64.31783\n0.879435\n39.91027\n7.228388\n3.015298\n51.03339\n0.000000\n3.131685\n0.000000\n3.131685\n0.000000\n4.054211\n299.5687\n303.6229\n10.146857\n0.000000\n4.882339\n15.029197\n437.8283\n\n\n7854\n2018\nFreEarly18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.049700\n0.262174\n3.737579\n0.000000\n3.737579\n68.44913\n0.000000\n23.86864\n13.506299\n1.456982\n38.83192\n4.657902\n4.259007\n0.605729\n9.522638\n9.832149\n11.595595\n459.5205\n471.1161\n6.839930\n0.486236\n5.595751\n12.921917\n614.7233\n\n\n\n\n\nThis dataset has 605 observations, with data about different steroidal alkaloids in the fruits of different tomato germplasm grown in 3 locations across 2 years. There is also some other metadata too."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#geoms-for-distributions",
    "href": "modules/module3/07_distributions/07_distributions.html#geoms-for-distributions",
    "title": "Understanding Data Distributions",
    "section": "Geoms for distributions",
    "text": "Geoms for distributions\n\ngeom_col()\nOften, people use bar charts, representing the height or the length of the bar as proportional to the average value that it represents. These charts are sometimes called dynamite plots because they resemble (when they have an error bar with whisker) those cartoon style dynamite sticks. Pow!\nHowever, these bar charts, even if you add a standard deviation/error, really can hide the true distribution of your data, and for this reason, I and others hope you don‚Äôt select to make them.\nAside: You may be thinking ‚ÄúJess you asked us to make one of these in Module 2 homework‚Äù and I did but also that was a little different. The plot I asked you to make shows the number of degrees awarded, a value for which there really is no distribution. So in that case we are using a bar plot to show something different than a bar plot which is meant to show somehow an average/median and distribution.\nI hope after today, you see that there is always a better chart type to make than a bar chart. But I will show you how to make them anyway.\nBefore we plot, let‚Äôs calculate some summary statistics so we know what we should expect.\n\nalkaloids %&gt;%\n  group_by(Class) %&gt;%\n  summarize(mean_tomatine = mean(Tomatine))\n\n# A tibble: 5 √ó 2\n  Class                 mean_tomatine\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 Cultivated Cherry              235.\n2 Cultivated Processing          330.\n3 S. pimpinellifolium            685.\n4 Wide Cross Hybrid              534.\n5 Wild Cherry                   4928.\n\n\n\n# this is wrong but an easy mistake to make\n# this is not what we want\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_col()\n\n\n\n\nJust calling geom_col() does not give us what we want. Look at the y-axis scale and how out of line this is with our summary statistics. The reason for this is that geom_col() defaults to position = \"stack\" which will just sum the alkaloid content across all the observations. Even changing to position = \"identity\" does not work. This is because we are plotting a transformation of the data (calculation of the mean) which these geoms are not doing.\nWe can calculate manually by generating the summary values and then piping that into our ggplot call.\n\nalkaloids %&gt;%\n  group_by(Class) %&gt;%\n  summarize(mean_tomatine = mean(Tomatine)) %&gt;%\n  ggplot(aes(x = Class, y = mean_tomatine)) +\n  geom_col()\n\n\n\n\n\n\nstat_summary()\nAn easier way to do this would be just with stat_summary(), which does not require the calculation of summary statistic first.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\n\nReordering x-variables\nNote in these plots the ordering of the x-axis categories ‚Äì they are alphabetical. This is the ggplot default. There are many reasons why this might not be the most compelling ordering for your data. You may want to order from lowest to highest mean, or in this case, I want to order the tomatoes from most cultivated on the left, to most wild on the right, since this is the prevailing theme of our paper.\nWe can do this in two ways:\nSimply reorder the plot.\n\n# set what the order is\nalkaloids_order &lt;- c(\"Cultivated Processing\",\n                     \"Cultivated Cherry\",\n                     \"Wide Cross Hybrid\",\n                     \"Wild Cherry\",\n                     \"S. pimpinellifolium\")\n\n# plot and re-level within aes()\nalkaloids %&gt;%\n  ggplot(aes(x = factor(Class, levels = alkaloids_order), y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\nChange the levels of the data so the reordering happens to every plot in the future.\n\n# what type of variable is Class?\nclass(alkaloids$Class)\n\n[1] \"character\"\n\n# convert to factor, and set levels\nalkaloids$Class &lt;- factor(alkaloids$Class,\n                          levels = c(\"Cultivated Processing\",\n                                     \"Cultivated Cherry\",\n                                     \"Wide Cross Hybrid\",\n                                     \"Wild Cherry\",\n                                     \"S. pimpinellifolium\"))\n\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\nMy tendency would be to re-level the data if I always want to use the same order, and just re-level the plot if I only want to do this once or twice.\n\n\n\ngeom_boxplot()\nA boxplot has the benefit of showing you more than the median and the standard deviation, so you can better see the true distribution of your data. In geom_boxplot():\n\nlower whisker = smallest observation greater than or equal to lower hinge - 1.5 * IQR\nlower hinge/bottom line of box part of boxplot = 25% quantile\nmiddle = median, 50% quantile\nupper hinge/top line of box part of boxplot = 75% quantile\nupper whisker = largest observation less than or equal to upper hinge + 1.5 * IQR\n\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot()\n\n\n\n\nOne reason why this is really importantly different from the bar plot is look at the number of outliers we are seeing for Wild Cherry. You don‚Äôt capture this at all with the median/mean bar plots.\nBecause of the scale of this data, it might be beneficial to log transform the y-axis.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot() +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 3 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\ngeom_jitter()\ngeom_jitter() is a shortcut for geom_point(position = \"jitter\"), but is common enough that the shortcut exists. It is often nice to jitter on top of a boxplot. Note, if you don‚Äôt want the outliers from geom_boxplot() to be plotted twice, you should indicate outlier.shape = NA.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\nTransformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 3 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nJittering introduces a small amount of variation into your points so they‚Äôre easier to see. A width of 0 is no horizontal jitter. A height of 0 is no vertical jitter. Typically you don‚Äôt want veritcal jitter so that the points retain their fidelity on the y-axis (which is where their concentration is plotted). I basically always use geom_jitter(height = 0) for plots where I want to retain y-axis fidelity.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(height = 0) +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\nTransformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 3 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\ngeom_histogram()\nWe could also look at these distribution more like histograms and it provides to us some additional information. When coupled with faceting, this can be very powerful.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Tomatine)) +\n  geom_histogram(bins = 75) + # default is bins = 30\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(vars(Class))\n\n\n\n\n\nalkaloids %&gt;%\n  ggplot(aes(x = Tomatine)) +\n  geom_density() +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(vars(Class))\n\n\n\n\n\n\ngg:ridges::geom_density_ridges()\nI really like the function geom_density_ridges() which is a part of the ggplot add-on package ggridges. It allows you to create ridgeline plots to show distributes in a single non-faceted plot.\n\nlibrary(ggridges) # for ridgeline plots\nlibrary(scales) # for comma format\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +\n  geom_density_ridges(alpha = 0.5) +\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\")\n\n\n\n\nYou can also use the function geom_density_ridges() which will allow you to easily map quantiles or other functions on top of your ridges.\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +\n  stat_density_ridges(alpha = 0.5,\n                      quantile_lines = TRUE,\n                      quantiles = 2) + # break into 2 groups, therefore median\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\",\n       caption = \"Black line represents tomato class median concent\")\n\nPicking joint bandwidth of 0.198\n\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\nChanging class labels\nI am bothered by the fact that S. pimpinellifolium (a species of wild tomato) is not indicated in italics. We don‚Äôt want to italicize all of the labels, just S. pimpinellifolium. Let‚Äôs fix that.\nWe can start by creating a vector of the labels how we want them to appear in the plot.\n\nclass_labels &lt;- c(\"Cultivated Processing\", \n                  \"Cultivated Cherry\",\n                  \"Wide Cross Hybrid\", \n                  \"Wild Cherry\",\n                  expression(italic(\"S. pimpinellifolium\")))\n\nclass_labels\n\nexpression(\"Cultivated Processing\", \"Cultivated Cherry\", \"Wide Cross Hybrid\", \n    \"Wild Cherry\", italic(\"S. pimpinellifolium\"))\n\n\nThen we can use one of the scale_*() functions to change our y-axis scale labels to how we want them to be.\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +\n  geom_density_ridges(alpha = 0.5) +\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  scale_y_discrete(labels = class_labels) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\")\n\nPicking joint bandwidth of 0.198\n\n\n\n\n\nIf for example your variables were mapped to color or fill, you could do this using scale_color_manual() or scale_fill_manual(), respectively.\n\n\n\nggdist functions\nAnother ggplot extension package ggdist has cool geoms you can integrate into ggplots to visualize distributions. I think these work better than geom_dotplot().\nSometimes using geom_jitter() when you have a lot of data points can look a bit messy. I think in this case, using geom_dots() works very well. The default orientation is is layout = \"bin\"\n\nlibrary(ggdist)\n\n\nAttaching package: 'ggdist'\n\n\nThe following objects are masked from 'package:ggridges':\n\n    scale_point_color_continuous, scale_point_color_discrete,\n    scale_point_colour_continuous, scale_point_colour_discrete,\n    scale_point_fill_continuous, scale_point_fill_discrete,\n    scale_point_size_continuous\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +  \n  geom_dots() +\n  scale_x_continuous(trans = \"log10\", labels = comma) + \n  scale_y_discrete(labels = class_labels) +\n  labs(x = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       y = \"\",\n       title = \"Distribution of alkaloid content found among \\ntomatoes of different classes\")\n\n\n\n\nYou can really change the feel of the plot by changing the orientation between horizontal and vertical. If you want to use the orientation layout = \"swarm\" you need the package ggbeeswarm. This is also a nice package that performs similarly to ggdist but has less functionality which is why I‚Äôm covering ggdist here.\n\nlibrary(ggbeeswarm) # required for layout = \"swarm\"\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_dots(side = \"both\", layout = \"swarm\") + # requires ggbeeswarm\n  scale_x_discrete(labels = class_labels) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  labs(x = \"\",\n       y = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       title = \"Distribution of alkaloid content found among tomatoes of different classes\")\n\n\n\n\nYou can also use stat_dotsinterval() which will by default add the median and the interquartile range (though you can change exactly what you want to be displayed).\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_dotsinterval(side = \"both\") +\n  scale_x_discrete(labels = class_labels) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  labs(x = \"\",\n       y = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       title = \"Distribution of alkaloid content found among tomatoes of different classes\")\n\n\n\n\nDon‚Äôt forget we can keep layering. We can always map other aethetics to our plot (e.g.¬†shape = as.factor(Year), and we include as.factor() because Year is a character datatype).\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Class, y = Tomatine, shape = as.factor(Year))) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  geom_dots(side = \"both\") +\n  theme_ggdist() +\n  theme(legend.position = c(.18, .99),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(5, 5, 5, 5)) +\n  labs(shape = \"Year\",\n       y = \"Alpha-tomatine (¬µg/100 g fresh weight)\")"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#useful-resources",
    "href": "modules/module3/07_distributions/07_distributions.html#useful-resources",
    "title": "Understanding Data Distributions",
    "section": "Useful resources",
    "text": "Useful resources\n\nggplot2 cheatsheet\nggplot2 documentation\nggplot2: elegant graphics for data analysis by Hadley Wickham\nA really compehensive list of resources compiled by Erik Gahner Larsen\nggridges\nggdist\nggbeeswarm\nPast ggplot Code Clubs:\n\nVisualizing Data by Michael Broe\nggplot round 2 by me\nFaceting, multi-plots, and animating\nVisualizing Data by Michael Broe a second one\nggplot round 2 a second one by me"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(rstatix)\nlibrary(ggpubr)\nlibrary(glue)\nlibrary(rcompanion)"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html#introduction",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(rstatix)\nlibrary(ggpubr)\nlibrary(glue)\nlibrary(rcompanion)"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "1. Is total cholesterol (TotChol) different by age (AgeDecade)?",
    "text": "1. Is total cholesterol (TotChol) different by age (AgeDecade)?\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint - you want to test your assumptions to see what tests to do. You might need to use different posthoc comparison methods than we did in class.\n\n\n\n\n\n\n\n\n\nNeed another hint? (Click to expand)\n\n\n\n\n\nAnother hint - the function rcompanion::cldList() will convert the resulting comparison table from a posthoc Dunn test to create a column with the letters indicating which groups are significantly different from each other."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html",
    "href": "modules/module4/10_pca/10_pca.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Today we are going to start Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\nPCA is a data reduction approach, and useful if you have many variables, for example, thousands of genes or metabolites. PCA creates summary variables (the principal components) which maximize the variation in the dataset. It can be categorized as an unsupervised approach, as PCA doesn‚Äôt know which samples belong to your different groups. When you look at a scores plot, points that are closer together are more similar based on your input data, and those further apart are more different. The location of the loadings helps you understand what is driving those differences in your scores plot.\nIf you are unfamiliar with PCA, I‚Äôd recommend these two Youtube videos by Josh Starmer of StatQuest which explain PCA in 5 mins, or with more detail in 20 min. Bam üí•!\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\n\n\n\nToday we are going to continue to use the same real research data from my group from the lessons on distributions and correlations. We will be reading in the supplementary data from a paper from my group written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 3. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloid_blups &lt;- read_excel(\"data/tpg220192-sup-0002-supmat.xlsx\",\n                             sheet = \"S3 BLUP Diversity Panel\")\n\nLet‚Äôs take a look at this new data sheet.\n\nknitr::kable(head(alkaloid_blups))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenotype\nPlot_Source\nSpecies\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nDehydrotomatidine\nTomatidine\nDehydrotomatineA1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\nLatitude\nLongitude\n\n\n\n\nCULBPT_05_11\n2K9-8584\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n0.0001504\n0.0043495\n0.0072241\n-0.0010738\n0.0061503\n0.1929106\n0.0102521\n0.1596566\n0.0352666\n0.0864042\n0.2917962\n-0.0680508\n-0.1242448\n0.0117481\n-0.1805475\n-0.0369856\n0.0056573\n-0.1693034\n-0.1636462\n-0.0340685\n-0.0061713\n-0.0371543\n-0.0773942\n0.0365648\n40.712800000000001\n-74.006\n\n\nCULBPT_05_15\n2K9-8622\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n-0.0001716\n0.0086547\n0.0000000\n0.0086547\n0.2824926\n0.0006910\n0.1118818\n0.0243950\n0.0189345\n0.1559021\n0.0038232\n0.0467380\n0.0024737\n0.0530349\n0.0066251\n0.0125334\n0.3417664\n0.3542998\n0.0138429\n0.0021833\n0.0139309\n0.0299571\n0.8907677\n40.712800000000001\n-74.006\n\n\nCULBPT_05_22\n2K17-7708-1\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000649\n-0.0002516\n0.0085205\n0.0000000\n0.0085205\n0.1755250\n-0.0001804\n0.0505547\n0.0040900\n0.0023229\n0.0576202\n0.0078240\n0.0069610\n-0.0001145\n0.0146705\n0.0031468\n0.0150674\n0.3840689\n0.3991363\n0.0007128\n0.0008227\n0.0028133\n0.0043489\n0.6618186\n40.712800000000001\n-74.006\n\n\nCULBPT04_1\n2K9-8566\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n0.0001259\n-0.0038737\n0.0000000\n-0.0038737\n-0.0061446\n0.0015930\n0.1059454\n0.0195829\n0.0083566\n0.1354779\n0.0098585\n0.0095344\n0.0007941\n0.0201870\n0.0049377\n0.0100416\n0.3466282\n0.3566699\n0.0100830\n0.0006432\n0.0089015\n0.0196277\n0.5269806\n40.712800000000001\n-74.006\n\n\nE6203\n2K9-8600\nProcessing\nCultivated Processing\nUSA\nCA\nSLL_processing_1\nSLL_processing_1_1\nSLL\nSLL_processing_CA\nArid\n-0.0000272\n0.0000159\n0.0099538\n0.0000000\n0.0099538\n0.2929606\n0.0000000\n0.0227075\n0.0039160\n0.0088279\n0.0354514\n0.0002365\n0.0395374\n0.0041794\n0.0439532\n0.0027439\n0.0133225\n0.3018732\n0.3151958\n0.0080894\n-0.0003574\n0.0100045\n0.0177365\n0.7179839\n36.778300000000002\n-119.4179\n\n\nF06-2041\n2K16-9843\nProcessing\nCultivated Processing\nUSA\nOH\nSLL_processing_1\nSLL_processing_1_3\nSLL\nSLL_processing_OH\nHumid\n-0.0000272\n-0.0001648\n-0.0011236\n0.0000000\n-0.0011236\n0.0169681\n0.0011237\n0.0195573\n-0.0016308\n-0.0016069\n0.0188454\n-0.0004237\n0.0271524\n-0.0005286\n0.0262002\n0.0053084\n0.0049033\n0.1647694\n0.1696728\n-0.0006298\n-0.0003074\n0.0019315\n0.0009943\n0.2352713\n40.417299999999997\n-82.9071\n\n\n\n\n\nWhat are the dimensions of this dataframe?\n\ndim(alkaloid_blups)\n\n[1] 107  37\n\n\n\n\n\nHere we have the best linear unbiased predictors (BLUPs) representing the alkaloid content of 107 genotypes of tomatoes. There is extra meta-data here we won‚Äôt use, so like we did in correlations, we are going to create a vector to indicate which column name reprents the alkaloids we want to include in our principal components analysis. Then we can create a new trimmed dataframe.\n\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\nalkaloid_blups_trim &lt;- alkaloid_blups %&gt;%\n  select(Genotype, Species, Class, all_of(alkaloid_total_names))\n\n# did it work?\ncolnames(alkaloid_blups_trim) # yes\n\n [1] \"Genotype\"                                \n [2] \"Species\"                                 \n [3] \"Class\"                                   \n [4] \"Dehydrotomatidine\"                       \n [5] \"Tomatidine\"                              \n [6] \"TotalDehydrotomatine\"                    \n [7] \"Tomatine\"                                \n [8] \"TotalHydroxytomatine\"                    \n [9] \"TotalAcetoxytomatine\"                    \n[10] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[11] \"TotalLycoperosideFGEsculeosideA\"         \n[12] \"TotalEsculeosideB\"                       \n[13] \"Total\""
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#introduction",
    "href": "modules/module4/10_pca/10_pca.html#introduction",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Today we are going to start Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\nPCA is a data reduction approach, and useful if you have many variables, for example, thousands of genes or metabolites. PCA creates summary variables (the principal components) which maximize the variation in the dataset. It can be categorized as an unsupervised approach, as PCA doesn‚Äôt know which samples belong to your different groups. When you look at a scores plot, points that are closer together are more similar based on your input data, and those further apart are more different. The location of the loadings helps you understand what is driving those differences in your scores plot.\nIf you are unfamiliar with PCA, I‚Äôd recommend these two Youtube videos by Josh Starmer of StatQuest which explain PCA in 5 mins, or with more detail in 20 min. Bam üí•!\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\n\n\n\nToday we are going to continue to use the same real research data from my group from the lessons on distributions and correlations. We will be reading in the supplementary data from a paper from my group written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 3. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloid_blups &lt;- read_excel(\"data/tpg220192-sup-0002-supmat.xlsx\",\n                             sheet = \"S3 BLUP Diversity Panel\")\n\nLet‚Äôs take a look at this new data sheet.\n\nknitr::kable(head(alkaloid_blups))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenotype\nPlot_Source\nSpecies\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nDehydrotomatidine\nTomatidine\nDehydrotomatineA1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\nLatitude\nLongitude\n\n\n\n\nCULBPT_05_11\n2K9-8584\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n0.0001504\n0.0043495\n0.0072241\n-0.0010738\n0.0061503\n0.1929106\n0.0102521\n0.1596566\n0.0352666\n0.0864042\n0.2917962\n-0.0680508\n-0.1242448\n0.0117481\n-0.1805475\n-0.0369856\n0.0056573\n-0.1693034\n-0.1636462\n-0.0340685\n-0.0061713\n-0.0371543\n-0.0773942\n0.0365648\n40.712800000000001\n-74.006\n\n\nCULBPT_05_15\n2K9-8622\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n-0.0001716\n0.0086547\n0.0000000\n0.0086547\n0.2824926\n0.0006910\n0.1118818\n0.0243950\n0.0189345\n0.1559021\n0.0038232\n0.0467380\n0.0024737\n0.0530349\n0.0066251\n0.0125334\n0.3417664\n0.3542998\n0.0138429\n0.0021833\n0.0139309\n0.0299571\n0.8907677\n40.712800000000001\n-74.006\n\n\nCULBPT_05_22\n2K17-7708-1\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000649\n-0.0002516\n0.0085205\n0.0000000\n0.0085205\n0.1755250\n-0.0001804\n0.0505547\n0.0040900\n0.0023229\n0.0576202\n0.0078240\n0.0069610\n-0.0001145\n0.0146705\n0.0031468\n0.0150674\n0.3840689\n0.3991363\n0.0007128\n0.0008227\n0.0028133\n0.0043489\n0.6618186\n40.712800000000001\n-74.006\n\n\nCULBPT04_1\n2K9-8566\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n0.0001259\n-0.0038737\n0.0000000\n-0.0038737\n-0.0061446\n0.0015930\n0.1059454\n0.0195829\n0.0083566\n0.1354779\n0.0098585\n0.0095344\n0.0007941\n0.0201870\n0.0049377\n0.0100416\n0.3466282\n0.3566699\n0.0100830\n0.0006432\n0.0089015\n0.0196277\n0.5269806\n40.712800000000001\n-74.006\n\n\nE6203\n2K9-8600\nProcessing\nCultivated Processing\nUSA\nCA\nSLL_processing_1\nSLL_processing_1_1\nSLL\nSLL_processing_CA\nArid\n-0.0000272\n0.0000159\n0.0099538\n0.0000000\n0.0099538\n0.2929606\n0.0000000\n0.0227075\n0.0039160\n0.0088279\n0.0354514\n0.0002365\n0.0395374\n0.0041794\n0.0439532\n0.0027439\n0.0133225\n0.3018732\n0.3151958\n0.0080894\n-0.0003574\n0.0100045\n0.0177365\n0.7179839\n36.778300000000002\n-119.4179\n\n\nF06-2041\n2K16-9843\nProcessing\nCultivated Processing\nUSA\nOH\nSLL_processing_1\nSLL_processing_1_3\nSLL\nSLL_processing_OH\nHumid\n-0.0000272\n-0.0001648\n-0.0011236\n0.0000000\n-0.0011236\n0.0169681\n0.0011237\n0.0195573\n-0.0016308\n-0.0016069\n0.0188454\n-0.0004237\n0.0271524\n-0.0005286\n0.0262002\n0.0053084\n0.0049033\n0.1647694\n0.1696728\n-0.0006298\n-0.0003074\n0.0019315\n0.0009943\n0.2352713\n40.417299999999997\n-82.9071\n\n\n\n\n\nWhat are the dimensions of this dataframe?\n\ndim(alkaloid_blups)\n\n[1] 107  37\n\n\n\n\n\nHere we have the best linear unbiased predictors (BLUPs) representing the alkaloid content of 107 genotypes of tomatoes. There is extra meta-data here we won‚Äôt use, so like we did in correlations, we are going to create a vector to indicate which column name reprents the alkaloids we want to include in our principal components analysis. Then we can create a new trimmed dataframe.\n\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\nalkaloid_blups_trim &lt;- alkaloid_blups %&gt;%\n  select(Genotype, Species, Class, all_of(alkaloid_total_names))\n\n# did it work?\ncolnames(alkaloid_blups_trim) # yes\n\n [1] \"Genotype\"                                \n [2] \"Species\"                                 \n [3] \"Class\"                                   \n [4] \"Dehydrotomatidine\"                       \n [5] \"Tomatidine\"                              \n [6] \"TotalDehydrotomatine\"                    \n [7] \"Tomatine\"                                \n [8] \"TotalHydroxytomatine\"                    \n [9] \"TotalAcetoxytomatine\"                    \n[10] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[11] \"TotalLycoperosideFGEsculeosideA\"         \n[12] \"TotalEsculeosideB\"                       \n[13] \"Total\""
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#run-pca",
    "href": "modules/module4/10_pca/10_pca.html#run-pca",
    "title": "Principal Components Analysis",
    "section": "Run PCA",
    "text": "Run PCA\nThere are many packages that have functions that run PCA (including ) but I think the most common function used is a part of base R, and is called prcomp().\n\n\n\n\n\n\nWarning\n\n\n\nNote, PCA will allow zeroes, but will throw an error if you feed it NAs.\n\n\n\nalkaloids_pca &lt;- prcomp(alkaloid_blups_trim[,-c(1:3)],\n                        scale = TRUE, # default is false\n                        center = TRUE) # default is true, just being explicit\n\nLet‚Äôs investigate alkaloids_pca.\n\nglimpse(alkaloids_pca)\n\nList of 5\n $ sdev    : num [1:10] 1.794 1.732 1.215 0.99 0.776 ...\n $ rotation: num [1:10, 1:10] 0.162 0.309 0.422 0.326 0.311 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n  .. ..$ : chr [1:10] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:10] 0.000169 0.00721 0.142798 1.865975 1.323755 ...\n  ..- attr(*, \"names\")= chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n $ scale   : Named num [1:10] 0.000505 0.024096 0.339379 5.889986 2.91824 ...\n  ..- attr(*, \"names\")= chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n $ x       : num [1:107, 1:10] -1.51 -1.5 -1.55 -1.55 -1.52 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:10] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\n\nprint(alkaloids_pca)\n\nStandard deviations (1, .., p=10):\n [1] 1.7944141686 1.7318715307 1.2151400892 0.9904494488 0.7763914595\n [6] 0.6695313752 0.4394142511 0.2041300753 0.1932182462 0.0001503975\n\nRotation (n x k) = (10 x 10):\n                                               PC1         PC2         PC3\nDehydrotomatidine                        0.1621938  0.05363402 -0.06746652\nTomatidine                               0.3088504 -0.13094437 -0.44243776\nTotalDehydrotomatine                     0.4222731 -0.30216776 -0.22116400\nTomatine                                 0.3263804 -0.28919346 -0.43424991\nTotalHydroxytomatine                     0.3111090 -0.07515005  0.42780515\nTotalAcetoxytomatine                     0.3190534 -0.17396967  0.54287286\nDehydrolycoperosideFGdehydroesculeosideA 0.2125680  0.49447329 -0.07346836\nTotalLycoperosideFGEsculeosideA          0.2130280  0.52056383 -0.08463631\nTotalEsculeosideB                        0.1864604  0.50165801 -0.10122926\nTotal                                    0.5191805  0.04443498  0.24834278\n                                                 PC4          PC5         PC6\nDehydrotomatidine                         0.92897283 -0.275272461 -0.09845544\nTomatidine                                0.13150651  0.482582095  0.65879800\nTotalDehydrotomatine                     -0.16555038 -0.170452131 -0.15777601\nTomatine                                 -0.13638091 -0.163532836 -0.41927007\nTotalHydroxytomatine                      0.15203796  0.706002906 -0.41591986\nTotalAcetoxytomatine                     -0.05462895 -0.326082234  0.41704857\nDehydrolycoperosideFGdehydroesculeosideA -0.16278360  0.013643330 -0.06919125\nTotalLycoperosideFGEsculeosideA          -0.09078436 -0.009476836 -0.02310538\nTotalEsculeosideB                         0.02495982 -0.037371129  0.01529154\nTotal                                    -0.11066018 -0.170588385  0.05600981\n                                                 PC7         PC8          PC9\nDehydrotomatidine                         0.13097525 -0.02201625 -0.012909093\nTomatidine                                0.03505366  0.05778857  0.054315300\nTotalDehydrotomatine                      0.03480557 -0.61023340 -0.475709677\nTomatine                                 -0.09303946  0.40353582  0.403874861\nTotalHydroxytomatine                     -0.04702814 -0.04023463  0.009165434\nTotalAcetoxytomatine                     -0.01377758 -0.02258828  0.182475550\nDehydrolycoperosideFGdehydroesculeosideA  0.61727735 -0.31823996  0.437114636\nTotalLycoperosideFGEsculeosideA           0.10012665  0.42335087 -0.583833541\nTotalEsculeosideB                        -0.76029219 -0.28461575  0.203734859\nTotal                                    -0.01570365  0.31194871 -0.025467058\n                                                  PC10\nDehydrotomatidine                        -0.0001604947\nTomatidine                                0.0010655927\nTotalDehydrotomatine                      0.0147128823\nTomatine                                  0.2559833151\nTotalHydroxytomatine                      0.1268345182\nTotalAcetoxytomatine                      0.5059522400\nDehydrolycoperosideFGdehydroesculeosideA  0.0080094872\nTotalLycoperosideFGEsculeosideA           0.3707973486\nTotalEsculeosideB                         0.0226436777\nTotal                                    -0.7239562737\n\n\n\nclass(alkaloids_pca)\n\n[1] \"prcomp\"\n\n\nWe can see that the resulting PCA object is a prcomp object, and is a list of 5 lists and vectors.\nThis includes:\n\nsdev: the standard deviations (square roots of the eigenvalues of the covariance matrix) of the principal components\nrotation: the PCs for the variables (i.e., the variable loadings)\nx: the PCs for samples (i.e., the scores)\ncenter: the centering used\nscale: the scaling used\n\nWe can also look at the output of our PCA in a different way using the function summary().\n\nsummary(alkaloids_pca) \n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.794 1.7319 1.2151 0.9904 0.77639 0.66953 0.43941\nProportion of Variance 0.322 0.2999 0.1477 0.0981 0.06028 0.04483 0.01931\nCumulative Proportion  0.322 0.6219 0.7696 0.8677 0.92796 0.97279 0.99210\n                           PC8     PC9      PC10\nStandard deviation     0.20413 0.19322 0.0001504\nProportion of Variance 0.00417 0.00373 0.0000000\nCumulative Proportion  0.99627 1.00000 1.0000000\n\n\nWe can convert this summary into something later usable by extraction the element importance from summary(alkaloids_pca) and converting it to a dataframe.\n\nimportance &lt;- summary(alkaloids_pca)$importance %&gt;%\n  as.data.frame()\n\nknitr::kable(head(importance))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\n\n\n\n\nStandard deviation\n1.794414\n1.731872\n1.21514\n0.9904494\n0.7763915\n0.6695314\n0.4394143\n0.2041301\n0.1932182\n0.0001504\n\n\nProportion of Variance\n0.321990\n0.299940\n0.14766\n0.0981000\n0.0602800\n0.0448300\n0.0193100\n0.0041700\n0.0037300\n0.0000000\n\n\nCumulative Proportion\n0.321990\n0.621930\n0.76959\n0.8676900\n0.9279600\n0.9727900\n0.9921000\n0.9962700\n1.0000000\n1.0000000\n\n\n\n\n\nBy looking at the summary we can see, for example, that the first two PCs explain 62.19% of variance.\nWe are going to go over making scree, scores and loadings plots using helper functions (here, they start fviz_() and come from the package factoextra, and manually via ggplot. The helper functions allow you look at each plot type simply. This is an important step because when you make your plots with ggplot, you want to be sure they look how they should."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#scree-plot",
    "href": "modules/module4/10_pca/10_pca.html#scree-plot",
    "title": "Principal Components Analysis",
    "section": "Scree plot",
    "text": "Scree plot\nA scree plot shows what percentage of total variance is explained by each principal component.\n\nUsing fviz_eig()\nWe can do this quickly using fviz_eig().\n\nfviz_eig(alkaloids_pca)\n\n\n\n\nYou can actually do this very easily with base R plotting as well. If you weren‚Äôt planning to publish this type of plot, it might not be important it look beautiful, and then both of these options would be great and quick. Note though that the base R plot is plotting at a different scale.\n\nplot(alkaloids_pca)\n\n\n\n\n\n\nManually\nIf you wanted to make a scree plot manually, you could by plotting using a wrangled version of the importance dataframe we made earlier.\n\nimportance_tidy &lt;- importance %&gt;%\n  rownames_to_column(var = \"measure\") %&gt;%\n  pivot_longer(cols = PC1:PC10,\n               names_to = \"PC\",\n               values_to = \"value\")\n\nimportance_tidy %&gt;%\n  filter(measure == \"Proportion of Variance\") %&gt;%\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col()\n\n\n\n\nAlmost! PC10 is displaying right after PC1 because alphabetically, this is the order. Let‚Äôs fix it.\n\n# create a vector with the order we want\nmy_order &lt;- colnames(importance)\n\n# relevel according to my_order\nimportance_tidy$PC &lt;- factor(importance_tidy$PC, levels = my_order)\n\n# check to see if it worked\nlevels(importance_tidy$PC)\n\n [1] \"PC1\"  \"PC2\"  \"PC3\"  \"PC4\"  \"PC5\"  \"PC6\"  \"PC7\"  \"PC8\"  \"PC9\"  \"PC10\"\n\n\nLet‚Äôs plot again.\n\nimportance_tidy %&gt;%\n  filter(measure == \"Proportion of Variance\") %&gt;%\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col()\n\n\n\n\nSuccess!\nIf we want to tighten up this plot we can.\n\nimportance_tidy %&gt;%\n  filter(measure == \"Proportion of Variance\") %&gt;%\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col(alpha = 0.1, color = \"black\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  labs(x = \"Principal component\",\n       y = \"Percent variance explained\",\n       title = \"Scree plot of 10 alkaloids analyzed across 107 tomato accessions\")\n\n\n\n\nThis is a perfectly ready scree plot for the supplementary materials of a publication."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#scores-plot",
    "href": "modules/module4/10_pca/10_pca.html#scores-plot",
    "title": "Principal Components Analysis",
    "section": "Scores plot",
    "text": "Scores plot\nWhen people talk about PCA plots, what they most often mean is PCA scores plots. Here, each point represents a sample, and we are plotting their coordinates typically for the first 2 PCs. Sometimes people make 3D PCA plots with the first 3 PCs but I think these are not easy to look in 2D and I wouldn‚Äôt recommend you to put them in your papers.\n\nUsing fviz_pca_ind()\nWe can also look at a scores plot using fviz_pca_ind() where ind means individuals. Here, each point is a sample.\n\nfviz_pca_ind(alkaloids_pca)\n\n\n\n\nBecause our alkaloids_pca doesn‚Äôt have any meta-data, this is a hard to interpret plot, where each number indicates the rownumber of that sample. Making the scores plot this way is useful because it shows us the shape of the plot which we can use to confirm that we have made a ggplot that looks like its been created correctly.\n\n\nManually\nWe want to plot the scores, which are in provided in alkaloids_pca$x.\nWe can convert the list into a dataframe of scores values by using as.data.frame(). Then we can bind back our relevant metadata so they‚Äôre all together. Note, to use bind_cols() both datasets need to be in the same order. In this case they are so we are good.\n\n# create a df of alkaloids_pca$x\nscores_raw &lt;- as.data.frame(alkaloids_pca$x)\n\n# bind meta-data\nscores &lt;- bind_cols(alkaloid_blups[,1:3], # first 3 columns\n                    scores_raw)\n\nNow we can plot.\n\nscores %&gt;%\n  ggplot(aes(x = PC1, y = PC2, color = Species)) +\n  geom_point() \n\n\n\n\nOur shapes are looking the same, this is good. Let‚Äôs pretty up our plot.\n\n# create objects indicating percent variance explained by PC1 and PC2\nPC1_percent &lt;- round((importance[2,1])*100, # index 2nd row, 1st column, times 100\n                     1) # round to 1 decimal\nPC2_percent &lt;- round((importance[2,2])*100, 1) \n\n# plot\n(scores_plot &lt;- scores %&gt;%\n  ggplot(aes(x = PC1, y = PC2, fill = Species)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\", size = 2.5, alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Scores Plot of 10 Alkaloids Present in 107 Tomato Accessions\"))\n\n\n\n\nThis looks nice."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#loadings-plot",
    "href": "modules/module4/10_pca/10_pca.html#loadings-plot",
    "title": "Principal Components Analysis",
    "section": "Loadings plot",
    "text": "Loadings plot\n\nUsing fviz_pca_var()\nWe can also look at a loadings plot using fviz_pca_var() where var means variables. Here, each point is a variable.\n\nfviz_pca_var(alkaloids_pca)\n\n\n\n\n\n\nManually\nWe can also make a more customized loadings plot manually using ggplot and using the dataframe alkaloids_pca$rotation.\n\n# grab raw loadings, without any metadata\nloadings_raw &lt;- as.data.frame(alkaloids_pca$rotation)\n\nloadings &lt;- loadings_raw %&gt;%\n  rownames_to_column(var = \"alkaloid\")\n\nWe can then plot with ggplot like normal.\n\nloadings %&gt;%\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_text() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot\") \n\n\n\n\nWe have two problems with this plot.\n\nThe names are abbreviated and not how we want them to appear\nThe label names are on top of the points/each other\n\nWe can fix both of these problems.\nWe can create a vector of the labels as we want them to appear, as we have done previously.\n\nalkaloid_labels &lt;- c(\"Dehydrotomatidine\",\n                     \"Tomatidine\",\n                     \"Dehydrotomatine\",\n                     \"Alpha-Tomatine\",\n                     \"Hydroxytomatine\",\n                     \"Acetoxytomatine\",\n                     \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\",\n                     \"Lycoperoside F, G, \\nor Escueloside A\",\n                     \"Escueloside B\",\n                     \"Total Steroidal Alkaloids\")\n\nThen we can re-plot with these labels.\n\nloadings %&gt;%\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_point() +\n  geom_text() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot\") \n\n\n\n\nOk the label names are better but they‚Äôre still smushed. The package ggrepel has some good functions to help us. You can try using geom_text_repel() and geom_label_repel().\n\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\nWith geom_text_repel()\n\nlibrary(ggrepel)\n\n(loadings_plot &lt;- loadings %&gt;%\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +  \n  geom_point() +\n  geom_text_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot using geom_text_repel()\"))\n\n\n\n\nWith geom_label_repel()\n\nloadings %&gt;%\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_label_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot using geom_label_repel()\")"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#patchwork",
    "href": "modules/module4/10_pca/10_pca.html#patchwork",
    "title": "Principal Components Analysis",
    "section": "patchwork",
    "text": "patchwork\nYou can pop these two plots side by side easing using the package patchwork.\n\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\n\nlibrary(patchwork)\n\nscores_plot + loadings_plot\n\n\n\n\nAdding the plots was easy, but now we see that they scaling of the plots is not exactly the same and doesn‚Äôt promote easy comparison. Let‚Äôs fix that.\n\n# setting the range of the plot\n(scores_plot_ranged &lt;- scores_plot +\n  coord_cartesian(xlim = c(-2, 6.5), ylim = c(-5, 6.5)))\n\n\n\n# what is the ratio of the space on each side of the axis for the scores plot?\n(x_ratio &lt;- 2/(2 + 6.5))\n\n[1] 0.2352941\n\n(y_ratio &lt;- 5/(5 + 6.5))\n\n[1] 0.4347826\n\n# check the ending range for the loadings plot\n# 0.6 units looks good for both x and y\n# what should the low range value be so that both plots are equally scaled?\n\n# making the loadings plot match this range\n(loadings_plot_ranged &lt;- loadings_plot +\n  coord_cartesian(xlim = c(-0.1846, 0.6), ylim = c(-0.4602, 0.6)))\n\n\n\n\nPlot\n\nscores_plot_ranged + loadings_plot_ranged"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#biplot",
    "href": "modules/module4/10_pca/10_pca.html#biplot",
    "title": "Principal Components Analysis",
    "section": "Biplot",
    "text": "Biplot\n\nUsing fviz_pca().\nYou can make a biplot quickly with fviz_pca(). Note, fviz_pca_biplot() and fviz_pca() are the same.\n\nfviz_pca(alkaloids_pca)\n\n\n\n\nInstead of making this plot manually, let‚Äôs go through how to alter the existing plot made with fviz_pca(). We can do this because factoextra creates ggplot objects. To start off, we need to be using a dataframe that includes our metadata.\n\nfviz_pca(alkaloids_pca, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = \"text\") +\n  geom_point(aes(fill = alkaloid_blups$Species), shape = 21) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot Plot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       fill = \"Species\")\n\n\n\n\nThis is almost what we want - except we have only the abbreviated names for the alkaloids. Since in a biplot, we are really plotting two different sets of data (the scores and the loadings)there isn‚Äôt the ability to use labeller or similar with fviz_pca for the loadings only. There is a workaround though, we can go into our PCA object, change the rownames of alkaloids_pca$rotation to be our longer labels, and that should inherit to our new plot.\n\n# save as a new df\nalkaloids_pca_labelled &lt;- alkaloids_pca\n\n# assign alkaloid_labels to rownames\nrownames(alkaloids_pca_labelled$rotation) &lt;- alkaloid_labels\n\n# plot\nfviz_pca(alkaloids_pca_labelled, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = c(\"text\", \"point\"),\n         col.var = \"black\") +\n  geom_point(aes(fill = alkaloid_blups$Species), shape = 21) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot Plot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       fill = \"Species\")\n\n\n\n\nVoila."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#useful-resources",
    "href": "modules/module4/10_pca/10_pca.html#useful-resources",
    "title": "Principal Components Analysis",
    "section": "Useful resources",
    "text": "Useful resources\n\nCode club about PCA by Jelmer Poelstra\nfactoextra\n\n\nggpattern::geom_density_pattern()\n\ngeom_density_pattern: na.rm = FALSE, orientation = NA\nstat_density: na.rm = FALSE, orientation = NA\nposition_identity"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html",
    "href": "modules/module4/10_pca/10_pca_recitation.html",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands compiled by f-imp and posted to Github.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#introduction",
    "href": "modules/module4/10_pca/10_pca_recitation.html#introduction",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands compiled by f-imp and posted to Github.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#run-a-pca",
    "href": "modules/module4/10_pca/10_pca_recitation.html#run-a-pca",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "1. Run a PCA",
    "text": "1. Run a PCA"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "2. Make a scree plot of the percent variance explained by each component",
    "text": "2. Make a scree plot of the percent variance explained by each component"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "3. Make a scores plot of samples, coloring each sample by its brand",
    "text": "3. Make a scores plot of samples, coloring each sample by its brand"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-loadings-plot-of-samples",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-loadings-plot-of-samples",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "4. Make a loadings plot of samples",
    "text": "4. Make a loadings plot of samples"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "href": "modules/module4/10_pca/10_pca_recitation.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together.",
    "text": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together."
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation.html",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation.html",
    "title": "Manhattan Plots Recitation",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation.html#introduction",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation.html#introduction",
    "title": "Manhattan Plots Recitation",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation.html#investigate-your-data.",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation.html#investigate-your-data.",
    "title": "Manhattan Plots Recitation",
    "section": "Investigate your data.",
    "text": "Investigate your data.\n\nWhat are your columns?\n\n\nHow many markers are there?\n\n\nHow are the markers distributed across the chromosomes?"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation.html#make-a-manhattan-plot.",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation.html#make-a-manhattan-plot.",
    "title": "Manhattan Plots Recitation",
    "section": "Make a Manhattan plot.",
    "text": "Make a Manhattan plot.\nColor by chromosome, make sure the x-axis breaks are appropriate, be sure your y-axis is -log10 pvalue. Label the top 3 most significant points with their SNP number."
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "",
    "text": "Get familiar with plotly\nDiscuss options for interactive plots"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#installing-plotly",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#installing-plotly",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Installing plotly",
    "text": "Installing plotly\n\n# From CRAN\ninstall.packages(\"plotly\")\n\n# From github\ndevtools::install_github(\"ropensci/plotly\")"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#ggplotly",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#ggplotly",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "ggplotly()",
    "text": "ggplotly()\n\n\n\n\n\n\n\n\n\nImage retrieved from this blog."
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#description",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#description",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Description",
    "text": "Description\nThis function converts a ggplot2::ggplot() object to a plotly object.\nggplotly(\n  p = ggplot2::last_plot(),\n  width = NULL,\n  height = NULL,\n  tooltip = \"all\",\n  dynamicTicks = FALSE,\n  layerData = 1,\n  originalData = TRUE,\n  source = \"A\",\n  ...\n)\nNow, we are going to explore using plotly on ggplot objects using ggplotly()."
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#basic-scatter-plot",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#basic-scatter-plot",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Basic scatter plot",
    "text": "Basic scatter plot\n\nggpenguins &lt;- palmerpenguins::penguins %&gt;% \n  ggplot(aes( bill_length_mm , body_mass_g, color = species )) + \n  geom_point() + \n  scale_color_d3() + # from ggsci\n  theme_bw() +\n  labs(x = \"Bill length, in mm\",\n       y = \"Body mass, in g\",\n       fill = \"Species\",\n       title = \"Palmer penguin body mass by bill length\")\n\nggpenguins\n\n\n\n\nThings you can do with an interactive plot:\n\nZoom in and out\nTurn off groups (click on the legend)\nDownload plot as .png\n\n\nggplotly(ggpenguins)"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#pca",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#pca",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "PCA",
    "text": "PCA\n\n# Library here just to make emphasis \nlibrary(ggfortify)\n\nIn this case we are going to explore a new library ggfortify that accepts prcomp results and creates an automatic scores PCA plot.\nAfter you perform PCA analysis, you can use autorplot() to create a ggplot2 object for using in the ggplotly() function.\n\ndf &lt;- iris[1:4] # Extract numeric variables\n\npca_res &lt;- prcomp(df, scale. = TRUE) # Making PCA\n\np &lt;- autoplot(pca_res, data = iris, colour = 'Species') + # PCA autoplot\n  scale_color_d3() +\n  labs(title = \"Scores plot of iris data\")\n\nggplotly(p)"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#boxplots",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#boxplots",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Boxplots",
    "text": "Boxplots\nData used here is midwest from ggplot2 which contains demographic information from census data collected in the 2000 US census. The variable percollege includes what percentage of the respondents are college educated.\n\nboxplot &lt;- ggplot(midwest, aes(state, percollege, color = state) ) + \n  geom_boxplot() +\n  scale_color_d3() +\n  theme_bw() + \n  coord_flip() +\n  labs(title = \"Percentage of college educated respondents\")\n\nggplotly(boxplot)"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#barplots",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#barplots",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Barplots",
    "text": "Barplots\n\nstack_barplot &lt;- ggplot(mpg, aes(class))   + \n  geom_bar(aes(fill = drv)) + \n  scale_fill_d3() + \n  theme_bw() +\n  labs(title = \"Class of cars in the mpg datase\")\n\nggplotly(stack_barplot)"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#tooltip",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#tooltip",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Tooltip",
    "text": "Tooltip\nTooltip is an argument that controls the text that is shown when you hover the mouse over data. By default, all aes mapping variables are shown. You can modify the order and the variables that are shown in the tooltip.\nIn the penguins data we have more variables that may want to include in the text shown in our plotly plot such as sex and island.\n\n# dt[seq(10),] subset the ten first row and then use glimpse to shorten the output\nglimpse(palmerpenguins::penguins[seq(10), ]) \n\nRows: 10\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n\n\n\nggplotly(ggpenguins)\n\n\n\n\n\n\n‚Äúcolour‚Äù is requiered and ‚Äúcolor‚Äù is not supported in ggplotly()\n\n\nggplotly(ggpenguins,\n         tooltip = c(\"colour\") )"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#changing-hover-details",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#changing-hover-details",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Changing hover details",
    "text": "Changing hover details\nYou might not like the default hover text aesthetics, and can change them! You can do this using style and layout and adding these functions using the pipe %&gt;%.\nCode taken from BioDash\n\n# setting fonts for the plot\nfont &lt;- list(\n  family = \"Arial\",\n  size = 15,\n  color = \"white\")\n\n# setting hover label specs\nlabel &lt;- list(\n  bgcolor = \"#3d1b40\",\n  bordercolor = \"transparent\",\n  font = font) # we can do this bc we already set font\n\n# amending our ggplotly call to include new fonts and hover label specs\nggplotly(ggpenguins, tooltip = \"colour\") %&gt;%\n  style(hoverlabel = label) %&gt;%\n  layout(font = font)"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots.html#helpful-resources",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots.html#helpful-resources",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Helpful resources",
    "text": "Helpful resources\n\nUsing different fonts: link\nggfortify: link\nggsci: link"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html",
    "href": "modules/module4/13_leftovers/13_leftovers.html",
    "title": "Leftover tidbits",
    "section": "",
    "text": "Today we are going to go over a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.4     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#introduction",
    "href": "modules/module4/13_leftovers/13_leftovers.html#introduction",
    "title": "Leftover tidbits",
    "section": "",
    "text": "Today we are going to go over a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.4     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#really-start-using-an-rproject",
    "href": "modules/module4/13_leftovers/13_leftovers.html#really-start-using-an-rproject",
    "title": "Leftover tidbits",
    "section": "Really start using an Rproject üìΩÔ∏è",
    "text": "Really start using an Rproject üìΩÔ∏è\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nI have noticed that many of you are still not using RProjects. I would really recommend that for easy file management that you do. Here is an a chapter in R for Data Science on how to set one up. If you want to start using Git in the future, you will need to set up a project."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#gghighlight",
    "href": "modules/module4/13_leftovers/13_leftovers.html#gghighlight",
    "title": "Leftover tidbits",
    "section": "gghighlight üî¶",
    "text": "gghighlight üî¶\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nThe package gghighlight allows you to highlight certain geoms in ggplot. Doing this helps your reader focus on the thing you want them to, and helps prevent plot spaghetti. To practice with gghighlight we are going to use some data from the R package gapminder\n\nInstall\n\ninstalll.packages(\"gghighlight\")\ninstall.packages(\"gapminder\")\n\n\n\nLoad libraries\nFirst let‚Äôs load our libraries.\n\nlibrary(gghighlight) # for highlighting\nlibrary(gapminder) # where data is\n\n\n\nWrangle\nWe can create a dataframe that includes only the data for the countries in the continent Americas.\n\ngapminder_americas &lt;- gapminder %&gt;%\n  filter(continent == \"Americas\")\n\n\n\nPlot\nIf we look at all the countries at once, we get plot spaghetti üçù.\n\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country, color = country)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")\n\n\n\n\nCreate a lineplot showing the life expectacy over 1952 to 2007 for all countries, highlighting the United States.\n\n# highlight just the US\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country, color = country)) +\n  geom_line() +\n  gghighlight(country == \"United States\") +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")\n\n\n\n\nFacet our plot, and highlight the country for each facet.\n\n# facet and highlight each country\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_line(aes(color = country)) +\n  gghighlight() +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.text.x = element_text(size = 8),\n        axis.text.x = element_text(angle = 90)) +\n  facet_wrap(~country) +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#patchwork-a-little-more",
    "href": "modules/module4/13_leftovers/13_leftovers.html#patchwork-a-little-more",
    "title": "Leftover tidbits",
    "section": "patchwork, a little more üìàüìäüìâ",
    "text": "patchwork, a little more üìàüìäüìâ\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nWe have talked a bit about patchwork in the lecture on PCA but its such a useful package I wanted to go over it a bit more. The goal of patchwork is to make it very simple to combine plots together.\n\nLoad libraries\n\nlibrary(patchwork)\nlibrary(palmerpenguins) # for making some plots to assemble\n\n\n\nMake some plots\n\nplot1 &lt;- penguins %&gt;%\n  ggplot(aes(x = species, y = body_mass_g, color = species)) +\n  geom_boxplot()\n\nplot2 &lt;- penguins %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()\n\nplot3 &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = island, y = flipper_length_mm, color = species)) +\n  geom_boxplot() +\n  facet_wrap(vars(sex))\n\n\n\nCombine plots\n\n(plot1 + plot2) / plot3 \n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n(plot1 + plot2) / plot3 + plot_annotation(tag_levels = \"A\",\n                                          title = \"Here is some information about penguins\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#gganimate",
    "href": "modules/module4/13_leftovers/13_leftovers.html#gganimate",
    "title": "Leftover tidbits",
    "section": "gganimate üíÉ",
    "text": "gganimate üíÉ\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nhttps://gganimate.com/reference/transition_states.html\n\nInstall\n\ninstall.packages(\"gganimate\") # gganimate\ninstall.packages(\"gapminder\") # gapminder data for example\ninstall.packages(\"magick\") # for gif rendering\n\n\n\nLoad libraries\n\nlibrary(gganimate)\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\n\n\nPlot\n\nplot_to_animate &lt;- gapminder_americas %&gt;%\n  ggplot(aes(x = lifeExp, y = pop, fill = country, label = country)) +\n  geom_point(shape = 21, color = \"black\") +\n  geom_text_repel() +\n  scale_y_log10() +\n  theme_classic() +\n  theme(legend.position = 'none') +\n  labs(title = \"Population and Life Expectancy in the Americas\",\n       subtitle = 'Year: {closest_state}', \n       x = \"Life Expectancy\", \n       y = \"Log10 Population\") +\n  transition_states(year) # what to gif over\n\n# set parameters for your animation\nanimated_plot &lt;- animate(plot = plot_to_animate, \n                        duration = 10, \n                        fps = 10, \n                        width = 700, \n                        height = 700,\n                        renderer = magick_renderer())\n\n\n\nPrint\nPrint your animation.\n\nanimated_plot\n\n\n\n\n\n\n\n\nSave\nSave your animation.\n\n# save it\nanim_save(filename = \"gapminder_gif.gif\",\n          animation = last_animation())"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#ggradar",
    "href": "modules/module4/13_leftovers/13_leftovers.html#ggradar",
    "title": "Leftover tidbits",
    "section": "ggradar üì°",
    "text": "ggradar üì°\nThe package ggradar allows you to create radar plots, which allow the plotting of multidimensional data on a two dimension chart. Typically with these plots, the goal is to compare the variables on the plot across different groups. We are going to try this out with the coffee tasting data from the distributions recitation.\nInstall ggradar if you don‚Äôt already have it. This package is not available on CRAN for the newest version of R, so we can use devtools and install_github() to install it. You could also try using install.packages() and see if that works for you.\n\ndevtools::install_github(\"ricardo-bion/ggradar\",\n                         dependencies = TRUE)\n\n\nlibrary(ggradar)\nlibrary(scales) # for scaling data\n\n# load coffee data from distributions recitation\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\n# extract out df on coffee_ratings\ncoffee &lt;- tuesdata$coffee_ratings\n\n# what are the column names again?\ncolnames(coffee)\n\n [1] \"total_cup_points\"      \"species\"               \"owner\"                \n [4] \"country_of_origin\"     \"farm_name\"             \"lot_number\"           \n [7] \"mill\"                  \"ico_number\"            \"company\"              \n[10] \"altitude\"              \"region\"                \"producer\"             \n[13] \"number_of_bags\"        \"bag_weight\"            \"in_country_partner\"   \n[16] \"harvest_year\"          \"grading_date\"          \"owner_1\"              \n[19] \"variety\"               \"processing_method\"     \"aroma\"                \n[22] \"flavor\"                \"aftertaste\"            \"acidity\"              \n[25] \"body\"                  \"balance\"               \"uniformity\"           \n[28] \"clean_cup\"             \"sweetness\"             \"cupper_points\"        \n[31] \"moisture\"              \"category_one_defects\"  \"quakers\"              \n[34] \"color\"                 \"category_two_defects\"  \"expiration\"           \n[37] \"certification_body\"    \"certification_address\" \"certification_contact\"\n[40] \"unit_of_measurement\"   \"altitude_low_meters\"   \"altitude_high_meters\" \n[43] \"altitude_mean_meters\" \n\n\n\ncoffee_radar &lt;- coffee %&gt;%\n  select(species, aroma:cupper_points) %&gt;% # first column is the groups\n  mutate_at(vars(-species), rescale) %&gt;% # columns need to be between 0 and 1\n  group_by(species) %&gt;%\n  summarize_if(is.numeric, mean) \n\ncoffee_labels &lt;- c(\"Aroma\",\n                   \"Flavor\",\n                   \"Aftertaste\",\n                   \"Acidity\",\n                   \"Body\",\n                   \"Balance\",\n                   \"Uniformity\",\n                   \"Clean cup\",\n                   \"Sweetness\",\n                   \"Cupper points\")\n\n\nggradar(coffee_radar)\n\n\n\n\n\nggradar(coffee_radar,\n        axis.labels = coffee_labels,\n        legend.position = \"bottom\",\n        axis.label.size = 3,\n        grid.label.size = 5) +\n  theme(legend.key = element_rect(fill = NA, color = NA),\n        plot.title = element_text(size = 16),\n        legend.text = element_text(size = 12)) +\n  labs(title = \"Difference in average coffee cupper score \\nin Arabica and Robusta beans\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#heatmaps",
    "href": "modules/module4/13_leftovers/13_leftovers.html#heatmaps",
    "title": "Leftover tidbits",
    "section": "Heatmaps üü•‚¨úÔ∏èüü¶",
    "text": "Heatmaps üü•‚¨úÔ∏èüü¶\n\nInstall\n\ninstall.packages(\"pheatmap\")\n\n\n\nLoad libraries\n\nlibrary(pheatmap)\n\n\n\nPlot\n\npheatmap(mtcars)\n\n\n\n\n\npheatmap(mtcars, \n         scale = \"column\",\n         cluster_rows = TRUE) # cluster rows based on similarity\n\n\n\n\n\n\nConplexHeatmap\nThe package ComplexHeatmap allows more customized and complicated heatmaps to be produced. If you are interested in making heatmaps, this package is worth to check out."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#useful-resources",
    "href": "modules/module4/13_leftovers/13_leftovers.html#useful-resources",
    "title": "Leftover tidbits",
    "section": "Useful resources",
    "text": "Useful resources\n\ngghighlight\npatchwork\ngganimate\nggradar\npheatmap\nComplexHeatmap"
  },
  {
    "objectID": "rmds/03_Rmd_recitation.html",
    "href": "rmds/03_Rmd_recitation.html",
    "title": "RMarkdown Recitation",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation.html#r-markdown-recitation",
    "href": "rmds/03_Rmd_recitation.html#r-markdown-recitation",
    "title": "RMarkdown Recitation",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HCS 7100: Data Visualization in R",
    "section": "",
    "text": "The Ohio State University, Autumn 2023\nInstructor: Jessica Cooperstone  Teaching assistant: Daniel Quiroz Moreno\n\n\n\n\n\nDatasaurus Dozen by Alberto Cairo\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html",
    "href": "assignments/modules/module2/module_2_solutions.html",
    "title": "Module 2 Assignment Solutions",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on RMarkdown, wrangling, ggplot101, and ggplot102.\nYou will submit this assignment by uploading a knitted .html to Carmen. Make sure you include the Code Download button so that I can see your code as well. Customize the YAML and the document so you like how it looks.\nRemember there are often many ways to reach the same end product.\n\nThis assignment will be due on Tuesday, October 3, 2023, at 11:59pm.\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded."
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#introduction",
    "href": "assignments/modules/module2/module_2_solutions.html#introduction",
    "title": "Module 2 Assignment Solutions",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on RMarkdown, wrangling, ggplot101, and ggplot102.\nYou will submit this assignment by uploading a knitted .html to Carmen. Make sure you include the Code Download button so that I can see your code as well. Customize the YAML and the document so you like how it looks.\nRemember there are often many ways to reach the same end product.\n\nThis assignment will be due on Tuesday, October 3, 2023, at 11:59pm.\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded."
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#writing-in-markdown-1",
    "href": "assignments/modules/module2/module_2_solutions.html#writing-in-markdown-1",
    "title": "Module 2 Assignment Solutions",
    "section": "Writing in Markdown 1",
    "text": "Writing in Markdown 1\nUsing coding in text, write a sentence in markdown that pulls from this data how many total PhDs were awarded in 2017. If you want to make some calculations in a code chunk first that is ok.\n\nlibrary(tidyverse)\nlibrary(scales) # for using comma format \n\nSetting as an object the number for the total Ph.D.s earned in 2017.\n\nphds_2017 &lt;- phd_field %&gt;%\n  filter(year == 2017) %&gt;%\n  select(n_phds) %&gt;%\n  colSums(na.rm = TRUE)\n\nHow to write in Markdown:\nIn 2017, there were were `r format(phds_2017, scientific = F, big.mark = \",\")` Ph.D.¬†degrees awarded in the United States.\nIn 2017, there were were `r phd_field %&gt;% filter(year == 2017) %&gt;% select(n_phds) %&gt;% colSums(na.rm = TRUE) %&gt;% format(scientific = F, big.mark = \",\")` Ph.D.¬†degrees awarded in the United States.\nHow this will look when rendered to html:\nIn 2017, there were 54,110 Ph.D.¬†degrees awarded in the United States.\nIn 2017, there were 54,110 Ph.D.¬†degrees awarded in the United States."
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#visualization-1",
    "href": "assignments/modules/module2/module_2_solutions.html#visualization-1",
    "title": "Module 2 Assignment Solutions",
    "section": "Visualization 1",
    "text": "Visualization 1\nMake a chart to visualize of the total number of PhDs awarded for each broad_field across the total time period of this data. You pick the type of chart that you think is appropriate, and make sure your plot is appropriately labelled and you are happy with how it looks. Hint, to do this you‚Äôll probably have to do some data wrangling first.\nFirst I will calculate the total number of Ph.D.s awarded across each broad_field for the whole time period.\n\nbroad_field_sum &lt;- phd_field %&gt;%\n  group_by(broad_field) %&gt;%\n  summarize(broad_field_sum = sum(n_phds, na.rm = TRUE)) %&gt;%\n  arrange(-broad_field_sum)\n\nbroad_field_sum\n\n# A tibble: 7 √ó 2\n  broad_field                       broad_field_sum\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 Life sciences                              205703\n2 Psychology and social sciences             119116\n3 Humanities and arts                         53045\n4 Education                                   52639\n5 Mathematics and computer sciences           35481\n6 Other                                       28855\n7 Engineering                                 18139\n\n\nThis summary helps me know what to expect to see in my plots. Now I can create some plots.\nThe first one will just show the total number of Ph.D.s across each broad_field. I decided to put the broad_field on the y-axis so that the labels are easier to read.\n\nbroad_field_sum %&gt;%\n  ggplot(aes(x = broad_field_sum, y = fct_reorder(broad_field, broad_field_sum))) +\n  geom_col(color = \"black\", fill = \"grey\") +\n  scale_x_continuous(labels = comma) + # add a comma to the x-axis breaks\n  theme_minimal() +\n  labs(x = \"Total number of Ph.D.s\",\n       y = \"\", # no label on the y\n       title = \"Number of PhDs awarded across different broad disciplines \\nfrom 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\nWe could also just look at the total number of Ph.Ds awarded during each year.\n\nbroad_field_sumonly_eachyear &lt;- phd_field %&gt;%\n  group_by(year) %&gt;%\n  summarize(all_the_phds = sum(n_phds, na.rm = TRUE)) \n\nbroad_field_sumonly_eachyear\n\n# A tibble: 10 √ó 2\n    year all_the_phds\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1  2008        48026\n 2  2009        49141\n 3  2010        47628\n 4  2011        48546\n 5  2012        50777\n 6  2013        52370\n 7  2014        53364\n 8  2015        54154\n 9  2016        54862\n10  2017        54110\n\n\nThen we can plot.\n\nbroad_field_sumonly_eachyear %&gt;%\n  ggplot(aes(x = year, y = all_the_phds)) +\n  geom_col(color = \"black\", fill = \"grey\") +\n  scale_y_continuous(labels = comma) + # add a comma to the x-axis breaks\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Number of Ph.Ds\", \n       title = \"Total number of Ph.D.s awarded in the United States per year from 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\nThis wasn‚Äôt exactly the question, but if we want to see a little bit better this over the time period, first we need to create a dataframe that has total PhDs per broad field per year. We can do this by adding year to our group_by() statement.\n\nbroad_field_sum_byyear &lt;- phd_field %&gt;%\n  group_by(broad_field, year) %&gt;%\n  summarize(total_phds = sum(n_phds, na.rm = TRUE)) \n\n`summarise()` has grouped output by 'broad_field'. You can override using the\n`.groups` argument.\n\n# what does that look like\nhead(broad_field_sum_byyear)\n\n# A tibble: 6 √ó 3\n# Groups:   broad_field [1]\n  broad_field  year total_phds\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Education    2008       6561\n2 Education    2009       6528\n3 Education    2010       5287\n4 Education    2011       4670\n5 Education    2012       4803\n6 Education    2013       4934\n\n\nNow we can plot. We can make different types of plots. Let‚Äôs start simpler. Here I‚Äôve re-ordered the legend to be in the same order as the chart so you can see the disciplines that award the most Ph.D.s just by looking at the order of the legend.\n\nbroad_field_sum_byyear %&gt;%\n  ggplot(aes(x = year, y = total_phds, color = reorder(broad_field, -total_phds))) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2008, 2017, 2)) +\n  scale_y_continuous(labels = comma) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Total number of Ph.D.s\",\n       color = \"Broad Field\",\n       title = \"Number of Ph.D.s awarded by discipline in the United States\",\n       subtitle = \"From 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\nCould also try facets.\n\n# re-setting factors of broad_field so that it is levelled by the discipline with\n# the most phds.\nbroad_field_sum_byyear$broad_field &lt;- fct_reorder(broad_field_sum_byyear$broad_field,\n                                                  -broad_field_sum_byyear$total_phds)\n\nbroad_field_sum_byyear %&gt;%\n  ggplot(aes(x = year, y = total_phds, color = broad_field)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2008, 2017, 2)) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(vars(broad_field)) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\",\n       y = \"Total number of Ph.D.s\",\n       color = \"Broad Field\",\n       title = \"Number of Ph.D.s awarded by discipline in the United States\",\n       subtitle = \"From 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\nCan also try with ‚Äúfree-y‚Äù axes. You can see the number better but can‚Äôt as easily compare between the disciplines.\n\nbroad_field_sum_byyear %&gt;%\n  ggplot(aes(x = year, y = total_phds, color = broad_field)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2008, 2017, 2)) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(vars(broad_field), scales = \"free_y\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\",\n       y = \"Total number of Ph.D.s\",\n       color = \"Broad Field\",\n       title = \"Number of Ph.D.s awarded by discipline in the United States\",\n       subtitle = \"From 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")"
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#visualization-2",
    "href": "assignments/modules/module2/module_2_solutions.html#visualization-2",
    "title": "Module 2 Assignment Solutions",
    "section": "Visualization 2",
    "text": "Visualization 2\nPick the field that most closely matches the area of your degree. Make a line graph (with points for each datapoint) that shows how the number of PhDs awarded in your field has changed from 2008 to 2017. Make sure your x-axis indicates each year for which you have data, your graph is appropriately labelled, and you think it is aesthetically pleasing.\n\n# what are the options for broad_field?\nphd_field %&gt;%\n  select(broad_field) %&gt;%\n  unique()\n\n# A tibble: 7 √ó 1\n  broad_field                      \n  &lt;chr&gt;                            \n1 Life sciences                    \n2 Mathematics and computer sciences\n3 Psychology and social sciences   \n4 Engineering                      \n5 Education                        \n6 Humanities and arts              \n7 Other                            \n\n# what are the options for life sciences?\nphd_field %&gt;%\n  filter(broad_field == \"Life sciences\") %&gt;%\n  select(major_field) %&gt;%\n  unique()\n\n# A tibble: 6 √ó 1\n  major_field                                          \n  &lt;chr&gt;                                                \n1 Agricultural sciences and natural resources          \n2 Biological and biomedical sciences                   \n3 Health sciences                                      \n4 Chemistry                                            \n5 Geosciences, atmospheric sciences, and ocean sciences\n6 Physics and astronomy                                \n\n# what are the options for Agricultural sciences and natural resources?\nphd_field %&gt;%\n  filter(major_field == \"Agricultural sciences and natural resources\") %&gt;%\n  select(field) %&gt;%\n  unique()\n\n# A tibble: 26 √ó 1\n   field                                        \n   &lt;chr&gt;                                        \n 1 Agricultural economics                       \n 2 Agricultural and horticultural plant breeding\n 3 Agricultural animal breeding                 \n 4 Agronomy and crop science                    \n 5 Animal nutrition                             \n 6 Animal science, poultry or avian             \n 7 Animal sciences, other                       \n 8 Environmental science                        \n 9 Fishing and fisheries sciences and management\n10 Food science                                 \n# ‚Ñπ 16 more rows\n\n\nI‚Äôm picking ‚ÄúHorticulture science‚Äù as my field.\n\nphd_field %&gt;%\n  filter(field == \"Horticulture science\") %&gt;%\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Number of Ph.D. Degrees Awarded\",\n       title = \"Number of Ph.D. Degrees Awarded in Horticulture Science in the United States\",\n       subtitle = \"Data collected by the National Science Foundation\")"
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#visualization-3",
    "href": "assignments/modules/module2/module_2_solutions.html#visualization-3",
    "title": "Module 2 Assignment Solutions",
    "section": "Visualization 3",
    "text": "Visualization 3\nPick at least 3 additional fields (you can use more if you like) that are adjacent to your Ph.D.¬†field. Make a faceted plot to show the number of degrees awarded in each of these disciplines across the same time period. Make sure you label your plot appropriately and you think it is aesthetic (i.e., if you have squished strip text you want to fix that).\nFirst I am making a vector of the fields in plant science.\n\nplant_sci &lt;- c(\"Agricultural and horticultural plant breeding\",\n               \"Agronomy and crop science\",\n               \"Horticulture science\",\n               \"Plant pathology and phytopathology, agricultural\",\n               \"Plant sciences, other\")\n\nThen I can plot. \\n adds an automatic line break.\n\nphd_field %&gt;%\n  filter(field %in% plant_sci) %&gt;%\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  facet_wrap(vars(field), labeller = label_wrap_gen()) + # wraps labels\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Year\",\n       y = \"Number of Ph.D. Degrees Awarded\",\n       title = \"Number of Ph.D. Degrees Awarded in Plant Science Subdisciplines \\nin the United States\",\n       subtitle = \"Data collected by the National Science Foundation\")\n\n\n\n\nOr I can make the same style plot but with bars instead of a lines/points.\n\nphd_field %&gt;%\n  filter(field %in% plant_sci) %&gt;%\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_col() +\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  facet_wrap(vars(field), labeller = label_wrap_gen()) + # wraps labels\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Year\",\n       y = \"Number of Ph.D. Degrees Awarded\",\n       title = \"Number of Ph.D. Degrees Awarded in Plant Science Subdisciplines \\nin the United States\",\n       subtitle = \"Data collected by the National Science Foundation\")"
  },
  {
    "objectID": "assignments/modules/module3/module_3_solutions.html",
    "href": "assignments/modules/module3/module_3_solutions.html",
    "title": "Module 3 Assignment Solutions",
    "section": "",
    "text": "This is your assignment for Module 3 Data Exploration, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info:\n\nPlease submit this assignment by uploading a knitted .html to Carmen\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing\nMake sure you include the Code Download button so that I can see your code as well\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\nThis assignment is due on Tuesday, October 31, 2023, at 11:59pm üëªüéÉüï∏\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)\nlibrary(tidyverse)"
  },
  {
    "objectID": "assignments/modules/module3/module_3_solutions.html#introduction",
    "href": "assignments/modules/module3/module_3_solutions.html#introduction",
    "title": "Module 3 Assignment Solutions",
    "section": "",
    "text": "This is your assignment for Module 3 Data Exploration, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info:\n\nPlease submit this assignment by uploading a knitted .html to Carmen\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing\nMake sure you include the Code Download button so that I can see your code as well\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\nThis assignment is due on Tuesday, October 31, 2023, at 11:59pm üëªüéÉüï∏\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)\nlibrary(tidyverse)"
  },
  {
    "objectID": "assignments/modules/module3/module_3_solutions.html#data-distributions-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_solutions.html#data-distributions-visualization-3-pts",
    "title": "Module 3 Assignment Solutions",
    "section": "1. Data distributions visualization (3 pts)",
    "text": "1. Data distributions visualization (3 pts)\nCreate a visualization that shows the distribution of tuition costs (both in_state_tuition and out_of_state_tuition) across public, private, and for-profit universities and colleges. You can use whatever type of plot you think is appropriate to show this distribution across different types of universities. Your plot should be publication ready quality.\n\nWrangle\n\n# wrangle data to be in tidy format for in state and out of state tuition\n# to facet, the faceting variable needs to be in one column\ntuition_cost_tidy &lt;- tuition_cost %&gt;%\n  pivot_longer(cols = c(in_state_tuition, out_of_state_tuition),\n               names_to = \"state_type\",\n               values_to = \"tuition\")\n\n\n\nAdjust labels\n\n# make a vector that has all the full names as wanted to appear in strip text\nstate_type_labels &lt;- c(\"In state tuition\", \"Out of state tuition\")\n\n# tell state_type_labels which original label to refer to\nnames(state_type_labels) &lt;- c(\"in_state_tuition\", \"out_of_state_tuition\")\n\n\n\nPlot\n\nDensity plot\nI think this one is my favorite.\n\n# plot\ntuition_cost_tidy %&gt;%\n  filter(type != \"Other\") %&gt;%\n  ggplot(aes(x = tuition, y = type, fill = type)) +\n  stat_density_ridges(quantile_lines = TRUE,\n                      quantiles = 2,\n                      alpha = 0.5) +\n  scale_x_continuous(labels = dollar) +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_wrap(vars(state_type),\n             labeller = labeller(state_type = state_type_labels)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Yearly tuition, in US dollars\",\n       y = \"Type of College/University\",\n       title = \"Distribution of tuition costs for both in-state and out-of-state students \\nacross public, private, and for-profit universities\",\n       caption = \"Line represents the median yearly tuition\")\n\n\n\n\nCould also instead facet by type. I divided tuition by 1000 to have the x-axis be in terms of thousands (i.e., K). A plot like this allows a better direct comparison between in and out of state, while the preious better allows a comparison between the types (i.e., public, for profit, private).\n\ntuition_cost_tidy %&gt;%\n  filter(type != \"Other\") %&gt;%\n  ggplot(aes(x = tuition/1000, y = state_type, fill = state_type)) +\n  stat_density_ridges(quantile_lines = TRUE,\n                      quantiles = 2,\n                      alpha = 0.5) +\n  scale_x_continuous(labels = scales::dollar_format(prefix = \"$\", suffix = \"K\")) +\n  scale_y_discrete(labels = c(\"In state tuition\", \"Out of state tuition\")) +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_wrap(vars(type)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(y = \"Yearly tuition, in US dollars\",\n       x = \"Type of College/University\",\n       title = \"Distribution of tuition costs for both in-state and out-of-state students \\nacross public, private, and for-profit universities\",\n       caption = \"Line represents the median yearly tuition\")\n\n\n\n\n\n\nBoxplot with dots\n\n# convert type to a factor and set levels\n# i ordered from lowest mean tuition to highest\ntuition_cost_tidy$type &lt;- factor(tuition_cost_tidy$type, \n                                 levels = c(\"Public\", \"For Profit\", \"Private\"))\n\n# plot\ntuition_cost_tidy %&gt;%\n  filter(type != \"Other\") %&gt;%\n  ggplot(aes(x = type, y = tuition, color = type)) +\n  geom_boxplot(outlier.shape = NA) +\n  ggdist::geom_dots(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(vars(state_type),\n             labeller = labeller(state_type = state_type_labels)) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Yearly tuition, in US dollars\",\n       y = \"Type of College/University\",\n       title = \"Distribution of tuition costs for both in-state and out-of-state students \\nacross public, private, and for-profit universities\")"
  },
  {
    "objectID": "assignments/modules/module3/module_3_solutions.html#adding-statistics-visualization-4-pts",
    "href": "assignments/modules/module3/module_3_solutions.html#adding-statistics-visualization-4-pts",
    "title": "Module 3 Assignment Solutions",
    "section": "2. Adding statistics visualization (4 pts)",
    "text": "2. Adding statistics visualization (4 pts)\nMake a plot that shows the difference in early_career_pay across private and public universities/colleges. Is there any statistical difference in pay across these two categories of institutions? Is the same true for mid_career_pay? This can be either one or two plots, its up to you. Make sure you are doing the right statistical test appropriate for your data.\n\nJoin tuition_cost and salary_potential\nHere I am using left_join() to only include salary data that is present for universities in tuition_cost. You could use other types of joins and those would also work ok in this application.\n\nsalary_cost_join &lt;- left_join(tuition_cost, salary_potential,\n                              by = \"name\")\n\n\n\nInvestigatory plots\nWrangle.\n\n# make column called pay_stage so can facet by career stage\nsalary_cost_join_tidy &lt;- salary_cost_join %&gt;%\n  pivot_longer(cols = early_career_pay:mid_career_pay,\n               names_to = \"pay_stage\",\n               values_to = \"pay\")\n\nsalary_cost_join_tidy &lt;- salary_cost_join_tidy %&gt;%\n  pivot_longer(cols = c(in_state_tuition, out_of_state_tuition),\n               names_to = \"in_or_out_of_state\",\n               values_to = \"tuition\")\n\n# make a vector that has all the full names as wanted to appear in strip text\npay_stage_labels &lt;- c(\"Early Career Pay\", \"Mid Career Pay\")\n\n# tell state_type_labels which original label to refer to\nnames(pay_stage_labels) &lt;- c(\"early_career_pay\", \"mid_career_pay\")\n\nFacet based on pay_stage and in_or_out_of_state. Dividing the pay and tuition by 1000 to simplify axes, and then this change is reflected in the continuous scale functions.\n\n# faceted plot\nsalary_cost_join_tidy %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = pay/1000, y = tuition/1000, color = type)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(labels = scales::dollar_format(prefix = \"$\", suffix = \"K\")) + \n  scale_y_continuous(labels = scales::dollar_format(prefix = \"$\", suffix = \"K\")) + \n  scale_color_manual(values = c(\"#fc8d62\", \"#8da0cb\")) + # pick colors\n  facet_grid(rows = vars(pay_stage), cols = vars(in_or_out_of_state),\n             labeller = labeller(pay_stage = pay_stage_labels,\n                                 in_or_out_of_state = state_type_labels)) +\n  theme_classic() +\n  theme(legend.position = \"top\") +\n  labs(x = \"Pay, in US Dollars\",\n       y = \"Tuition, in US Dollars\",\n       title = \"Relationship between career pay and tuition \\nacross private and public US universities\",\n       color = \"University type\")\n\n\n\n\n\n# early career pay only\nsalary_cost_join %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = early_career_pay, y = in_state_tuition, color = type)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  stat_cor(method = \"pearson\") + # add correlation coefficient and pval\n  scale_x_continuous(labels = dollar) + # convert y-axis labels to dollar format\n  scale_y_continuous(labels = dollar) + # convert y-axis labels to dollar format\n  scale_color_manual(values = c(\"#fc8d62\", \"#8da0cb\")) + # pick colors\n  theme_minimal() +\n  labs(x = \"Early Career Pay, in US Dollars\",\n       y = \"In State Tuition, in US Dollars\",\n       title = \"Relationship between early career pay and in state tuition \\nacross private and public US universities\",\n       color = \"University type\")\n\n\n\n# mid career pay only\nsalary_cost_join %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = mid_career_pay, y = out_of_state_tuition, color = type)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  stat_cor(method = \"pearson\") + # add correlation coefficient and pval\n  scale_x_continuous(labels = dollar) + # convert y-axis labels to dollar format\n  scale_y_continuous(labels = dollar) + # convert y-axis labels to dollar format\n  scale_color_manual(values = c(\"#fc8d62\", \"#8da0cb\")) + # pick colors\n  theme_minimal() +\n  labs(x = \"Mid Career Pay, in US Dollars\",\n       y = \"In State Tuition, in US Dollars\",\n       title = \"Relationship between mid career pay and in state tuition \\nacross private and public US universities\",\n       color = \"University type\")\n\n\n\n\n\n\nTest assumptions\nUntransformed data.\n\n# normality\nsalary_cost_join %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  drop_na(early_career_pay, mid_career_pay, type) %&gt;% # remove NAs\n  group_by(type) %&gt;% # group by private vs public\n  shapiro_test(early_career_pay,\n               mid_career_pay) # test for normality\n\n# A tibble: 4 √ó 4\n  type    variable         statistic        p\n  &lt;chr&gt;   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 Private early_career_pay     0.951 4.83e-11\n2 Private mid_career_pay       0.951 3.95e-11\n3 Public  early_career_pay     0.954 1.28e- 7\n4 Public  mid_career_pay       0.952 6.50e- 8\n\n# constant variance early_career_pay\nsalary_cost_join %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  drop_na(early_career_pay, mid_career_pay, type) %&gt;% # remove NAs\n  levene_test(early_career_pay ~ type) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic         p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     1   725      18.2 0.0000223\n\n# constant variance mid_career_pay\nsalary_cost_join %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  drop_na(early_career_pay, mid_career_pay, type) %&gt;% # remove NAs\n  levene_test(mid_career_pay ~ type) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic          p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1     1   725      20.6 0.00000663\n\n\nNot normal and no constant variance. Non-parametric it is.\n\n\nWilcoxon test\n\n# early career pay\nsalary_cost_join %&gt;%\n  filter(type != \"For Profit\") %&gt;%\n  drop_na(early_career_pay, type) %&gt;% # remove NAs\n  wilcox_test(early_career_pay ~ type,\n              paired = FALSE)\n\n# A tibble: 1 √ó 7\n  .y.              group1  group2    n1    n2 statistic         p\n* &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 early_career_pay Private Public   451   276    72986. 0.0000918\n\n# mid career pay\nsalary_cost_join %&gt;%\n  filter(type != \"For Profit\") %&gt;%\n  drop_na(mid_career_pay, type) %&gt;% # remove NAs\n  wilcox_test(mid_career_pay ~ type,\n              paired = FALSE)\n\n# A tibble: 1 √ó 7\n  .y.            group1  group2    n1    n2 statistic         p\n* &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 mid_career_pay Private Public   451   276     74294 0.0000115\n\n\nStatistically significantly different early and mid career pay for people attending public vs private institutions.\n\n\nBoxplot\n\nUsing stat_compare_means()\n\nsalary_cost_join %&gt;%\n  drop_na(type, early_career_pay) %&gt;% # drop missing values for variables to plot\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;% # only private and public unis\n  ggplot(aes(x = type, y = early_career_pay, fill = type)) +\n  geom_boxplot(alpha = 0.5) + # boxplot\n  scale_y_continuous(labels = dollar) + # convert y-axis labels to dollar format\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) + # pick colors\n  stat_compare_means(label.y = 92000) + # run stats, indicate label position\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Early Career Pay, in US Dollars\",\n       title = \"Differences in average early career pay across public \\nand private US universities/colleges\")\n\n\n\nsalary_cost_join %&gt;%\n  drop_na(type, mid_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = mid_career_pay, fill = type)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  stat_compare_means(label.y = 163000) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Mid Career Pay, in US Dollars\",\n       title = \"Differences in average mid career pay across public \\nand private US universities/colleges\")\n\n\n\n\n\n\nWith annotate()\n\nsalary_cost_join %&gt;%\n  drop_na(type, early_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = early_career_pay, fill = type)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  annotate(geom = \"text\", x = 2, y = 80000, label = \"*\", size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Early Career Pay, in US Dollars\",\n       title = \"Differences in average early career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\nsalary_cost_join %&gt;%\n  drop_na(type, mid_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = mid_career_pay, fill = type)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  annotate(geom = \"text\", x = 2, y = 145000, label = \"*\", size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Mid Career Pay, in US Dollars\",\n       title = \"Differences in average mid career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\n\n\n\nWith geom_text()\n\nsalary_cost_join %&gt;%\n  drop_na(type, early_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = early_career_pay, fill = type)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  geom_text(aes(x = 2, y = 80000, label = \"*\"), size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Early Career Pay, in US Dollars\",\n       title = \"Differences in average early career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\nsalary_cost_join %&gt;%\n  drop_na(type, mid_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = mid_career_pay, fill = type)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  geom_text(aes(x = 2, y = 145000, label = \"*\"), size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Mid Career Pay, in US Dollars\",\n       title = \"Differences in average mid career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\n\n\nAnd a faceted plot\nI learned how to use geom_text() and geom_segment() to add stats annotation on a faceted plot with the help of this blog post. First I create a dataframe that includes the coordinates of where the segments and text should go, and then I make the plot.\n\n\n\n\n\n\n\n\n\nThis image helps you to see what the different variables are.\n\n# create a dataframe that includes\ntext_to_plot &lt;- data.frame(\n  x1 = c(1, 1), x2 = c(2, 2), \n  y1 = c(100000, 170000), y2 = c(120000, 190000), \n  xstar = c(1.5, 1.5), ystar = c(130000, 200000),\n  label = c(\"*\", \"*\"),\n  pay_stage = c(\"early_career_pay\", \"mid_career_pay\")\n)\n\nsalary_cost_join_tidy %&gt;%\n  drop_na(type, pay_stage, pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = pay, fill = pay_stage)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_x_discrete(labels = c(\"Private\", \"Public\")) +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  facet_wrap(vars(pay_stage),\n             labeller = labeller(pay_stage = pay_stage_labels)) +\n  geom_text(data = text_to_plot,\n            aes(x = xstar,  y = ystar, label = label), size = 8) +\n  geom_segment(data = text_to_plot, \n               aes(x = x1, xend = x1, \n                   y = y1, yend = y2), color = \"black\") +\n  geom_segment(data = text_to_plot,\n               aes(x = x2, xend = x2, \n                   y = y1, yend = y2), color = \"black\") +\n  geom_segment(data = text_to_plot, \n               aes(x = x1, xend = x2, \n                   y = y2, yend = y2), color = \"black\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  ylim(c(0, 210000)) +\n  labs(x = \"University Type\",\n       y = \"Yearly Pay, in US Dollars\",\n       title = \"Differences in early and mid career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\n\n\n\n\n\nDotplot\n\nUsing stat_compare_means()\n\nsalary_cost_join %&gt;%\n  drop_na(type, early_career_pay) %&gt;% # drop missing values for variables to plot\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;% # only private and public unis\n  ggplot(aes(x = type, y = early_career_pay, fill = type)) +\n  stat_dotsinterval(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) + # convert y-axis labels to dollar format\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) + # pick colors\n  stat_compare_means(label.y = 92000) + # run stats, indicate label position\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Early Career Pay, in US Dollars\",\n       title = \"Differences in average early career pay across public \\nand private US universities/colleges\")\n\n\n\nsalary_cost_join %&gt;%\n  drop_na(type, mid_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = mid_career_pay, fill = type)) +\n  stat_dotsinterval(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  stat_compare_means(label.y = 163000) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Mid Career Pay, in US Dollars\",\n       title = \"Differences in average mid career pay across public \\nand private US universities/colleges\")\n\n\n\n\n\n\nWith annotate()\n\nsalary_cost_join %&gt;%\n  drop_na(type, early_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = early_career_pay, fill = type)) +\n  stat_dotsinterval(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  annotate(geom = \"text\", x = 2, y = 80000, label = \"*\", size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Early Career Pay, in US Dollars\",\n       title = \"Differences in average early career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\nsalary_cost_join %&gt;%\n  drop_na(type, mid_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = mid_career_pay, fill = type)) +\n  stat_dotsinterval(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  annotate(geom = \"text\", x = 2, y = 145000, label = \"*\", size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Mid Career Pay, in US Dollars\",\n       title = \"Differences in average mid career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\n\n\n\nWith geom_text()\n\nsalary_cost_join %&gt;%\n  drop_na(type, early_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = early_career_pay, fill = type)) +\n  stat_dotsinterval(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  geom_text(aes(x = 2, y = 80000, label = \"*\"), size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Early Career Pay, in US Dollars\",\n       title = \"Differences in average early career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\nsalary_cost_join %&gt;%\n  drop_na(type, mid_career_pay) %&gt;%\n  filter(type %in% c(\"Private\", \"Public\")) %&gt;%\n  ggplot(aes(x = type, y = mid_career_pay, fill = type)) +\n  stat_dotsinterval(side = \"both\", layout = \"swarm\") +\n  scale_y_continuous(labels = dollar) +\n  scale_fill_manual(values = c(\"#fc8d62\", \"#8da0cb\")) +\n  geom_text(aes(x = 2, y = 145000, label = \"*\"), size = 8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"University type\",\n       y = \"Mid Career Pay, in US Dollars\",\n       title = \"Differences in average mid career pay across public \\nand private US universities/colleges\",\n       caption = \"Asterisks indicates a significant difference using the Wilcoxon Rank Sun test\")\n\n\n\n\nCould also make violin plots or histograms."
  },
  {
    "objectID": "assignments/modules/module3/module_3_solutions.html#understanding-correlations-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_solutions.html#understanding-correlations-visualization-3-pts",
    "title": "Module 3 Assignment Solutions",
    "section": "3. Understanding correlations visualization (3 pts)",
    "text": "3. Understanding correlations visualization (3 pts)\nMake a visualization that investigates and then visualizes correlation between early_career_pay, mid_career_pay and university tuition (both in state and out of state) showing correlation coefficients. Show how this is different across public and private universities. If you feel like you want to make a couple plots to display this relationship, that is fine.\n\nWrangling\nI want to pick only the variables to include in the correlation analysis, get rid of missing values, and make it into a matrix (as rcorr() needs a matrix).\n\n# create matrix for correlation\nto_rcorr_public &lt;- salary_cost_join %&gt;%\n  filter(type == \"Public\") %&gt;%\n  select(in_state_tuition, out_of_state_tuition, early_career_pay, mid_career_pay) %&gt;%\n  drop_na() %&gt;%\n  as.matrix() # rcorr() needs a matrix\n\n# create matrix for correlation\nto_rcorr_private &lt;- salary_cost_join %&gt;%\n  filter(type == \"Private\") %&gt;%\n  select(in_state_tuition, out_of_state_tuition, early_career_pay, mid_career_pay) %&gt;%\n  drop_na() %&gt;%\n  as.matrix() # rcorr() needs a matrix\n\n\n\nCreate correlation matrix with pvalues.\nI‚Äôm doing two separate correlations, for both private and public institutions.\n\n# create a correlation matrix that includes the pvalues for the correlations\npublic_rcorr &lt;- rcorr(to_rcorr_public, type = \"pearson\")\nprivate_rcorr &lt;- rcorr(to_rcorr_private, type = \"pearson\")\n\n\n\nGet labels set up\n\n# create a vector of the alkaloid names for labeling\naxis_labels &lt;- c(\"In state tuition\",\n                 \"Out of state tuition\",\n                 \"Early career pay\",\n                 \"Mid career pay\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(public_rcorr$r) &lt;- axis_labels\nrownames(public_rcorr$r) &lt;- axis_labels\ncolnames(private_rcorr$r) &lt;- axis_labels\nrownames(private_rcorr$r) &lt;- axis_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(public_rcorr$P) &lt;- axis_labels\nrownames(public_rcorr$P) &lt;- axis_labels\ncolnames(private_rcorr$P) &lt;- axis_labels\nrownames(private_rcorr$P) &lt;- axis_labels\n\n\n\nPlot\n\n# plot public\ncorrplot(public_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = public_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"white\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 1, # size of correlation font\n         title = \"Correlations between tuition and salaries \\nfor public universities/colleges\", # indicate title\n         mar = c(0,0,3,0)) # move title down\n\n\n\n# plot private\ncorrplot(private_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = private_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"white\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 1, # size of correlation font\n         title = \"Correlations between tuition and salaries \\nfor private universities/colleges\", # indicate title\n         mar = c(0,0,3,0)) # move title down"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html",
    "href": "assignments/modules/module4/module_4_assignment.html",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "This is your assignment for Module 4 Putting It All Together, focused on the material you learned in the lectures and recitation activities on PCA, Manhattan plots, interactive plots, and the leftovers.\nSubmission info:\n\nPlease submit this assignment by uploading a knitted .html to Carmen\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing. Starting on this assignment, I will be considering for overall format and readability of your assignment as part of your grade. I am doing this because the format of your report will be considered for your final capstone assignment. This means you should have reasonable headers and header levels, understandable flow between plots and code, and use Markdown language when appropriate.\nMake sure you include the Code Download button so that I can see your code as well\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\nThis assignment will be due on Wednesday, December 5, 2023, at 11:59pm.\n\n\n\nThe data we will be using is the same we used in the ggplot102 recitation that includes information about dog breed trait information from the American Kennel Club.\nDownload the data using the code below. Don‚Äôt use the code from week 5 recitation.\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/dataviz-site/master/assignments/data/breed_traits_fixed.csv')\n\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/trait_description.csv')\n\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_rank.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(gghighlight)"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#introduction",
    "href": "assignments/modules/module4/module_4_assignment.html#introduction",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "This is your assignment for Module 4 Putting It All Together, focused on the material you learned in the lectures and recitation activities on PCA, Manhattan plots, interactive plots, and the leftovers.\nSubmission info:\n\nPlease submit this assignment by uploading a knitted .html to Carmen\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing. Starting on this assignment, I will be considering for overall format and readability of your assignment as part of your grade. I am doing this because the format of your report will be considered for your final capstone assignment. This means you should have reasonable headers and header levels, understandable flow between plots and code, and use Markdown language when appropriate.\nMake sure you include the Code Download button so that I can see your code as well\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\nThis assignment will be due on Wednesday, December 5, 2023, at 11:59pm.\n\n\n\nThe data we will be using is the same we used in the ggplot102 recitation that includes information about dog breed trait information from the American Kennel Club.\nDownload the data using the code below. Don‚Äôt use the code from week 5 recitation.\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/dataviz-site/master/assignments/data/breed_traits_fixed.csv')\n\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/trait_description.csv')\n\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_rank.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(gghighlight)"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#principal-components-analysis-pca-of-american-kennel-club-dog-bred-trait-data-6-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#principal-components-analysis-pca-of-american-kennel-club-dog-bred-trait-data-6-pts",
    "title": "Module 4 Assignment",
    "section": "1. Principal components analysis (PCA) of American Kennel Club dog bred trait data (6 pts)",
    "text": "1. Principal components analysis (PCA) of American Kennel Club dog bred trait data (6 pts)\nRun a PCA on breed_traits for all of the numeric data present in that dataset. Create the following plots and make them of publication quality:\n\nA scree plot\nA scores plot\nA loadings plot\nA two panel plot that has the scores plot and the scree plot together"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#make-your-pca-plot-interactive-2-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#make-your-pca-plot-interactive-2-pts",
    "title": "Module 4 Assignment",
    "section": "2. Make your PCA plot interactive (2 pts)",
    "text": "2. Make your PCA plot interactive (2 pts)\nMake your PCA scores plot interactive, and so that when you hover each point, you can see what the name of that dog breed is (and only the breed of that dog)."
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#see-how-your-pca-related-to-breed-popularity-2-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#see-how-your-pca-related-to-breed-popularity-2-pts",
    "title": "Module 4 Assignment",
    "section": "3. See how your PCA related to breed popularity (2 pts)",
    "text": "3. See how your PCA related to breed popularity (2 pts)\nUsing breed_traits and breed_rank_all, label the points that show data for the top 10 dog breeds in 2020 and color them different from the rest of the points. Your plot does not need to be interactive."
  },
  {
    "objectID": "assignments/capstone.html",
    "href": "assignments/capstone.html",
    "title": "Capstone assignment",
    "section": "",
    "text": "At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nThere will be 1 capstone assignment.\nDue Date: The capstone plan will be due 11/7/2023. The capstone assignment will be due 12/8/2023.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/capstone.html#capstone-assignment",
    "href": "assignments/capstone.html#capstone-assignment",
    "title": "Capstone assignment",
    "section": "",
    "text": "At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nThere will be 1 capstone assignment.\nDue Date: The capstone plan will be due 11/7/2023. The capstone assignment will be due 12/8/2023.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/reflections.html",
    "href": "assignments/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "After each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday September 1, the reflection for that class is due on Tuesday September 8 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/reflections.html#reflections",
    "href": "assignments/reflections.html#reflections",
    "title": "Reflections",
    "section": "",
    "text": "After each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday September 1, the reflection for that class is due on Tuesday September 8 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html",
    "href": "assignments/modules/module3/module_3_assignment.html",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "This is your assignment for Module 3, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info:\n\nPlease submit this assignment by uploading a knitted .html to Carmen\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing\nMake sure you include the Code Download button so that I can see your code as well\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\nThis assignment will be due on Friday, October 31, 2023, at 11:59pm.\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)"
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#introduction",
    "href": "assignments/modules/module3/module_3_assignment.html#introduction",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "This is your assignment for Module 3, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info:\n\nPlease submit this assignment by uploading a knitted .html to Carmen\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing\nMake sure you include the Code Download button so that I can see your code as well\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\nThis assignment will be due on Friday, October 31, 2023, at 11:59pm.\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)"
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#data-distributions-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#data-distributions-visualization-3-pts",
    "title": "Module 3 Assignment",
    "section": "1. Data distributions visualization (3 pts)",
    "text": "1. Data distributions visualization (3 pts)\nCreate a visualization that shows the distribution of tuition costs (both in_state_tuition and out_of_state_tuition) across public, private, and for-profit universities and colleges. You can use whatever type of plot you think is appropriate to show this distribution across different types of universities. Your plot should be publication ready quality."
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#adding-statistics-visualization-4-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#adding-statistics-visualization-4-pts",
    "title": "Module 3 Assignment",
    "section": "2. Adding statistics visualization (4 pts)",
    "text": "2. Adding statistics visualization (4 pts)\nMake a plot that shows the difference in early_career_pay across private and public universities/colleges. Is there any statistical difference in pay across these two categories of institutions? Is the same true for mid_career_pay? This can be either one or two plots, its up to you. Make sure you are doing the right statistical test appropriate for your data."
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#understanding-correlations-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#understanding-correlations-visualization-3-pts",
    "title": "Module 3 Assignment",
    "section": "3. Understanding correlations visualization (3 pts)",
    "text": "3. Understanding correlations visualization (3 pts)\nMake a visualization that investigates and then visualizes correlation between early_career_pay, mid_career_pay and university tuition (both in state and out of state) showing correlation coefficients. Show how this is different across public and private universities. If you feel like you want to make a couple plots to display this relationship, that is fine."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html",
    "href": "assignments/modules/module2/module_2_assignment.html",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on RMarkdown, wrangling, ggplot101, and ggplot102.\nYou will submit this assignment by uploading a knitted .html to Carmen. Make sure you include the Code Download button so that I can see your code as well. Customize the YAML and the document so you like how it looks.\nRemember there are often many ways to reach the same end product.\n\nThis assignment will be due on Tuesday, October 3, 2023, at 11:59pm.\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#introduction",
    "href": "assignments/modules/module2/module_2_assignment.html#introduction",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on RMarkdown, wrangling, ggplot101, and ggplot102.\nYou will submit this assignment by uploading a knitted .html to Carmen. Make sure you include the Code Download button so that I can see your code as well. Customize the YAML and the document so you like how it looks.\nRemember there are often many ways to reach the same end product.\n\nThis assignment will be due on Tuesday, October 3, 2023, at 11:59pm.\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#writing-in-markdown-1-1-pt",
    "href": "assignments/modules/module2/module_2_assignment.html#writing-in-markdown-1-1-pt",
    "title": "Module 2 Assignment",
    "section": "Writing in Markdown 1 (1 pt)",
    "text": "Writing in Markdown 1 (1 pt)\nUsing coding in text, write a sentence in markdown that pulls from this data how many total PhDs were awarded in 2017. If you want to make some calculations in a code chunk first that is ok."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-1-3-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-1-3-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 1 (3 pts)",
    "text": "Visualization 1 (3 pts)\nMake a chart to visualize of the total number of PhDs awarded for each broad_field across the total time period of this data. You pick the type of chart that you think is appropriate, and make sure your plot is appropriately labelled and you are happy with how it looks. Hint, to do this you‚Äôll probably have to do some data wrangling first."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-2-3-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-2-3-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 2 (3 pts)",
    "text": "Visualization 2 (3 pts)\nPick the field that most closely matches the area of your degree. Make a line graph (with points for each datapoint) that shows how the number of PhDs awarded in your field has changed from 2008 to 2017. Make sure your x-axis indicates each year for which you have data, your graph is appropriately labelled, and you think it is aesthetically pleasing."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-3-3-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-3-3-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 3 (3 pts)",
    "text": "Visualization 3 (3 pts)\nPick at least 3 additional fields (you can use more if you like) that are adjacent to your Ph.D.¬†field. Make a faceted plot to show the number of degrees awarded in each of these disciplines across the same time period. Make sure you label your plot appropriately and you think it is aesthetic (i.e., if you have squished strip text you want to fix that)."
  },
  {
    "objectID": "assignments/modules/module1/module_1_assignment.html",
    "href": "assignments/modules/module1/module_1_assignment.html",
    "title": "Module 1 Assignment",
    "section": "",
    "text": "This is your assignment for Module 1.\nPlease upload to Carmen:\n\n1 good visualization, along with 1 paragraph about why it is good\n1 bad visualization, along with 1 paragraph about why it is bad.\n\n\nThis assignment will be due on Monday, August 28, 2023, at 11:59pm so we can talk about them in class the next day."
  },
  {
    "objectID": "assignments/modules/module1/module_1_assignment.html#introduction",
    "href": "assignments/modules/module1/module_1_assignment.html#introduction",
    "title": "Module 1 Assignment",
    "section": "",
    "text": "This is your assignment for Module 1.\nPlease upload to Carmen:\n\n1 good visualization, along with 1 paragraph about why it is good\n1 bad visualization, along with 1 paragraph about why it is bad.\n\n\nThis assignment will be due on Monday, August 28, 2023, at 11:59pm so we can talk about them in class the next day."
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D.\nEmail address: cooperstone dot 1 at osu dot edu (preferred contact method)\nPhone number: 614-292-2843 (non-preferred contact method)\nTA: Daniel Quiroz Moreno, quirozmoreno dot 1 at osu dot edu\nOffice hours:\n\nDaniel: Fridays 1-2 pm Howlett 340 (email him if you plan to go) or via Zoom\nJess: Tuesdays 11:30 am - 12:30 pm in Howlett 348 or via Zoom"
  },
  {
    "objectID": "syllabus/syllabus.html#instructor",
    "href": "syllabus/syllabus.html#instructor",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D.\nEmail address: cooperstone dot 1 at osu dot edu (preferred contact method)\nPhone number: 614-292-2843 (non-preferred contact method)\nTA: Daniel Quiroz Moreno, quirozmoreno dot 1 at osu dot edu\nOffice hours:\n\nDaniel: Fridays 1-2 pm Howlett 340 (email him if you plan to go) or via Zoom\nJess: Tuesdays 11:30 am - 12:30 pm in Howlett 348 or via Zoom"
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nThis course aims to introduce students to the principles and practice of data visualization. Students will learn fundamental principles of data visualization and create figures that appropriately and ethically represent their data. Data visualizations will be created in the R programming environment, using tools including the grammar of graphics implemented in ggplot2. In the process of creating visualizations, students will also become familiar with data handling and wrangling in R.\nCourse learning outcomes\nBy the end of this course, students should successfully be able to:\n\nRecall and describe the fundamental goals and principles of data visualization.\nDistinguish between good and bad visualizations, and understand how to make those are ineffective more effective.\nLearn to use R, R Markdown, and ggplot to make clear, descriptive, and aesthetic visualization.\nApply principles learned in class, both theoretical and technical, to create effective visualizations."
  },
  {
    "objectID": "syllabus/syllabus.html#course-delivery",
    "href": "syllabus/syllabus.html#course-delivery",
    "title": "Syllabus",
    "section": "Course delivery",
    "text": "Course delivery\nMode of delivery: This course is a hybrid-delivered course where I provide ~ 50 min of lecture material, followed by ~1 hour of recitation activities to do in-class (with assistance from the instructor and the TA). You can attend class in person or virtually via Zoom. You are also welcome to come to class with questions about the weekly videos, or problems you are currently encountering in creating your visualizations.\nAttendance and participation requirements: This is a hybrid course, but is taught synchronously, meaning it is expected that you attend class, either in person or virtually (via Zoom) during its meeting time. I will not take attendance. If circumstances require you to miss class, it will be expected you watch the recorded sessions, and catch up on material on your own. I have found that students who attend classes more quickly and completely master course content.\nClass recordings: To help you master material, and to better accommodate students, classes will be recorded, and recordings uploaded directly after class to Carmen. You can find a link to a OneDrive folder with the recorded lectures on the Syllabus page on Carmen."
  },
  {
    "objectID": "syllabus/syllabus.html#course-schedule",
    "href": "syllabus/syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course schedule",
    "text": "Course schedule\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2023-08-22\n1: Principles\nPrinciples of data visualization\n\n\n2023-08-29\n1: Principles\nGood and bad visualizations\n\n\n2023-09-05\n2: Coding fundamentals\nR Markdown for reproducible research\n\n\n2023-09-12\n2: Coding fundamentals\nWrangling, the basics\n\n\n2023-09-19\n2: Coding fundamentals\nggplot 101\n\n\n2023-09-26\n2: Coding fundamentals\nThemes, labels, facets (ggplot 102)\n\n\n2023-10-03\n3: Data exploration\nData distributions\n\n\n2023-10-10\n3: Data exploration\nCorrelations\n\n\n2023-10-17\nOpen session, capstone prep\nOpen session, capstone prep\n\n\n2023-10-24\n3: Data exploration\nAnnotating statistics\n\n\n2023-10-31\n4: Putting it together\nPrincipal components analysis\n\n\n2023-11-07\n4: Putting it together\nManhattan plots and making lots of plots at once\n\n\n2023-11-14\n4: Putting it together\nInteractive plots\n\n\n2023-11-21\nNo class, Thanksgiving\nRelaxing and eating\n\n\n2023-11-28\n4: Putting it together\nggplot extension packages and complexheatmap\n\n\n2023-12-05\n4: Putting it together\nCapstone assignment open session"
  },
  {
    "objectID": "syllabus/syllabus.html#course-resources",
    "href": "syllabus/syllabus.html#course-resources",
    "title": "Syllabus",
    "section": "Course resources",
    "text": "Course resources\nThere are no required textbooks for this course, though you will find many of the recommend texts and resources belowvery useful.\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nIntroduction to Data Science by Rafael Irizarry\nData Visualization with R by Rob Kabacoff\nData Visualization, A practical introduction by Kieran Healy\nHands-On Data Visualization by Jack Dougherty and Ilya Ilyankou\nFundamentals of Data Visualization by Claus O. Wilke\nggplot2, Elegant Graphics for Data Analysis by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nModern Data Science with R by Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton\nR Markdown Cookbook by Yihui Xie, Christophe Dervieux, Emily Riederer\nThe tidyverse style guide\nA ggplot2 tutorial for beautiful plotting in R by C√©dric Scherer\nRStudio cheatsheets, including ggplot2, dplyr, tidyr, readr and other data import packages, stringr for managing character strings, forcats for managing factors, and R Markdown"
  },
  {
    "objectID": "syllabus/syllabus.html#required-software",
    "href": "syllabus/syllabus.html#required-software",
    "title": "Syllabus",
    "section": "Required software",
    "text": "Required software\n\nR: We will use the R programming environment for this class https://www.r-project.org/ (free). You can do so many things in R (including building this course website).\nRStudio Desktop: This IDE (integrated development environment) allows a user-friendly interface with the R programming environment, which we will use in class as well. You must have R before you download RStudio (free).\nMicrosoft Office 365: All Ohio State students are now eligible for free Microsoft Office 365. Full instructions for downloading and installation can be found at go.osu.edu/office365help."
  },
  {
    "objectID": "syllabus/syllabus.html#prior-r-experience",
    "href": "syllabus/syllabus.html#prior-r-experience",
    "title": "Syllabus",
    "section": "Prior R experience",
    "text": "Prior R experience\nYou do not need to be an R expert for this class, but I will assume working-level knowledge of R programming. If you have no experience with R, but would still like to take this class, you can. I ask then you get yourself up to speed by taking this free online class https://www.edx.org/course/data-science-r-basics (audit only) before the start of the 3rd week of class. The course will take 8-16 hours to complete so please leave yourself enough time to do so before week 3. Tips and tricks in R will be scattered throughout the course material."
  },
  {
    "objectID": "syllabus/syllabus.html#grading",
    "href": "syllabus/syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nHow your grade is calculated\n\nModule assignments: 40 points (10 points per assignment, 4 assignments)\nClass reflections: 20 points (2 points per reflection, 10 reflections)\nCapstone assignment: 40 points\n\nSee the Assignments tab for additional information.\n\n\nAssignment descriptions\n\nModule assignments\nDescription: After each module, there will be an assignment to provide practice for the techniques learned in class. Assignments will be posted at least one week prior to their due date, and due dates can be found on Carmen.\nGrading: Each part of the assignment will have a certain number of points associated with it, provided along with the assignment.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nClass reflections\nAfter each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday September 1, the reflection for that class is due on Tuesday September 8 by 11:59pm.\nGrading: Reflections will be graded as follows:\n\nFull credit: Reflections thoughtfully engage with course content, demonstrate the student thought about material and how it would (or wouldn‚Äôt) be relevant to their work and development. This is the level of engagement I expect for this course.\nHalf credit: Reflections are superficial and demonstrate minimal engagement with the course content. A half credit grade indicates better engagement is required for the next reflection. I do not expect to assign these grades often.\n\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nCapstone assignment\nDescription: At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nGrading: Guidance will be provided for the grading of the capstone assignment. Assignments completed outside R Markdown, or not knitted to a .html are not acceptable.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\n\nLate assignments\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.\nPlease refer to the Assignments tab or Carmen for due dates.\n\n\nGrading scale\n\n\n\n\n\nScore\nGrade\n\n\n\n\n93‚Äì100\nA\n\n\n90‚Äì92.9\nA-\n\n\n87‚Äì89.9\nB+\n\n\n83‚Äì86.9\nB+\n\n\n80‚Äì82.9\nB-\n\n\n77‚Äì79.9\nC+\n\n\n73‚Äì76.9\nC\n\n\n70‚Äì72.9\nC-\n\n\n67‚Äì69.9\nD+\n\n\n60‚Äì66.9\nD\n\n\nBelow 60\nE\n\n\n\n\n\n\n\nInstructor feedback and response time\n\nGrading and feedback: For assignments, you can generally expect feedback within 7 days.\nEmail: I will reply to emails within 48 hours on days when class is in session at the university."
  },
  {
    "objectID": "syllabus/syllabus.html#other-course-policies",
    "href": "syllabus/syllabus.html#other-course-policies",
    "title": "Syllabus",
    "section": "Other course policies",
    "text": "Other course policies\n\nDiscussion and communication guidelines\nI expect all communication will be respectful and thoughtful.\n\n\nAcademic Misconduct/Academic Integrity\nAcademic integrity is essential to maintaining an environment that fosters excellence in teaching, research, and other educational and scholarly activities. Thus, The Ohio State University and the Committee on Academic Misconduct (COAM) expect that all students have read and understand the University‚Äôs Code of Student Conduct, and that all students will complete all academic and scholarly assignments with fairness and honesty. Students must recognize that failure to follow the rules and guidelines established in the University‚Äôs Code of Student Conduct and this syllabus may constitute Academic Misconduct.\nThe Ohio State University‚Äôs Code of Student Conduct (Section 3335-23-04) defines academic misconduct as: Any activity that tends to compromise the academic integrity of the University, or subvert the educational process. Examples of academic misconduct include (but are not limited to) plagiarism, collusion (unauthorized collaboration), copying the work of another student, and possession of unauthorized materials during an examination. Ignorance of the University‚Äôs Code of Student Conduct is never considered an excuse for academic misconduct, so I recommend that you review the Code of Student Conduct and, specifically, the sections dealing with academic misconduct.\nIf I suspect that a student has committed academic misconduct in this course, I am obligated by University Rules to report my suspicions to the Committee on Academic Misconduct. If COAM determines that you have violated the University‚Äôs Code of Student Conduct (i.e., committed academic misconduct), the sanctions for the misconduct could include a failing grade in this course and suspension or dismissal from the University.\nIf you have any questions about the above policy or what constitutes academic misconduct in this course, please contact me.\n\n\nCreating an environment free from harassment, discrimination, and sexual misconduct\nThe Ohio State University is committed to building and maintaining a community to reflect diversity and to improve opportunities for all. All Buckeyes have the right to be free from harassment, discrimination, and sexual misconduct. Ohio State does not discriminate on the basis of age, ancestry, color, disability, ethnicity, gender, gender identity or expression, genetic information, HIV/AIDS status, military status, national origin, pregnancy (childbirth, false pregnancy, termination of pregnancy, or recovery therefrom), race, religion, sex, sexual orientation, or protected veteran status, or any other bases under the law, in its activities, academic programs, admission, and employment. Members of the university community also have the right to be free from all forms of sexual misconduct: sexual harassment, sexual assault, relationship violence, stalking, and sexual exploitation.\nTo report harassment, discrimination, sexual misconduct, or retaliation and/or seek confidential and non-confidential resources and supportive measures, contact the Office of Institutional Equity by: 1. Online reporting form at equity.osu.edu, 2. Call 614-247-5838 or TTY 614-688-8605, 3. Or Email equity@osu.edu\n\n\nDiversity\nThe Ohio State University affirms the importance and value of diversity of people and ideas. We believe in creating equitable research opportunities for all students and to providing programs and curricula that allow our students to understand critical societal challenges from diverse perspectives and aspire to use research to promote sustainable solutions for all. We are committed to maintaining an inclusive community that recognizes and values the inherent worth and dignity of every person; fosters sensitivity, understanding, and mutual respect among all members; and encourages each individual to strive to reach their own potential. The Ohio State University does not discriminate on the basis of age, ancestry, color, disability, gender identity or expression, genetic information, HIV/AIDS status, military status, national origin, race, religion, sex, gender, sexual orientation, pregnancy, protected veteran status, or any other bases under the law, in its activities, academic programs, admission, and employment.\nIn addition, this course adheres to The Principles of Community adopted by the College of Food, Agricultural, and Environmental Sciences. These principles are located on the Carmen site for this course; and can also be found at https://go.osu.edu/principlesofcommunity. For additional information on Diversity, Equity, and Inclusion in CFAES, contact the CFAES Office for Diversity, Equity, and Inclusion (https://equityandinclusion.cfaes.ohio-state.edu/). If you have been a victim of or a witness to a bias incident, you can report it online and anonymously (if you choose) at https://equity.osu.edu/.\n\n\nCounseling and Consultation Services/Mental Health\nAs a student you may experience a range of issues that can cause barriers to learning, such as strained relationships, increased anxiety, alcohol/drug problems, feeling down, difficulty concentrating and/or lack of motivation. These mental health concerns or stressful events may lead to diminished academic performance or reduce a student‚Äôs ability to participate in daily activities. The Ohio State University offers services to assist you with addressing these and other concerns you may be experiencing. If you or someone you know are suffering from any of the aforementioned conditions, you can learn more about the broad range of confidential mental health services available on campus via the Office of Student Life Counseling and Consultation Services (CCS) by visiting ccs.osu.edu or calling (614) 292- 5766. CCS is located on the 4th Floor of the Younkin Success Center and 10th Floor of Lincoln Tower. You can reach an on-call counselor when CCS is closed at (614) 292-5766 and 24 hour emergency help is also available through the 24/7 National Suicide Prevention Hotline at 1-(800)-273-TALK or at suicidepreventionlifeline.org\nDavid Wirt, wirt.9@osu.edu, is the CFAES embedded mental health counselor in Columbus. He is available for new consultations and to establish routine care. To schedule with David, please call 614-292-5766. Students should mention their affiliation with CFAES when setting up a phone screening.\nDr.¬†Schaad, schaad.15@osu.edu, is the CFAES embedded mental health counselor in Wooster. She is available for new consultations and to establish routine care. To schedule with Dr.¬†Schaad, please call 614-292-5766. Students should mention their affiliation with CFAES when setting up a phone screening.\n\n\nLand Acknowledgement\nWe would like to acknowledge the land that The Ohio State University occupies is the ancestral and contemporary lands of the Shawnee, Potawatomi, Delaware, Miami, Peoria, Seneca, Wyandotte, Ojibwe and Cherokee peoples. The university resides on land ceded in the 1795 Treaty of Greeneville and the forced removal of tribes through the Indian Removal Act of 1830. We honor the resiliency of these tribal nations and recognize the historical contexts that have and continue to affect the Indigenous peoples of this land.\n\n\nAccessibility accomodations\nThe university strives to make all learning experiences as accessible as possible. In light of the current pandemic, students seeking to request COVID-related accommodations may do so through the university‚Äôs request process, managed by Student Life Disability Services. If you anticipate or experience academic barriers based on your disability (including mental health, chronic, or temporary medical conditions), please let me know immediately so that we can privately discuss options. To establish reasonable accommodations, I may request that you register with Student Life Disability Services. After registration, make arrangements with me as soon as possible to discuss your accommodations so that they may be implemented in a timely fashion.\nSLDS contact information: slds@osu.edu; 614-292-3307; slds.osu.edu; 098 Baker Hall, 113 W. 12th Avenue."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html",
    "href": "rmds/03_Rmd_recitation_solutions.html",
    "title": "R Markdown Recitation Solutions",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html#r-markdown-recitation",
    "href": "rmds/03_Rmd_recitation_solutions.html#r-markdown-recitation",
    "title": "R Markdown Recitation Solutions",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html#second-biggest-header",
    "href": "rmds/03_Rmd_recitation_solutions.html#second-biggest-header",
    "title": "R Markdown Recitation Solutions",
    "section": "Second biggest header",
    "text": "Second biggest header\n\nThird biggest header\n\n\nFour biggest header\nYou get the idea.\nAdd a hyperlink to our class website\nAdd an image. Note you will do this differently if you are adding an image from internet vs one you have on your local machine. Also remember your working directory is the location of your Rmd and you may want to have a directory called img where images are stored.\n\n\n\nThis is my dog Nacho\n\n\nAdd a block quote\n\nHere is an important block quote\n\nMake a bulleted list\n\nThing 1\nThing 2\nThing 3\nThis also works\nTo make\nA bulleted list\n\n\nYou can also\nMake\nNumbered lists"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html",
    "title": "Leftover tidbits recitation",
    "section": "",
    "text": "Today‚Äôs recitation materials are on a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\nlibrary(gghighlight) # for highlighting\nlibrary(gganimate) # animating plots\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\nlibrary(scales) # for easy scaling\nlibrary(plotly) # for ggplotly\nlibrary(glue) # for easy pasting\n\nlibrary(gapminder) # for data for viz2"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#introduction",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#introduction",
    "title": "Leftover tidbits recitation",
    "section": "",
    "text": "Today‚Äôs recitation materials are on a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\nlibrary(gghighlight) # for highlighting\nlibrary(gganimate) # animating plots\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\nlibrary(scales) # for easy scaling\nlibrary(plotly) # for ggplotly\nlibrary(glue) # for easy pasting\n\nlibrary(gapminder) # for data for viz2"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#really-start-using-an-rproject",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#really-start-using-an-rproject",
    "title": "Leftover tidbits recitation",
    "section": "Really start using an Rproject üìΩÔ∏è",
    "text": "Really start using an Rproject üìΩÔ∏è\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nIf you don‚Äôt have a Rproject for class, set one up."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-1",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-1",
    "title": "Leftover tidbits recitation",
    "section": "Visualization 1",
    "text": "Visualization 1\nWe are going to interrogate a dataset from Gapminder that includes information about Happiness Scores collected across different countries and years.\nCreate a visualization that shows the happiness scores for all countries from 2008 to 2010. Highlight in some way the top 3 countries with the highest happiness scores per continent.\nI‚Äôve put the data on Github so you can easily download it with the code below. Note, the question asks you to make a plot considering continent so I‚Äôve also provided you a key that has each country, and the continent to which it belows for you to join together.\n\nLoad data\n\nhappiness &lt;- read_csv(\"https://github.com/jcooperstone/dataviz-site/raw/master/4_12_leftovers/data/hapiscore_whr.csv\")\n\ncountry_continent &lt;- read_csv(\"https://github.com/jcooperstone/dataviz-site/raw/master/4_12_leftovers/data/country_continent.csv\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-2",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-2",
    "title": "Leftover tidbits recitation",
    "section": "Visualization 2",
    "text": "Visualization 2\nRecreate a plot in the vein of the one here. You can make the same interactive plot (use the data from 2007, which is slightly older and different from what you see in the online plot), or choose to animate it over year. Or do both.\nUse the data gapminder::gapminder which you can access from R."
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots_recitation.html",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots_recitation.html",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "",
    "text": "Today we are going to work with microbiome data. In this recitation we are going to provide microbiome data as the result of the shotgun sequencing of the pig gut microbiome.\n\nPig microbiome study: paper, data."
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots_recitation.html#how-many-rows-and-columns-do-the-data-have",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots_recitation.html#how-many-rows-and-columns-do-the-data-have",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "How many rows and columns do the data have?",
    "text": "How many rows and columns do the data have?"
  },
  {
    "objectID": "modules/module4/12_interactive-plots/12_interactive-plots_recitation.html#how-many-phyla-do-the-data-contains-and-how-many-columns-represents-metadata-of-the-experiment",
    "href": "modules/module4/12_interactive-plots/12_interactive-plots_recitation.html#how-many-phyla-do-the-data-contains-and-how-many-columns-represents-metadata-of-the-experiment",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "How many phyla do the data contains and how many columns represents metadata of the experiment?",
    "text": "How many phyla do the data contains and how many columns represents metadata of the experiment?\n\nCreate a new column with a new phyla assignation\nKeep the phyla when they are Firmicutes or Bacteroidetes, otherwise assign Phyla to ‚ÄúOther phyla‚Äù.\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint: You may need to pivot the data to evaluate the column names as observations\n\n\n\n\n\nCompute the cumulative abundance by the new Phyla levels that you created\n\n\nCreate the bar plot in ggplot\n\n\nCreate the plot interactive"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan.html",
    "href": "modules/module4/11_manhattan/11_manhattan.html",
    "title": "Manhattan Plots",
    "section": "",
    "text": "Today we are going to continue putting it together in Module 4. Today‚Äôs material is on making Manhattan plots, which is a commonly used plot type for visualizing the result of genome wide association studies (GWAS). The name comes from its resemblance to the skyscrapers in Manhattan, poking above the background of the rest of the buildings.\n\n\n\n\n\nFigure from Wikipedia\n\n\n\n\nThe plot visualizes the relationship between a trait and genetic markers. The x-axis shows the position on each chromosome, and the y-axis shows the negative log (usually log10) p-value of the quantitative response of a trait to that specific marker. Negative log10 p-value is used because a significant p-value is always small, and this transformation converts low p-value to a number that can be seen easily among the background of non-significant associations.\nIf you work in genetics/genomics, it is likely you will create Manhattan plots. Even if you think you‚Äôll never make one of these types of plots, its a useful activity to see additional ways of customizing your plots.\n\nlibrary(tidyverse) # everything\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels\n\n\n\nToday we are going to continue to use some different real research data collected by Emma Bilbrey from my team where we conducted many GWAS in apple. This work was published in 2021 in New Phytologist and can be found here. This data is more complex than a typical GWAS so we are only going to use a small portion of it.\nWe will be reading in Table S16 which includes the -log10 p-values for the GWAS conducted across all apples for all features found in the LC-MS negative ionization mode metabolomics dataset.\nThe data is present in a .csv file, so we will use the function read_csv() from the tidyverse. We want to import Supplemental Table 16.\nThis will take a second, its a big file.\n\ngwas &lt;- read_csv(\"data/nph17693-sup-0007-tables16.csv\") # be patient\n\n\n\nRows: 11165 Columns: 4704\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4704): Index, Linkage_Group, Genetic_Distance, X885.2037_2.98177, X525....\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat are the dimensions of this dataframe? What kind of object is it?\n\ndim(gwas)\n\n[1] 11165  4704\n\nclass(gwas)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nBecause this dataframe is so big, if we use head(gwas) we will get a print out of the first 6 rows, and all the columns. In thi case there are 4704 columns so that will be unwieldy.\nEmma came up with a simple way to approach this when she was writing her code, she wrote herself a little function that she could use regularly to extract out the first 5 rows, and the first 5 columns, without having to index each time.\nIf we wanted to just see the first 5 rows, the first 5 columsn we could do this:\n\ngwas[1:5,1:5]\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\nhead_short &lt;- function(x){\n  x[1:5,1:5] # this function shows the first 5 rows and columns of an object\n  } \n\nNow instead of indexing all the time, we can just run head_short() which I think is easier. We will talk a little bit more about writing functions in the class on making many plots at once.\n\nhead_short(gwas)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\n\n\nHow many markers are included here?\n\nnrow(gwas)\n\n[1] 11165\n\n\nHow many linkage groups do we have? (Each linkage group is a chromosome.)\n\nunique(gwas$Linkage_Group)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n\n\nWhat is the range of Genetic_Distance for each chromosome?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  summarize(min_genetic_distance = min(Genetic_Distance),\n            max_genetic_distance = max(Genetic_Distance))\n\n# A tibble: 17 √ó 3\n   Linkage_Group min_genetic_distance max_genetic_distance\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n 1             1                0                     63.1\n 2             2                0                     78.4\n 3             3                0                     74.0\n 4             4                0.004                 65.5\n 5             5                0                     77.8\n 6             6                0                     75.3\n 7             7                0.001                 82.4\n 8             8                0                     68.5\n 9             9                0.292                 67.0\n10            10                0                     81.3\n11            11                0                     80.9\n12            12                0.382                 65.4\n13            13                0                     71.4\n14            14                0                     64.4\n15            15                0                    112. \n16            16                0.001                 67.5\n17            17                0                     71.8\n\n\nHow are the Index distributed across Linkage_Group?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  summarize(min_index = min(Index),\n            max_index = max(Index))\n\n# A tibble: 17 √ó 3\n   Linkage_Group min_index max_index\n           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1             1         1       663\n 2             2       664      1687\n 3             3      1688      2630\n 4             4      2635      3432\n 5             5      3433      4530\n 6             6      4533      5266\n 7             7      5270      5934\n 8             8      5936      6792\n 9             9      6793      7623\n10            10      7624      8648\n11            11      8652      9728\n12            12      9731     10719\n13            13     10721     11558\n14            14     11560     12365\n15            15     12366     13605\n16            16     13610     14428\n17            17     14431     15260\n\n\nOk here we can see Index does not repeat, but Genetic_Distance restarts with each chromosome."
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan.html#introduction",
    "href": "modules/module4/11_manhattan/11_manhattan.html#introduction",
    "title": "Manhattan Plots",
    "section": "",
    "text": "Today we are going to continue putting it together in Module 4. Today‚Äôs material is on making Manhattan plots, which is a commonly used plot type for visualizing the result of genome wide association studies (GWAS). The name comes from its resemblance to the skyscrapers in Manhattan, poking above the background of the rest of the buildings.\n\n\n\n\n\nFigure from Wikipedia\n\n\n\n\nThe plot visualizes the relationship between a trait and genetic markers. The x-axis shows the position on each chromosome, and the y-axis shows the negative log (usually log10) p-value of the quantitative response of a trait to that specific marker. Negative log10 p-value is used because a significant p-value is always small, and this transformation converts low p-value to a number that can be seen easily among the background of non-significant associations.\nIf you work in genetics/genomics, it is likely you will create Manhattan plots. Even if you think you‚Äôll never make one of these types of plots, its a useful activity to see additional ways of customizing your plots.\n\nlibrary(tidyverse) # everything\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels\n\n\n\nToday we are going to continue to use some different real research data collected by Emma Bilbrey from my team where we conducted many GWAS in apple. This work was published in 2021 in New Phytologist and can be found here. This data is more complex than a typical GWAS so we are only going to use a small portion of it.\nWe will be reading in Table S16 which includes the -log10 p-values for the GWAS conducted across all apples for all features found in the LC-MS negative ionization mode metabolomics dataset.\nThe data is present in a .csv file, so we will use the function read_csv() from the tidyverse. We want to import Supplemental Table 16.\nThis will take a second, its a big file.\n\ngwas &lt;- read_csv(\"data/nph17693-sup-0007-tables16.csv\") # be patient\n\n\n\nRows: 11165 Columns: 4704\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4704): Index, Linkage_Group, Genetic_Distance, X885.2037_2.98177, X525....\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat are the dimensions of this dataframe? What kind of object is it?\n\ndim(gwas)\n\n[1] 11165  4704\n\nclass(gwas)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nBecause this dataframe is so big, if we use head(gwas) we will get a print out of the first 6 rows, and all the columns. In thi case there are 4704 columns so that will be unwieldy.\nEmma came up with a simple way to approach this when she was writing her code, she wrote herself a little function that she could use regularly to extract out the first 5 rows, and the first 5 columns, without having to index each time.\nIf we wanted to just see the first 5 rows, the first 5 columsn we could do this:\n\ngwas[1:5,1:5]\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\nhead_short &lt;- function(x){\n  x[1:5,1:5] # this function shows the first 5 rows and columns of an object\n  } \n\nNow instead of indexing all the time, we can just run head_short() which I think is easier. We will talk a little bit more about writing functions in the class on making many plots at once.\n\nhead_short(gwas)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\n\n\nHow many markers are included here?\n\nnrow(gwas)\n\n[1] 11165\n\n\nHow many linkage groups do we have? (Each linkage group is a chromosome.)\n\nunique(gwas$Linkage_Group)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n\n\nWhat is the range of Genetic_Distance for each chromosome?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  summarize(min_genetic_distance = min(Genetic_Distance),\n            max_genetic_distance = max(Genetic_Distance))\n\n# A tibble: 17 √ó 3\n   Linkage_Group min_genetic_distance max_genetic_distance\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n 1             1                0                     63.1\n 2             2                0                     78.4\n 3             3                0                     74.0\n 4             4                0.004                 65.5\n 5             5                0                     77.8\n 6             6                0                     75.3\n 7             7                0.001                 82.4\n 8             8                0                     68.5\n 9             9                0.292                 67.0\n10            10                0                     81.3\n11            11                0                     80.9\n12            12                0.382                 65.4\n13            13                0                     71.4\n14            14                0                     64.4\n15            15                0                    112. \n16            16                0.001                 67.5\n17            17                0                     71.8\n\n\nHow are the Index distributed across Linkage_Group?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  summarize(min_index = min(Index),\n            max_index = max(Index))\n\n# A tibble: 17 √ó 3\n   Linkage_Group min_index max_index\n           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1             1         1       663\n 2             2       664      1687\n 3             3      1688      2630\n 4             4      2635      3432\n 5             5      3433      4530\n 6             6      4533      5266\n 7             7      5270      5934\n 8             8      5936      6792\n 9             9      6793      7623\n10            10      7624      8648\n11            11      8652      9728\n12            12      9731     10719\n13            13     10721     11558\n14            14     11560     12365\n15            15     12366     13605\n16            16     13610     14428\n17            17     14431     15260\n\n\nOk here we can see Index does not repeat, but Genetic_Distance restarts with each chromosome."
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan.html#manhattan-plot-chlorogenic-acid",
    "href": "modules/module4/11_manhattan/11_manhattan.html#manhattan-plot-chlorogenic-acid",
    "title": "Manhattan Plots",
    "section": "Manhattan plot: chlorogenic acid",
    "text": "Manhattan plot: chlorogenic acid\nAt its core, a Manhattan plot is a scatter plot. The data we are working with has 4701 traits, which here are relative metabolite abundance. We are going to pick one metabolite to start working with.\nWe will start with the feature that represents chlorogenic acid, a caffeoyl-quinic acid you find in apples. The column we want is X353.09194_2.23795. The data is already present as the -log10 p-value for the relationship between allelic variation at that marker, and relative abundance of chlorogenic acid.\n\n# rename X353.09194_2.23795 to chlorogenic_acid\ngwas &lt;- gwas %&gt;%\n  rename(chlorogenic_acid = `X353.09194_2.23795`)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point()\n\n\n\n\nSee how color is plotted on a continuous scale? This is because Linkage_Group is a continuous, numeric variable. Since each chromosome is actually discrete, let‚Äôs convert Linkage_Group to a factor and then plot again.\n\nLinkage_Group as a factor\n\ngwas$Linkage_Group &lt;- as.factor(gwas$Linkage_Group)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point()\n\n\n\n\nBetter but this really isn‚Äôt what we want. We want our x-axis to indicate the chromosome number in the middle of the block of that chromosome, not label by Index which just is a key for linking back to each specific marker.\n\n\nSet axis\nIf we want to label the x-axis with breaks for each chromosome, we have to do some wrangling first. Just like we did some calculations in the lesson on adding statistics, we will calculate some min, center, and max for each chromosome so we know where to put the labels.\n\n(set_axis &lt;- gwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  summarize(min = min(Index),\n            max = max(Index),\n            center = (max - min)/2))\n\n# A tibble: 17 √ó 4\n   Linkage_Group   min   max center\n   &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 1                 1   663   331 \n 2 2               664  1687   512.\n 3 3              1688  2630   471 \n 4 4              2635  3432   398.\n 5 5              3433  4530   548.\n 6 6              4533  5266   366.\n 7 7              5270  5934   332 \n 8 8              5936  6792   428 \n 9 9              6793  7623   415 \n10 10             7624  8648   512 \n11 11             8652  9728   538 \n12 12             9731 10719   494 \n13 13            10721 11558   418.\n14 14            11560 12365   402.\n15 15            12366 13605   620.\n16 16            13610 14428   409 \n17 17            14431 15260   414.\n\n\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  theme_classic() +\n  theme(legend.position = \"none\") + # legend not really necessary\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"GWAS of chlorogenic acid in apple\")\n\n\n\n\n\n\nAlternate colors\nHaving a rainbow of colors is not really necessary here,a nd in fact telling exactly where chromosome 15 ends and 16 begins is difficult because the colors are so similar.\nWhat you will see in a lot of papers is people simply alternate the colors of their points by chromosome so you can easily tell which points belong to which chromosome.\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nRemoving that annoying front gap\nThe gap between chromosome 1 and the y-axis of the plot sort of bothers me. Let‚Äôs remove it.\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(expand = c(0,0), # remove gap between y-axis and chr1\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"grey52\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nAdd p-value hline\n\n# what would the pvalue cut off with a bonferroni correction be?\nbonferroni_pval &lt;- -log10(0.05/nrow(gwas))\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nColor sig points\n\n# select all SNPs with -log10 pvalue &gt; bonferroni cutoff for chlorogenic acid\nchlorogenic_acid_sig &lt;- gwas %&gt;%\n  filter(chlorogenic_acid &gt; bonferroni_pval) %&gt;%\n  select(Index, Linkage_Group, Genetic_Distance, chlorogenic_acid)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_point(data = chlorogenic_acid_sig, \n             aes(x = Index, y = chlorogenic_acid), color = \"red\") +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nLabel most sig marker\nWe might be interested to know the marker that has the most significant association with chlorogenic acid content, and label it on our plot.\n\nbiggest_pval &lt;- chlorogenic_acid_sig %&gt;% \n  filter(chlorogenic_acid == max(chlorogenic_acid))\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_point(data = chlorogenic_acid_sig, \n             aes(x = Index, y = chlorogenic_acid), color = \"red\") +\n  geom_label_repel(data = biggest_pval,\n                   aes(x = Index, y = chlorogenic_acid, label = glue(\"Index: {Index}\"))) +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan.html#investigating-other-traits",
    "href": "modules/module4/11_manhattan/11_manhattan.html#investigating-other-traits",
    "title": "Manhattan Plots",
    "section": "Investigating other traits",
    "text": "Investigating other traits\nIn this study, we conducted a series of GWAS on thousands of metabolomic features in apple. What if we wanted to see Manhattan plots for certain features based on how important we could predict they would be? For example, what if we want to see the Manhattan plot for the feature with biggest -log10p-value? Or the feature that has a significant association with the largest number of markers?\nTo make this wrangling easier, we will convert our data, as we have many times before, from wide to long with pivot_longer().\n\nWide to long (again)\n\ngwas_tidy &lt;- gwas %&gt;%\n  pivot_longer(cols = starts_with(\"X\"),\n               names_to = \"Feature\",\n               values_to = \"NegLog10P\")\n\n\n\nSet p-value cutoff\nWe can make another df that includes only the features that have at least one marker where there is a significant p-value.\n\n# make df of associations that pass bonferroni correction\ngwas_tidy_bonferroni &lt;- gwas_tidy %&gt;%\n  filter(NegLog10P &gt; bonferroni_pval)\n\n# how many unique features are there?\nlength(unique(gwas_tidy_bonferroni$Feature))\n\n[1] 962\n\n# how many unique markers are there?\nlength(unique(gwas_tidy_bonferroni$Index))\n\n[1] 544\n\n\nThere are 962 unique features/metabolite that have a Bonferroni adjusted significant p-value with at least one marker. There are 544 unique markers that have a Bonferroni adjusted significant p-value with at least one feature/metabolite.\n\n\nData investigating\nWhat features are associated with the largest number of markers?\n\ngwas_tidy_bonferroni %&gt;%\n  group_by(Feature) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 962 √ó 2\n# Groups:   Feature [962]\n   Feature                n\n   &lt;chr&gt;              &lt;int&gt;\n 1 X417.13237_1.82968    46\n 2 X349.15073_1.79191    44\n 3 X601.13217_2.40546    34\n 4 X593.12835_2.53465    31\n 5 X291.0768_2.44657     30\n 6 X591.1485_2.86273     30\n 7 X637.09169_2.78692    30\n 8 X661.08791_2.10005    30\n 9 X137.02484_2.44808    29\n10 X561.13983_2.53357    29\n# ‚Ñπ 952 more rows\n\n\nWow, the marker X417.13237_1.82968 has significant associations with 46 markers. What would that Manhattan plot look like?\n\ngwas_tidy %&gt;%\n  filter(Feature == \"X417.13237_1.82968\") %&gt;%\n  ggplot(aes(x = Index, y = NegLog10P, color = Linkage_Group)) +\n  geom_point() +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for 417.13237 m/z at retention time 1.82968 in Apple\")"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan.html#making-many-plots-at-once",
    "href": "modules/module4/11_manhattan/11_manhattan.html#making-many-plots-at-once",
    "title": "Manhattan Plots",
    "section": "Making many plots at once",
    "text": "Making many plots at once\nWhat if we want to make Manhattan plots for the 50 features/metabolites that are associated with the most markers? This is probably too many plots to facet, so we can do some calculations, and then write a function to make plots, and apply it over our dataframe.\nFirst, how many significant associations with a Bonferroni multiple testing correction are there?\n\n# make df of associations that pass bonferroni correction\ngwas_tidy_bonferroni &lt;- gwas_tidy %&gt;%\n  filter(NegLog10P &gt; bonferroni_pval)\n\n# how many unique features are this?\ngwas_tidy_bonferroni %&gt;%\n  count(Feature) \n\n# A tibble: 962 √ó 2\n   Feature                 n\n   &lt;chr&gt;               &lt;int&gt;\n 1 X1000.22158_2.71331    12\n 2 X1000.72569_2.72017     2\n 3 X1001.23392_2.70506     5\n 4 X1008.71962_2.91507     3\n 5 X1008.72084_2.64352     2\n 6 X1009.22592_2.91262     5\n 7 X1009.72353_2.64479     2\n 8 X1010.22369_2.64604     2\n 9 X1010.72865_2.64538     1\n10 X1014.70219_2.12784     2\n# ‚Ñπ 952 more rows\n\n# how many unique markers are there?\ngwas_tidy_bonferroni %&gt;%\n  count(Index) \n\n# A tibble: 544 √ó 2\n   Index     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   170     1\n 2   217     1\n 3   218     1\n 4   233     2\n 5   294     1\n 6   311     1\n 7   341     2\n 8   368     1\n 9   386     4\n10   520     6\n# ‚Ñπ 534 more rows\n\n\nWhich features are associated with the largest number of markers?\n\ngwas_tidy_bonferroni %&gt;%\n  count(Feature) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 962 √ó 2\n   Feature                n\n   &lt;chr&gt;              &lt;int&gt;\n 1 X417.13237_1.82968    46\n 2 X349.15073_1.79191    44\n 3 X601.13217_2.40546    34\n 4 X593.12835_2.53465    31\n 5 X291.0768_2.44657     30\n 6 X591.1485_2.86273     30\n 7 X637.09169_2.78692    30\n 8 X661.08791_2.10005    30\n 9 X137.02484_2.44808    29\n10 X561.13983_2.53357    29\n# ‚Ñπ 952 more rows\n\n\nWhich markers are associated with the largest number of features?\n\ngwas_tidy_bonferroni %&gt;%\n  count(Index) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 544 √ó 2\n   Index     n\n   &lt;dbl&gt; &lt;int&gt;\n 1 13684   320\n 2 13685   318\n 3 13681   317\n 4 13715   298\n 5 13657   223\n 6 13660   223\n 7 13675   221\n 8 13630   219\n 9 13623   199\n10 13617   198\n# ‚Ñπ 534 more rows\n\n\nWe will make a new df that includes only the 50 features with the most makers associated with them.\n\n# create a df with only the top 50 features with the most marker associations\ntop50 &lt;- gwas_tidy_bonferroni %&gt;%\n  count(Feature) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 50)\n\n# filter the whole dataset to include only these features\ngwas_top50 &lt;- gwas_tidy %&gt;%\n  filter(Feature %in% top50$Feature)\n\n# go from long to wide\ngwas_top50_wide &lt;- gwas_top50 %&gt;%\n  pivot_wider(names_from = Feature, values_from = NegLog10P)\n\n# what are our new dimensions?\ndim(gwas_top50_wide)\n\n[1] 11165    54\n\nhead_short(gwas_top50_wide)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance chlorogenic_acid X599.12186_2.10421\n  &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n1     1 1                        0                0.361            0.266  \n2     3 1                        0.002            0.239            0.0346 \n3     4 1                        0.003            1.03             0.120  \n4     5 1                        0.004            0.188            0.497  \n5     6 1                        0.005            1.23             0.00356\n\n\n\nWriting a function to plot\nHere, we are just modifying our plotting code slightly to allow it to be run across different features. The first thing we will do is to use our favorite function pivot_longer() to create tidy data.\n\ngwas_top50_long &lt;- gwas_top50_wide %&gt;%\n  pivot_longer(cols = starts_with(\"X\"),\n               names_to = \"feature\",\n               values_to = \"pvalue\")\n\nThen we can write a function to plot, where we will iterate across feature_of_interest. Here, feature_of_interest is just the name I‚Äôve assigned here, but you could easily call it x or i or whatever.\n\n# write a function to make your plots across the features of interest\nmanhattan_plot &lt;- function(feature_of_interest){\n  gwas_top50_long %&gt;% # our df with only the top 50, but long\n  filter(feature == feature_of_interest) %&gt;% # pick the feature_of_interest only\n  ggplot(aes(x = Index, y = pvalue, color = Linkage_Group)) +\n  geom_point() + \n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"gray\"),17)) +\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = glue(\"{feature_of_interest}\")) + # here we glue the feature name in the title\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5))\n  }\n\nBefore trying to use our function on 50 features, let‚Äôs try it out on one. We can provide our feature of interest as a string.\n\nmanhattan_plot(\"X599.12186_2.10421\")\n\n\n\n\nWe are going to iterate over the names features_to_plot. I‚Äôm creating a vector of the names of the features we want to iterate over.\n\nfeatures_to_plot &lt;- unique(gwas_top50_long$feature)\n\n\n\nApplying the function with lapply()\nOnce we have our function written, we can use a function in the apply() family of functions (here, lapply() which applies and creates a list). Here I‚Äôm just printing the first 6 plots in the list.\n\n# use lapply to run your function over the features of interest\n# if you don't want your plots to print, you should assign them to something\nmy_plots &lt;- lapply(X = features_to_plot, # what to iterate over\n                   FUN = manhattan_plot) # what function to use\n\n# print the first 6\nmy_plots[1:6]\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\nSaving out plots\nBut you can print them all, save particular ones usingggsave(), or do what we are going to do here, which is save each of them to a new folder, each as their own .svg because why use raster when you can vectorize.\nFirst we will create a vector of what we want our file names to look like. Then we will save.\n\n# use str_c to combine two character vectors\n# here, features_to_plot and adding .svg so the file name \n# includes the extension type\n# then set that as the names for my_plots\nnames(my_plots) &lt;- str_c(features_to_plot, \".svg\")\n\n# use pwalk to \"walk\" across the different plots and save them\npwalk(list(names(my_plots), my_plots), # what to iterate over and output\n      ggsave, # what the function is\n      path = \"img/\") # where they should go\n\nNow all of your plots are in your working directory. Remember, you need to add the directory img if you want to save with the code I‚Äôm using here."
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan.html#useful-resources",
    "href": "modules/module4/11_manhattan/11_manhattan.html#useful-resources",
    "title": "Manhattan Plots",
    "section": "Useful resources",
    "text": "Useful resources\n\nthe apply() family of functions"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html",
    "title": "Manhattan Plots Recitation Solutions",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html#introduction",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html#introduction",
    "title": "Manhattan Plots Recitation Solutions",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html#investigate-your-data.",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html#investigate-your-data.",
    "title": "Manhattan Plots Recitation Solutions",
    "section": "Investigate your data.",
    "text": "Investigate your data.\n\nWhat are your columns?\n\nglimpse(gwasResults)\n\nRows: 16,470\nColumns: 4\n$ SNP &lt;chr&gt; \"rs1\", \"rs2\", \"rs3\", \"rs4\", \"rs5\", \"rs6\", \"rs7\", \"rs8\", \"rs9\", \"rs‚Ä¶\n$ CHR &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ BP  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,‚Ä¶\n$ P   &lt;dbl&gt; 0.91480604, 0.93707541, 0.28613953, 0.83044763, 0.64174552, 0.5190‚Ä¶\n\n\n\n\nHow many markers are there?\n\n# dimensions of dataframe\ndim(gwasResults)\n\n[1] 16470     4\n\n# how many unique SNP\nlength(unique(gwasResults$SNP))\n\n[1] 16470\n\n\n\n\nHow are the markers distributed across the chromosomes?\nThe function summarize() from dplyr is sometimes masked by another function. If you have find you are getting an error when you call summarize() that says something like ‚Äúargument‚Äùby‚Äù is missing, with no default‚Äù then specify explicitly that you want to use dplyr::summarize(). You could also use the British spelling summarise() and that also works. Here‚Äôs a stack overflow post where you can learn more.\n\ngwasResults %&gt;%\n  group_by(CHR) %&gt;%\n  dplyr::summarize(min_bp = min(BP),\n                   max_bp = max(BP),\n                   number_of_markers = length(SNP))\n\n# A tibble: 22 √ó 4\n     CHR min_bp max_bp number_of_markers\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt;             &lt;int&gt;\n 1     1      1   1500              1500\n 2     2      1   1191              1191\n 3     3      1   1040              1040\n 4     4      1    945               945\n 5     5      1    877               877\n 6     6      1    825               825\n 7     7      1    784               784\n 8     8      1    750               750\n 9     9      1    721               721\n10    10      1    696               696\n# ‚Ñπ 12 more rows"
  },
  {
    "objectID": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html#make-a-manhattan-plot.",
    "href": "modules/module4/11_manhattan/11_manhattan_recitation_solutions.html#make-a-manhattan-plot.",
    "title": "Manhattan Plots Recitation Solutions",
    "section": "Make a Manhattan plot.",
    "text": "Make a Manhattan plot.\nColor by chromosome, make sure the x-axis breaks are appropriate, be sure your y-axis is -log10 pvalue. Label the top 3 most significant points with their SNP number.\nCreate new column called NegLog10P which is the negative log10 pvalue.\n\ngwasResults_neglog10 &lt;- gwasResults %&gt;%\n  mutate(NegLog10P = -log10(P))\n\nCreate a new variable called SNP_number so we can plot this on the x-axis.\n\ngwasResults_neglog10 &lt;- gwasResults_neglog10 %&gt;%\n  mutate(SNP_number = parse_number(SNP))\n\nrange(gwasResults_neglog10$SNP_number)\n\n[1]     1 16470\n\n\nSet axis for plotting.\n\n(set_axis &lt;- gwasResults_neglog10 %&gt;%\n  group_by(CHR) %&gt;%\n  summarise(min = min(SNP_number),\n            max = max(SNP_number),\n            center = (max - min)/2))\n\n# A tibble: 22 √ó 4\n     CHR   min   max center\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1  1500   750.\n 2     2  1501  2691   595 \n 3     3  2692  3731   520.\n 4     4  3732  4676   472 \n 5     5  4677  5553   438 \n 6     6  5554  6378   412 \n 7     7  6379  7162   392.\n 8     8  7163  7912   374.\n 9     9  7913  8633   360 \n10    10  8634  9329   348.\n# ‚Ñπ 12 more rows\n\n\nSet what the Bonferroni -log10 pvalue needs to be for significance.\n\nbonferroni_pval &lt;- -log10(0.05/nrow(gwasResults_neglog10))\n\nIndicate which are the significant points in the plot.\n\nsignificant &lt;- gwasResults_neglog10 %&gt;%\n  filter(NegLog10P &gt; bonferroni_pval) %&gt;%\n  arrange(-NegLog10P) %&gt;%\n  slice(1:3) # take the first 3 which works bc we arranged descending\n\nPlot\n\ngwasResults_neglog10 %&gt;%\n  ggplot(aes(x = SNP_number, y = NegLog10P, color = as.factor(CHR))) +\n  geom_point() +\n  geom_label_repel(data = significant,\n                  aes(x = SNP_number, y = NegLog10P, label = SNP)) +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$CHR) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  theme_classic() +\n  theme(legend.position = \"none\") + # legend not really necessary\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"I actually don't know what trait this GWAS is for\",\n       subtitle = \"Points above the dotted line are significant after a Bonferroni multiple testing correction\")"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#introduction",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#introduction",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#run-a-pca",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#run-a-pca",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "1. Run a PCA",
    "text": "1. Run a PCA\nLet‚Äôs take a look at this new dataset\n\nknitr::kable(head(pizza))\n\n\n\n\nbrand\nid\nmois\nprot\nfat\nash\nsodium\ncarb\ncal\n\n\n\n\nA\n14069\n27.82\n21.43\n44.87\n5.11\n1.77\n0.77\n4.93\n\n\nA\n14053\n28.49\n21.26\n43.89\n5.34\n1.79\n1.02\n4.84\n\n\nA\n14025\n28.35\n19.99\n45.78\n5.08\n1.63\n0.80\n4.95\n\n\nA\n14016\n30.55\n20.15\n43.13\n4.79\n1.61\n1.38\n4.74\n\n\nA\n14005\n30.49\n21.28\n41.65\n4.82\n1.64\n1.76\n4.67\n\n\nA\n14075\n31.14\n20.23\n42.31\n4.92\n1.65\n1.40\n4.67\n\n\n\n\n\n\npizza_pca &lt;- prcomp(pizza[,-c(1:2)],\n                    center = TRUE,\n                    scale = TRUE)\n\nWe can also look at the output of our PCA in a different way using the function summary().\n\nsummary(pizza_pca) \n\nImportance of components:\n                         PC1    PC2     PC3    PC4     PC5     PC6      PC7\nStandard deviation     2.042 1.5134 0.64387 0.3085 0.16636 0.01837 0.003085\nProportion of Variance 0.596 0.3272 0.05922 0.0136 0.00395 0.00005 0.000000\nCumulative Proportion  0.596 0.9232 0.98240 0.9960 0.99995 1.00000 1.000000\n\n\nWe can convert this summary into something later usable by extraction the element importance from summary(alkaloids_pca) and converting it to a dataframe.\n\nimportance &lt;- summary(pizza_pca)$importance %&gt;%\n  as.data.frame()\n\nknitr::kable(head(importance))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\n\n\n\n\nStandard deviation\n2.042494\n1.513426\n0.6438652\n0.3085032\n0.1663641\n0.0183741\n0.0030853\n\n\nProportion of Variance\n0.595970\n0.327210\n0.0592200\n0.0136000\n0.0039500\n0.0000500\n0.0000000\n\n\nCumulative Proportion\n0.595970\n0.923180\n0.9824000\n0.9960000\n0.9999500\n1.0000000\n1.0000000\n\n\n\n\n\nBy looking at the summary we can see, for example, that the first two PCs explain 92.32% of variance."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "2. Make a scree plot of the percent variance explained by each component",
    "text": "2. Make a scree plot of the percent variance explained by each component\n\nUsing fviz_eig()\nWe can do this quickly using fviz_eig().\n\nfviz_eig(pizza_pca)\n\n\n\n\n\n\nManually\nIf you wanted to make a scree plot manually, you could by plotting using a wrangled version of the importance dataframe we made earlier.\n\n# pivot longer\nimportance_tidy &lt;- importance %&gt;%\n  rownames_to_column(var = \"measure\") %&gt;%\n  pivot_longer(cols = PC1:PC7,\n               names_to = \"PC\",\n               values_to = \"value\")\n\n# plot\nimportance_tidy %&gt;%\n  filter(measure == \"Proportion of Variance\") %&gt;%\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col(alpha = 0.1, color = \"black\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  labs(x = \"Principal component\",\n       y = \"Percent variance explained\",\n       title = \"Pizza scree plot\")"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "3. Make a scores plot of samples, coloring each sample by its brand",
    "text": "3. Make a scores plot of samples, coloring each sample by its brand\n\nUsing fviz_pca_ind()\nWe can also look at a scores plot using fviz_pca_ind() where ind means individuals. Here, each point is a sample.\n\nfviz_pca_ind(pizza_pca)\n\n\n\n\n\nManually\nWe want to plot the scores, which are in provided in pizza_pca$x.\nWe can convert the list into a dataframe of scores values by using as.data.frame(). Then we can bind back our relevant metadata so they‚Äôre all together. Note, to use bind_cols() both datasets need to be in the same order. In this case they are so we are good.\n\n# create a df of alkaloids_pca$x\nscores_raw &lt;- as.data.frame(pizza_pca$x)\n\n# bind meta-data\nscores &lt;- bind_cols(pizza[,1], # first columns\n                    scores_raw)\n\n\n# create objects indicating percent variance explained by PC1 and PC2\nPC1_percent &lt;- round((importance[2,1])*100, # index 2nd row, 1st column, times 100\n                     1) # round to 1 decimal\nPC2_percent &lt;- round((importance[2,2])*100, 1) \n\n# plot\n(scores_plot &lt;- scores %&gt;%\n  ggplot(aes(x = PC1, y = PC2, fill = brand)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Scores Plot of Pizza Proximate Analysis Across 10 Brands\",\n       fill = \"Brand\"))"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#make-a-loadings-plot-of-samples",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#make-a-loadings-plot-of-samples",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "4. Make a loadings plot of samples",
    "text": "4. Make a loadings plot of samples\n\nUsing fviz_pca_var()\nWe can also look at a loadings plot using fviz_pca_var() where var means variables. Here, each point is a variable.\n\nfviz_pca_var(pizza_pca)\n\n\n\n\n\n\nManually\nWe can also make a more customized loadings plot manually using ggplot and using the dataframe alkaloids_pca$rotation.\n\n# grab raw loadings, without any metadata\nloadings_raw &lt;- as.data.frame(pizza_pca$rotation)\n\n# convert rownames to column\nloadings &lt;- loadings_raw %&gt;%\n  rownames_to_column(var = \"analysis_type\")\n\n# create vector of labels as we want them to appear\nanalysis_type_labels &lt;- c(\"Moisture\",\n                          \"Protein\",\n                          \"Fat\",\n                          \"Ash\",\n                          \"Sodium\",\n                          \"Carbo-\\nhydrates\",\n                          \"Calories\")\n\nWe can then plot with ggplot like normal.\n\nlibrary(emojifont)\npizza_emoji &lt;- emoji(search_emoji('pizza'))\n\n(loadings_plot &lt;- loadings %&gt;%\n  ggplot(aes(x = PC1, y = PC2, label = analysis_type_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_label_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot for Pizza\"))"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together.",
    "text": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together.\n\n# setting the range of the plot\n(scores_plot_ranged &lt;- scores_plot +\n  coord_cartesian(xlim = c(-3, 5.5), ylim = c(-3, 3.5)))\n\n\n\n# what is the ratio of the space on each side of the axis for the scores plot?\n(x_ratio &lt;- 3/(3+5.5))\n\n[1] 0.3529412\n\n(y_ratio &lt;- 3/(3+3.5))\n\n[1] 0.4615385\n\n# check the ending range for the loadings plot\n# 0.7 units looks good for x\nx_scalar &lt;- 0.7\n# 0.6 units looks good for y\ny_scalar &lt;- 0.6\n\n# what should the low range value be so that both plots are equally scaled?\n\n# making the loadings plot match this range\n(loadings_plot_ranged &lt;- loadings_plot +\n  coord_cartesian(xlim = c(-0.3818, x_scalar), ylim = c(-0.5140, y_scalar)))\n\n\n\n\nPlot\n\nscores_plot_ranged + loadings_plot_ranged"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation_solutions.html#biplot",
    "href": "modules/module4/10_pca/10_pca_recitation_solutions.html#biplot",
    "title": "Principal Components Analysis Recitation Solutions üçï",
    "section": "Biplot",
    "text": "Biplot\n\nUsing fviz_pca().\nYou can make a biplot quickly with fviz_pca(). Note, fviz_pca_biplot() and fviz_pca() are the same.\n\nfviz_pca(pizza_pca)\n\n\n\n\nInstead of making this plot manually, let‚Äôs go through how to alter the existing plot made with fviz_pca(). We can do this because factoextra creates ggplot objects. To start off, we need to be using a dataframe that includes our metadata.\n\n# save as a new df\npizza_pca_labelled &lt;- pizza_pca\n\n# assign alkaloid_labels to rownames\nrownames(pizza_pca_labelled$rotation) &lt;- analysis_type_labels\n\n# plot\nfviz_pca(pizza_pca_labelled, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = c(\"text\", \"point\", \"arrow\"),\n         col.var = \"black\") +\n  geom_point(aes(fill = pizza$brand), shape = 21) +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Pizza Biplot\",\n       fill = \"Brand\")\n\n\n\n\n\n\nManually\nOr we could do this manually. First we need to scale our data so that the scores and loadings are on the same scale.\nI can write a quick function to allow normalization.\n\nnormalize &lt;- function(x) return((x - min(x))/(max(x) - min(x)))\n\nThen I can nornalize the scores using the scale function, since the loadings are already nornalized.\n\nscores_normalized &lt;- scores %&gt;%\n  mutate(PC1_norm = scale(normalize(PC1), center = TRUE, scale = FALSE)) %&gt;%\n  mutate(PC2_norm = scale(normalize(PC2), center = TRUE, scale = FALSE)) %&gt;%\n  select(brand, PC1_norm, PC2_norm, everything()) # reorder \n\nHow did it go? PC1_norm and PC2_norm should all now be between -1 and 1\n\nhead(scores_normalized) # looks good\n\n# A tibble: 6 √ó 10\n  brand PC1_norm[,1] PC2_norm[,1]   PC1   PC2      PC3     PC4     PC5      PC6\n  &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 A            0.653        0.475  5.00  2.67 0.0393   -0.144   0.284  -0.00233\n2 A            0.654        0.448  5.02  2.53 0.0969   -0.353   0.215   0.00295\n3 A            0.626        0.474  4.80  2.67 0.0753    0.108  -0.0350  0.00541\n4 A            0.582        0.405  4.46  2.28 0.120     0.0539  0.174   0.00562\n5 A            0.583        0.383  4.46  2.16 0.000736 -0.117   0.313   0.00169\n6 A            0.587        0.384  4.50  2.16 0.175    -0.115   0.200   0.00518\n# ‚Ñπ 1 more variable: PC7 &lt;dbl&gt;\n\n\nNow we can plot together the scores and loadings in one plot.\n\nscores_normalized %&gt;%\n  ggplot() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(aes(x = PC1_norm, y = PC2_norm, fill = brand), shape = 21) +\n  geom_point(data = loadings, aes(x = PC1, y = PC2)) +\n  geom_text_repel(data = loadings, \n                  aes(x = PC1, y = PC2, label = analysis_type_labels)) +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot of Pizza Proximate Analysis Across 10 Brands\",\n       fill = \"Brand\",\n       caption = \"Proximate analyses in black are loadings\")"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html",
    "title": "Adding Statistics Recitation Solutions",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\n\nlibrary(tidyverse) # for everything\nlibrary(NHANES) # for data\nlibrary(rstatix) # for pipe friendly statistics functions\nlibrary(ggpubr) # for easy annotating of stats\nlibrary(glue) # for easy pasting\nlibrary(rcompanion) # for creating the comparison table"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#introduction",
    "title": "Adding Statistics Recitation Solutions",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\n\nlibrary(tidyverse) # for everything\nlibrary(NHANES) # for data\nlibrary(rstatix) # for pipe friendly statistics functions\nlibrary(ggpubr) # for easy annotating of stats\nlibrary(glue) # for easy pasting\nlibrary(rcompanion) # for creating the comparison table"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "title": "Adding Statistics Recitation Solutions",
    "section": "1. Is total cholesterol (TotChol) different by age (AgeDecade)?",
    "text": "1. Is total cholesterol (TotChol) different by age (AgeDecade)?\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint - you want to test your assumptions to see what tests to do. You might need to use different posthoc comparison methods than we did in class.\n\n\n\n\n\n\n\n\n\nNeed another hint? (Click to expand)\n\n\n\n\n\nAnother hint - the function rcompanion::cldList() will convert the resulting comparison table from a posthoc Dunn test to create a column with the letters indicating which groups are significantly different from each other.\n\n\n\n\nBase plot\nPlot to get an overview.\n\n(totchol_age_baseplot &lt;- NHANES %&gt;%\n  drop_na(AgeDecade, TotChol) %&gt;%\n  ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Age, by Decade\",\n       y = \"Total Cholesterol (mmol/L)\",\n       title = \"Differences in total cholesterol by age in NHANES 2009/2010, and 2011/2012\"))\n\n\n\n\nUse stat_compare_means()\n\nNHANES %&gt;%\n  drop_na(AgeDecade, TotChol) %&gt;%\n  ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) +\n  geom_boxplot() +\n  stat_compare_means() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Age, by Decade\",\n       y = \"Total Cholesterol (mmol/L)\",\n       title = \"Differences in total cholesterol by age in NHANES 2009/2010, and 2011/2012\")\n\n\n\n\n\n\nTesting assumptions\n\nNormality\n\n# testing normality by group\nNHANES %&gt;%\n  drop_na(AgeDecade, TotChol) %&gt;% # remove NAs\n  group_by(AgeDecade) %&gt;%\n  shapiro_test(TotChol) # test for normality\n\n# A tibble: 8 √ó 4\n  AgeDecade variable statistic        p\n  &lt;fct&gt;     &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 \" 0-9\"    TotChol      0.986 5.23e- 4\n2 \" 10-19\"  TotChol      0.971 5.15e-15\n3 \" 20-29\"  TotChol      0.988 1.85e- 8\n4 \" 30-39\"  TotChol      0.963 1.58e-17\n5 \" 40-49\"  TotChol      0.960 7.88e-19\n6 \" 50-59\"  TotChol      0.987 6.76e- 9\n7 \" 60-69\"  TotChol      0.986 2.11e- 7\n8 \" 70+\"    TotChol      0.983 6.20e- 6\n\n\nNot normal.\n\n\nConstant variance\n\nNHANES %&gt;%\n  drop_na(AgeDecade, TotChol) %&gt;% # remove NAs\n  levene_test(TotChol ~ AgeDecade) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic        p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1     7  8158      48.6 4.39e-68\n\n\nNon constant variance. Non-parametric it is.\n\n\n\nLog transformed tests\n\nNHANES_log &lt;- NHANES %&gt;%\n  mutate(TotChol_log2 = log2(TotChol))\n\n\nNormality\n\n# testing normality by group\nNHANES_log %&gt;%\n  drop_na(AgeDecade, TotChol_log2) %&gt;% # remove NAs\n  group_by(AgeDecade) %&gt;%\n  shapiro_test(TotChol_log2) # test for normality\n\n# A tibble: 8 √ó 4\n  AgeDecade variable     statistic            p\n  &lt;fct&gt;     &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n1 \" 0-9\"    TotChol_log2     0.995 0.219       \n2 \" 10-19\"  TotChol_log2     0.991 0.00000180  \n3 \" 20-29\"  TotChol_log2     0.989 0.0000000306\n4 \" 30-39\"  TotChol_log2     0.995 0.000536    \n5 \" 40-49\"  TotChol_log2     0.993 0.00000977  \n6 \" 50-59\"  TotChol_log2     0.993 0.0000100   \n7 \" 60-69\"  TotChol_log2     0.989 0.00000472  \n8 \" 70+\"    TotChol_log2     0.995 0.0751      \n\n\nStill pretty not normal via Shapiro Test. Let‚Äôs look at the log2 transformed total choletserol distributions across the different age groups.\n\nNHANES_log %&gt;%\n  drop_na(TotChol_log2, AgeDecade) %&gt;%\n  ggplot(aes(x = TotChol_log2)) +\n  geom_histogram() +\n  facet_wrap(vars(AgeDecade))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThese actually look reasonably normal to me.\n\n\nConstant variance\n\nNHANES_log %&gt;%\n  drop_na(AgeDecade, TotChol_log2) %&gt;% # remove NAs\n  levene_test(TotChol_log2 ~ AgeDecade) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic        p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1     7  8158      16.8 4.03e-22\n\n\nStill not constant variance.\n\n\n\nKruskal Wallis test\n\n(kruskal_chol &lt;- NHANES %&gt;%\n  drop_na(AgeDecade, TotChol) %&gt;% # remove NAs\n  kruskal_test(TotChol ~ AgeDecade))\n\n# A tibble: 1 √ó 6\n  .y.         n statistic    df     p method        \n* &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 TotChol  8166     1672.     7     0 Kruskal-Wallis\n\n\nOk significant difference exists. Where is it?\n\n\nPost-hoc analysis\nRun Dunn test\n\n(kruskal_chol_posthoc &lt;- NHANES %&gt;%\n  drop_na(AgeDecade, TotChol) %&gt;% # remove NAs\n  dunn_test(TotChol ~ AgeDecade,\n            p.adjust.method = \"BH\")) # Benjamini Hochberg multiple testing correction\n\n# A tibble: 28 √ó 9\n   .y.     group1  group2    n1    n2 statistic         p     p.adj p.adj.signif\n * &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       \n 1 TotChol \" 0-9\"  \" 10-‚Ä¶   415  1214    0.0500 9.60e-  1 9.60e-  1 ns          \n 2 TotChol \" 0-9\"  \" 20-‚Ä¶   415  1257   10.6    3.66e- 26 6.83e- 26 ****        \n 3 TotChol \" 0-9\"  \" 30-‚Ä¶   415  1273   15.9    1.02e- 56 2.86e- 56 ****        \n 4 TotChol \" 0-9\"  \" 40-‚Ä¶   415  1354   21.3    2.60e-100 1.22e- 99 ****        \n 5 TotChol \" 0-9\"  \" 50-‚Ä¶   415  1234   22.2    8.53e-109 4.78e-108 ****        \n 6 TotChol \" 0-9\"  \" 60-‚Ä¶   415   873   18.8    1.16e- 78 4.64e- 78 ****        \n 7 TotChol \" 0-9\"  \" 70+\"   415   546   14.9    4.59e- 50 1.07e- 49 ****        \n 8 TotChol \" 10-1‚Ä¶ \" 20-‚Ä¶  1214  1257   14.8    1.16e- 49 2.50e- 49 ****        \n 9 TotChol \" 10-1‚Ä¶ \" 30-‚Ä¶  1214  1273   22.3    4.50e-110 3.15e-109 ****        \n10 TotChol \" 10-1‚Ä¶ \" 40-‚Ä¶  1214  1354   30.1    3.66e-199 5.13e-198 ****        \n# ‚Ñπ 18 more rows\n\n\nUse rcompanion::cldList() to create the groups for us. Reading the documentation about cldList() helped me learn that:\n\nthere needs to be a formula that compares the p-values (here, p.adj) to a comparison column (here, one I created called comparison)\nthere needs to be a comparison column that is in the form similar to ‚ÄúTreat.A - Treat.B = 0‚Äù where =, 0 are removed by default. Since we have hyphens in our group names, I removed them since this column only allows one hyphen between the groups to be compared\nset a threshold for what p-value is considered significant\n\nTo do this, first:\n\nI removed the hyphen from group1 and group2 in new variables called group1_rep and group2_rep\nThen, I made a new column called comparison that combined the values from group1_rep and group2_rep\n\n\n# combine group1 and group2 to make one column called comparison\n# then replace hyphens with something else because cldList can only have one hyphen\nkruskal_chol_posthoc_1 &lt;- kruskal_chol_posthoc %&gt;%\n  mutate(group1_rep = str_replace_all(group1, pattern = \"-\", replacement = \"to\"),\n         group2_rep = str_replace_all(group2, pattern = \"-\", replacement = \"to\")) %&gt;%\n  mutate(comparison = glue(\"{group1_rep} -{group2_rep}\"))\n\n# run cldList()\n(group_cldList &lt;- cldList(p.adj ~ comparison,\n        data = kruskal_chol_posthoc_1,\n        threshold = 0.05))\n\n  Group Letter MonoLetter\n1   to9      a      a    \n2 1to19      a      a    \n3 2to29      b       b   \n4 3to39      c        c  \n5 4to49     de         de\n6 5to59      d         d \n7 6to69      e          e\n8    7+      c        c  \n\n\nCreate groups from kruskal_chol_posthoc results manually.\n\nunique(NHANES$AgeDecade)\n\n[1]  30-39  0-9    40-49  60-69  50-59  10-19  20-29  70+   &lt;NA&gt;  \nLevels:  0-9  10-19  20-29  30-39  40-49  50-59  60-69  70+\n\ngroup_manual &lt;- data.frame(group = c(\"a\", \"a\", \"b\", \"c\", \"de\", \"d\", \"e\", \"c\"))\n\nMake a dataframe that has the maximum total cholesterol for each age so that we know where to place the numbers on the plot.\n\n(max_chol &lt;- NHANES %&gt;%\n  drop_na(TotChol, AgeDecade) %&gt;%\n  group_by(AgeDecade) %&gt;%\n  summarize(max_tot_chol = max(TotChol)))\n\n# A tibble: 8 √ó 2\n  AgeDecade max_tot_chol\n  &lt;fct&gt;            &lt;dbl&gt;\n1 \" 0-9\"            6.34\n2 \" 10-19\"          7.76\n3 \" 20-29\"          8.2 \n4 \" 30-39\"          9.93\n5 \" 40-49\"         13.6 \n6 \" 50-59\"         12.3 \n7 \" 60-69\"         10.3 \n8 \" 70+\"            9.05\n\n\nBind the groups to the maximum cholesterol df.\n\ndunn_for_plotting &lt;- bind_cols(max_chol, group_cldList$Letter) %&gt;%\n  rename(Letter = `...3`)\n\nNew names:\n‚Ä¢ `` -&gt; `...3`\n\n\n\n\nPlot\n\n# using geom_text()\ntotchol_age_baseplot +\n  geom_text(data = dunn_for_plotting,\n            aes(x = AgeDecade, \n                y = max_tot_chol + 1,\n                label = Letter)) +\n  labs(caption = \"Groups with different letters are significant different using the Kruskal Wallis test, \\nand the Dunn test for pairwise comparisons at p &lt; 0.05\")\n\n\n\n# using annotate()\ntotchol_age_baseplot +\n  annotate(geom = \"text\",\n           x = seq(1:8),\n           y = dunn_for_plotting$max_tot_chol + 1,\n           label = dunn_for_plotting$Letter) +\n  labs(caption = \"Groups with different letters are significant different using the Kruskal Wallis test, \\nand the Dunn test for pairwise comparisons at p &lt; 0.05\")"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html",
    "href": "modules/module3/09_add-stats/09_add-stats.html",
    "title": "Annotating Statistics onto Plots",
    "section": "",
    "text": "Figure from XKCD"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats.html#introduction",
    "title": "Annotating Statistics onto Plots",
    "section": "Introduction",
    "text": "Introduction\nNow that we‚Äôve spent some time going through how to make plots, today we will focus on how to annotate statistics that you‚Äôve calculated to show statistical differences, embedded within your plot. I will go over a few different ways to do this.\nThe purpose of today‚Äôs session is more to give you practical experience with running and retrieving statistical analysis output, than teaching about the assumptions and background of the test itself. If you are looking for a good statistics class, I would recommend Dr.¬†Kristin Mercer‚Äôs HCS 8887 Experimental Design.\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\nWe are going to use data that was collection about body characteristics of penguins on Palmer Station in Antarctica. This data is in a dataframe called penguins in the package palmerpenguins which you can download from CRAN.\n\n\n\n\n\nFrom Palmer Penguins\n\n\n\n\n\n# install.packages(palmerpenguins)\nlibrary(tidyverse)\nlibrary(palmerpenguins) # for penguins data\nlibrary(rstatix) # for pipeable stats testing\nlibrary(agricolae) # for posthoc tests \nlibrary(ggpubr) # extension for adding stats to plots\nlibrary(glue) # for easy pasting\n\n\nknitr::kable(head(penguins)) # kable to make a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-t-tests-or-similar",
    "href": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-t-tests-or-similar",
    "title": "Annotating Statistics onto Plots",
    "section": "2 group comparisons (t-tests or similar)",
    "text": "2 group comparisons (t-tests or similar)\n\nOur question: Is there a significant difference in the body_weight_g of male and female penguins?\n\nBefore we run the statistics, let‚Äôs make a plot to see what this data looks like.\n\n# what are the values for sex?\nunique(penguins$sex)\n\n[1] male   female &lt;NA&gt;  \nLevels: female male\n\n# plot\n(penguins_by_sex &lt;- penguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  ggplot(aes(x = sex, y = body_mass_g, color = sex)) + \n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(height = 0, width = 0.3) +\n  scale_x_discrete(labels = c(\"Female\", \"Male\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sex\",\n       y = \"Body Mass (g)\",\n       title = \"Body mass of penguins by sex\",\n       subtitle = \"Collected from Palmer Station, Antarctica\",\n       caption = \"Data accessed from the R package palmerpenguins\"))\n\n\n\n\nIt looks like there is a difference here. Before adding the statistics to our plot, let‚Äôs:\n\ntest that our data is suitable for running the text we want\nrun the statistical test separately from the plot\n\n\nTesting assumptions\nBriefly, in order to use parametric procedures (like a t-test), we need to be sure our data meets the assumptions for 1) normality and 2) constant variance. This is just one way to do these tests, there are others that I am not going to go over.\n\n\n\n\n\nIllustration by Allison Horst\n\n\n\n\n\nNormality\nWe will test normality by the Shapiro-Wilk test using the function rstatix::shapiro_test(). This function is a pipe-friendly wrapper for the function shapiro.test(), which just means you can use it with pipes.\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  group_by(sex) %&gt;% # test by sex\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 2 √ó 4\n  sex    variable    statistic            p\n  &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 female body_mass_g     0.919 0.0000000616\n2 male   body_mass_g     0.925 0.000000123 \n\n\nThis data is not normal, which means we need to use non-parametric tests. Since we are not meeting the assumption for nornality, really you don‚Äôt need to test for constant variance, but I‚Äôll show you how to do it anyway.\n\n\nConstant variance\nWe can test for equal variance using Levene‚Äôs test, levene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  levene_test(body_mass_g ~ sex) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic      p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1   331      6.06 0.0143\n\n\nNo constant variance. Double Non-parametric.\nCan we visualize normality another way?\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  ggplot(aes(x = body_mass_g, y = sex, fill = sex)) +\n  ggridges::geom_density_ridges(alpha = 0.7) +\n  scale_y_discrete(labels = c(\"Female\", \"Male\")) +\n  theme_classic() +  \n  theme(legend.position = \"none\") +\n  labs(x = \"Body Mass (g)\",\n       y = \"Sex\",\n       title = \"Distribution of body weights for male and female penguins\")\n\nPicking joint bandwidth of 235\n\n\n\n\n\nSome of these distribution are bimodal (i.e., not normal). This is likely because we have 3 different species of penguins here. You can see below that actually each species looks reasonably normal.\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  ggplot(aes(x = body_mass_g, fill = sex)) +\n  geom_histogram() +\n  facet_grid(cols = vars(species), rows = vars(sex)) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Body Mass (g)\",\n       y = \"Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNon-parametric t-test\nThis means if we want to test for different means, we can use the Wilcoxon rank sun test, or Mann Whitney test. If your data was normal, you could just change wilcox_test() to t_test() and the rest would be the same.\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  wilcox_test(body_mass_g ~ sex,\n              paired = FALSE)\n\n# A tibble: 1 √ó 7\n  .y.         group1 group2    n1    n2 statistic        p\n* &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 body_mass_g female male     165   168     6874. 1.81e-15\n\n\nThis is not surprising, that there is a significant difference in body weight between male and female penguins. We can see this clearly in our plot.\nHow can we add the stats to our plot?\n\n\nPlot\n\nUsing stat_compare_means()\nThe function stat_compare_means() allows mean comparison p-values to be easily added to a ggplot.\nNote, the function should look at your data and test for normality and pick the statistical test accordingly. You can see that is working in the chunk below, but I would recommend that you always do your own statistical test and make sure you plot accordingly.\n\npenguins_by_sex +\n  stat_compare_means()\n\n\n\npenguins_by_sex +\n  stat_compare_means(method = \"wilcox.test\") \n\n\n\n\n\n\nManually with geom_text() or annotate()\nIn general, plotting using geom_text() is easier, and follows classic geom_() syntax (e.g., includes aes()) but for some reason these don‚Äôt pass as vectorized objects so sometimes it yields low quality images. Using annotate() passes as vectors and thus tends to be higher quality. You can decide which you want to use depending on your purpose.\nIf I‚Äôm being honest, the most common way that I would add statistics to a plot if I was trying to do just a few simple plots at once, would be with annotate() . I like to use annotate() over geom_text() or geom_label() because it is vectorized and don‚Äôt become low quality down the road.\nWith geom_text()\n\npenguins_by_sex +\n  geom_text(aes(x = 2, y = 6500, label = \"*\"), # x, y, and label within aes()\n            color = \"black\", size = 6)\n\n\n\n\nWith annotate()\n\npenguins_by_sex +\n  annotate(geom = \"text\", # note no aes()\n           x = 2, y = 6500, \n           label = \"*\", \n           size = 6)\n\n\n\n\nYou can also add multiple annotation layers. I‚Äôm introducing a new function here, glue() which is amazing for easy syntax pasting of strings with data.\nThe syntax for glue() is like this:\n\nx &lt;- 2 + 3\n\nglue(\"2 + 3 = {x}\")\n\n2 + 3 = 5\n\n\n\n# we did this already, just assigning to object\nby_sex_pval &lt;- penguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  wilcox_test(body_mass_g ~ sex,\n              paired = FALSE)\n\n# plot\npenguins_by_sex +\n  ylim(2500, 7500) + # adjust the y-axis so there's space for the label\n  annotate(geom = \"text\", x = 2, y = 6500, label = \"*\", size = 6) +\n  annotate(geom = \"text\", x = 2, y = 7000,\n           label = glue(\"Wilcoxon signed rank test \\np-value = {by_sex_pval$p}\"))"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-anova-or-similar",
    "href": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-anova-or-similar",
    "title": "Annotating Statistics onto Plots",
    "section": ">2 group comparisons (ANOVA or similar)",
    "text": "&gt;2 group comparisons (ANOVA or similar)\nWhen we are comparing means between more than 2 samples, we will have to first run a statistical test to see if there are any significant differences among our groups, and then if there are, run a post-hoc test. Before we do that, let‚Äôs plot.\nAre there significant differences in body mass\n\n(penguins_f_massbyspecies &lt;- penguins %&gt;%\n  drop_na(body_mass_g, species, sex) %&gt;%\n  filter(sex == \"female\") %&gt;%\n  ggplot(aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(outlier.shape = NA,\n              draw_quantiles = 0.5) + # add the median by drawing 50% quantile\n  ggdist::geom_dots(side = \"both\", color = \"black\", alpha = 0.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Penguin Species\",\n       y = \"Body Mass (g)\",\n       title = \"Body mass of female penguins by species\",\n       subtitle = \"Collected from Palmer Station, Antarctica\",\n       caption = \"Data accessed from the R package palmerpenguins\"))\n\nWarning in geom_violin(outlier.shape = NA, draw_quantiles = 0.5): Ignoring\nunknown parameters: `outlier.shape`\n\n\n\n\n\n\nTesting assumptions\n\nNormality\n\n# testing normality by group\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  filter(sex == \"female\") %&gt;%\n  group_by(species) %&gt;% # test by species\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 3 √ó 4\n  species   variable    statistic     p\n  &lt;fct&gt;     &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    body_mass_g     0.977 0.199\n2 Chinstrap body_mass_g     0.963 0.306\n3 Gentoo    body_mass_g     0.981 0.511\n\n# testing normality across all data\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  filter(sex == \"female\") %&gt;%\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 1 √ó 3\n  variable    statistic            p\n  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 body_mass_g     0.919 0.0000000616\n\n\nOk looks like we have normally distributed data among the different species of female penguins.\n\n\nConstant variance\nlevene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex, species) %&gt;% # remove NAs\n  filter(sex == \"female\") %&gt;%\n  levene_test(body_mass_g ~ species) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2   162    0.0357 0.965\n\n\nWe have constant variance. Along with normally distributed data, this means that we can use parametric tests. In the case of &gt;2 samples, that would be ANOVA.\n\n\n\nANOVA\nThe most commonly used function to run ANOVA in R is called aov() which is a part of the stats package that is pre-loaded with base R. So no new packages need to be installed here.\nIf we want to learn more about the function aov() we can do so using the code below. The help documentation will show up in the bottom right quadrant of your RStudio.\n\n?aov()\n\nWe can run an ANOVA by indicating our model, and here I‚Äôm also selecting to drop the NAs for our variables of interest, and filtering within the data = argument.\n\naov_female_massbyspecies &lt;- \n  aov(data = penguins %&gt;% \n             filter(sex == \"female\") %&gt;%\n             drop_na(body_mass_g, species),\n      body_mass_g ~ species)\n\nNow lets look at the aov object.\n\nsummary(aov_female_massbyspecies)\n\n             Df   Sum Sq  Mean Sq F value Pr(&gt;F)    \nspecies       2 60350016 30175008   393.2 &lt;2e-16 ***\nResiduals   162 12430757    76733                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can take the output of our ANOVA and use the function tidy() within the broom package to turn our output into a tidy table. Here, the notation broom::tidy() means I want to use the function tidy() that is a part of the broom package. This works even though I haven‚Äôt called library(broom) at the beginning of my script.\n\ntidy_anova &lt;- broom::tidy(aov_female_massbyspecies)\n\nknitr::kable(tidy_anova)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nspecies\n2\n60350016\n30175008.01\n393.2465\n0\n\n\nResiduals\n162\n12430757\n76733.07\nNA\nNA\n\n\n\n\n\nSee how this is different from just saving the ANOVA summary? Open both anova_summary and tidy_anova and note the differences.\n\nanova_summary &lt;- summary(aov_female_massbyspecies)\n\n\n\nPosthoc group analysis\nNow that we see we have a significant difference somewhere in the body mass of the 3 species of female penguins, we can do a posthoc test to see which groups are significantly different. We will do our post-hoc analysis using Tukey‚Äôs Honestly Significant Difference test and the function HSD.test() which is a part of the useful package agricolae.\n\ntukey_massbyspecies &lt;- HSD.test(aov_female_massbyspecies, \n                      trt = \"species\", \n                      console = TRUE) # prints the results to console\n\n\nStudy: aov_female_massbyspecies ~ \"species\"\n\nHSD Test for body_mass_g \n\nMean Square Error:  76733.07 \n\nspecies,  means\n\n          body_mass_g      std  r  Min  Max\nAdelie       3368.836 269.3801 73 2850 3900\nChinstrap    3527.206 285.3339 34 2700 4150\nGentoo       4679.741 281.5783 58 3950 5200\n\nAlpha: 0.05 ; DF Error: 162 \nCritical Value of Studentized Range: 3.345258 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n          body_mass_g groups\nGentoo       4679.741      a\nChinstrap    3527.206      b\nAdelie       3368.836      c\n\n\nLike we did with the aov object, you can also look at the resulting HSD.test object (here, tukey_massbyspecies) in your environment pane.\nHere, instead of using the broom package, you can convert the part of the tukey_bill_length object that contains the post-hoc groupings into a dataframe using as.data.frame().\n\ntidy_tukey &lt;- as.data.frame(tukey_massbyspecies$groups)\n\ntidy_tukey\n\n          body_mass_g groups\nGentoo       4679.741      a\nChinstrap    3527.206      b\nAdelie       3368.836      c\n\n\n\n\nPlot\n\nUsing stat_compare_means()\n\npenguins_f_massbyspecies +\n  stat_compare_means()\n\n\n\npenguins_f_massbyspecies +\n  stat_compare_means(method = \"anova\")\n\n\n\n\n\n\nManually with geom_text() or annotate()\nIn general, plotting using geom_text() is easier, and follows classic geom_() syntax (e.g., includes aes()) but for some reason these don‚Äôt pass as vectorized objects so sometimes it yields low quality images. Using annotate() passes as vectors and thus tends to be higher quality. You can decide which you want to use depending on your purpose.\nWe want to add the letters to this plot, so we can tell which groups of penguin species are significantly different.\nBefore we can do this, we will need to do some of everyone‚Äôs favorite task, wrangling. We are going to figure out what the maximum body_mass_g for each species is, so it will help us determine where to put our letter labels. Then, we can add our labels to be higher than the largest data point. We will calculate this for each group, so that the letters are always right about our boxplot.\n\nbody_mass_max &lt;- penguins %&gt;%\n  filter(sex == \"female\") %&gt;%\n  drop_na(body_mass_g, species) %&gt;%\n  group_by(species) %&gt;%\n  summarize(max_body_mass = max(body_mass_g))\n\nbody_mass_max\n\n# A tibble: 3 √ó 2\n  species   max_body_mass\n  &lt;fct&gt;             &lt;int&gt;\n1 Adelie             3900\n2 Chinstrap          4150\n3 Gentoo             5200\n\n\nLet‚Äôs add our post-hoc group info to body_mass_max, since those two dataframes are not in the same order. Instead of binding the two dataframes together, we are going to join them using one of the dplyr _join() functions, which allows you to combine dataframes based on a specific common column. The join functions work like this:\n\ninner_join(): includes all rows in x and y.\nleft_join(): includes all rows in x.\nright_join(): includes all rows in y.\nfull_join(): includes all rows in x or y.\n\nIn this case, it doesn‚Äôt matter which _join() we use because our dfs all have the exact same rows.\n\ntidier_tukey &lt;- tidy_tukey %&gt;%\n  rownames_to_column() %&gt;% # converts rownames to columns\n  rename(species = rowname) # renames the column now called rowname to species\n  \n# join\nbody_mass_for_plotting &lt;- full_join(tidier_tukey, body_mass_max,\n                               by = \"species\")\n\nLet‚Äôs plot. First using geom_text()\n\npenguins_f_massbyspecies +\n  geom_text(data = body_mass_for_plotting,\n            aes(x = species,\n                y = 175 + max_body_mass, \n                label = groups))\n\n\n\n\nNext using annotate().\n\npenguins_f_massbyspecies +\n  annotate(geom = \"text\",\n           x = c(3,2,1),\n           y = 175 + body_mass_for_plotting$max_body_mass,\n           label = body_mass_for_plotting$groups)"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#useful-resources",
    "href": "modules/module3/09_add-stats/09_add-stats.html#useful-resources",
    "title": "Annotating Statistics onto Plots",
    "section": "Useful resources",
    "text": "Useful resources\nThere have been previous Code Club sessions about adding statistics to plots:\n\nggpubr to add stats to plots by Daniel Quiroz\nt-tests in R by Mike Sovic\nRunning ANOVA in R and accesing output\nTesting ANOVA assumptions"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n--- Compiling #TidyTuesday Information for 2020-07-07 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\n\n--- Download complete ---\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(tidyverse) # for everything\nlibrary(ggridges) # for ridgeline plots\nlibrary(ggdist) # for nice dotplots"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#introduction",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n--- Compiling #TidyTuesday Information for 2020-07-07 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\n\n--- Download complete ---\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(tidyverse) # for everything\nlibrary(ggridges) # for ridgeline plots\nlibrary(ggdist) # for nice dotplots"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#total-cupping-score-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#total-cupping-score-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "Total cupping score in arabica and robusta",
    "text": "Total cupping score in arabica and robusta\nMake 3 different visualizations that shows the distribution of total cupping score across arabica and robusta beans. Make the plots so you think they look good.\n\nA histogram\nSince there are so few robusta observations, I decided to make the y-axes on different scales\n\ncoffee_ratings %&gt;%\n  ggplot(aes(x = total_cup_points, fill = species)) +\n  geom_histogram(bins = 150) +\n  geom_vline(aes(xintercept = mean(total_cup_points)), color = \"black\") +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(xlim = c(60,100)) +\n  facet_wrap(vars(species),\n             nrow = 2, # make two rows so can align histograms top to bottom\n             scales = \"free_y\",\n             strip.position = \"top\") +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       fill = \"Species\",\n       title = \"Distribution of cupping scores across 14,000 coffee samples\",\n       subtitle = \"Note the y-axes are different between plots\",\n       caption = \"Vertical line represents the median total cupping score across all samples\")\n\n\n\n\n\n\nDensity plot\nI might like this a bit better than a histogram.\n\ncoffee_ratings %&gt;%\n  ggplot(aes(x = total_cup_points, fill = species)) +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(total_cup_points)), color = \"black\") +\n  coord_cartesian(xlim = c(65,92)) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  facet_wrap(vars(species),\n             nrow = 2, # make two rows so can align histograms top to bottom\n             scales = \"free_y\") +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       fill = \"Species\",\n       title = \"Distribution of cupping scores across 14,000 coffee samples\",\n       caption = \"Vertical line represents the median total cupping score across all samples\")\n\n\n\n\n\n\nDot plot\nYou can also see here how many fewer robusta observations there are.\n\n# dot plot\ncoffee_ratings %&gt;%\n  ggplot(aes(x = total_cup_points, color = species, fill = species)) +\n  geom_dots() +\n  scale_color_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  theme_ggdist() +\n  xlim(50,100) +\n  theme(legend.position = c(.18, .99),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.box.background = element_rect(size = 0.5),\n        legend.box.margin = margin(5, 5, 5, 5)) +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       color = \"Species\",\n       fill = \"Species\")\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n\n\nWarning: Removed 1 rows containing missing values (`geom_dotsinterval()`).\n\n\n\n\n\n\n\nRidgeline plot\n\n# ridgeline plot\ncoffee_ratings %&gt;%\n  ggplot(aes(x = total_cup_points, y = species, fill = species)) +\n  stat_density_ridges(quantile_lines = TRUE,\n                      quantiles = 2,\n                      alpha = 0.5) +\n  xlim(55, 100) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  theme_ggdist() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       title = \"Distribution of cupping scores across 14,000 coffee samples\",\n       subtitle = \"The brown color theme is very coffee-esque\")\n\nPicking joint bandwidth of 0.605\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_density_ridges()`).\n\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "Individual characteristic cupping scores in arabica and robusta",
    "text": "Individual characteristic cupping scores in arabica and robusta\nMake 3 different visualizations that show the distribution of all the individual contributors to total cupping score across arabica and robusta in one plot.\nWrangling:\n\n# go from wide (each coffee attribute in a separate column)\n# to long data (1 column w/ all characteristics, 1 column w/ all ratings)\ncoffee_ratings_tidy &lt;- coffee_ratings %&gt;%\n  pivot_longer(cols = aroma:cupper_points, # columns from aroma to cupper_points\n               names_to = \"characteristic\",\n               values_to = \"rating\")\n\nPrepare to clean up facet strip text using the function labeller().\n\n# getting labels ready for plotting\n# what are the coffee characteristics again?\n(coffee_characteristics &lt;- unique(coffee_ratings_tidy$characteristic) %&gt;%\n  sort()) # sort alphabetically, arrange won't work here bc not numeric\n\n [1] \"acidity\"       \"aftertaste\"    \"aroma\"         \"balance\"      \n [5] \"body\"          \"clean_cup\"     \"cupper_points\" \"flavor\"       \n [9] \"sweetness\"     \"uniformity\"   \n\n# create a vector of the coffee characteristic names as i want them to appear on the plot\ncoffee_labels &lt;- c(\"Acidity\",\n                   \"Aftertaste\",\n                   \"Aroma\",\n                   \"Balance\",\n                   \"Body\",\n                   \"Clean Cup\",\n                   \"Cupper Points\",\n                   \"Flavor\",\n                   \"Sweetness\",\n                   \"Uniformity\")\n\n# tell coffee_labels which original label to refer to\n# these need to be in the same order (which is why i used sort())\nnames(coffee_labels) &lt;- coffee_characteristics\n\nManaging fonts:\n\n# get fonts not default available in R\nlibrary(sysfonts) # aux packagew here fonts live\nlibrary(showtext) # package that helps use non-standard fonts\n\nLoading required package: showtextdb\n\nlibrary(ragg)\n\n# add the font Atkison Hyperlegible bc i like it\nfont_add_google(\"Atkinson Hyperlegible\")\n\n# what fonts do i have to choose from?\n# remove head() to see them all\nhead(font_info_google())\n\n         family   category num_variants                    variants num_subsets\n1       ABeeZee sans-serif            2             regular, italic           1\n2          Abel sans-serif            1                     regular           1\n3  Abhaya Libre      serif            5 regular, 500, 600, 700, 800           3\n4 Abril Fatface    display            1                     regular           2\n5      Aclonica sans-serif            1                     regular           1\n6          Acme sans-serif            1                     regular           1\n                    subsets version lastModified\n1                     latin     v20   2022-01-27\n2                     latin     v12   2020-09-10\n3 latin, latin-ext, sinhala     v11   2022-01-25\n4          latin, latin-ext     v18   2022-01-27\n5                     latin     v16   2022-01-25\n6                     latin     v17   2022-01-27\n\n# use to indicate that showtext is needed \nshowtext_auto()\n\n\nBoxplots\nThis is just ok.\n\ncoffee_ratings_tidy %&gt;%\n  ggplot(aes(x = species, y = rating, fill = species)) +\n  geom_boxplot(color = \"black\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(ylim = c(5,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels), # all that work we did earlier\n             nrow = 2) + # 2 rows in the faceted plot\n  theme_classic() + \n  theme(legend.position = \"none\",\n        text = element_text(family = \"Atkinson Hyperlegible\")) + # changing font\n  labs(x = \"Coffee Species\",\n       y = \"Cupper rating (out of 10)\",\n       title = \"Distribution of cupper scores for individual coffee attributes\",\n       caption = \"Line represents the median rating per species\")\n\n\n\n\n\n\nViolin plot\n\n# violin plot\ncoffee_ratings_tidy %&gt;%\n  ggplot(aes(x = species, y = rating, fill = species)) +\n  geom_violin(draw_quantiles = 0.5, color = \"black\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(ylim = c(5,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels), # all that work we did earlier\n             nrow = 2) + # 2 rows in the faceted plot\n  theme_classic() + \n  theme(legend.position = \"none\",\n        text = element_text(family = \"Atkinson Hyperlegible\")) + # changing font\n  labs(x = \"Coffee Species\",\n       y = \"Cupper rating (out of 10)\",\n       title = \"Distribution of cupper scores for individual coffee attributes\",\n       caption = \"Line represents the median rating per species\")\n\n\n\n\n\n\nDot plots\nThis is just ok.\n\ncoffee_ratings_tidy %&gt;%\n  ggplot(aes(x = species, y = rating, color = species)) +\n  geom_dots(side = \"both\", layout = \"swarm\") +\n  scale_color_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(ylim = c(6,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels),\n             nrow = 2) +\n  theme_classic() + \n  theme(legend.position = \"none\",\n        text = element_text(family = \"Atkinson Hyperlegible\")) + # changing font\n  labs(x = \"Cupper rating (out of 10)\",\n       y = \"Coffee Species\",\n       title = \"Distribution of cupper scores for individual coffee attributes\")\n\n\n\n\n\n\nDensity plot\nThis one I think is my favorite.\n\ncoffee_ratings_tidy %&gt;%\n  ggplot(aes(x = rating, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.8,\n                      quantile_lines = TRUE,\n                      quantiles = 2) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(xlim = c(6,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels),\n             nrow = 2) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  labs(x = \"Cupper rating (out of 10)\",\n       y = \"Coffee Species\",\n       title = \"Distribution of cupper scores for individual coffee attributes\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html",
    "title": "Visualizing Correlations Recitation",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#introduction",
    "title": "Visualizing Correlations Recitation",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-measures-of-blood-pressure",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-measures-of-blood-pressure",
    "title": "Visualizing Correlations Recitation",
    "section": "1. How correlated are different measures of blood pressure?",
    "text": "1. How correlated are different measures of blood pressure?\nIn the NHANES dataset, there are 3 measurements for each systolic (the first/top number) and diastolic blood (the second/bottom number) pressure. How reproducible is each type of blood pressure measurement over the 3 samplings? Make visualizations to convey your findings."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "title": "Visualizing Correlations Recitation",
    "section": "2. How correlated are different physical measurements, health, and lifestyle variables?",
    "text": "2. How correlated are different physical measurements, health, and lifestyle variables?\nIn the NHANES dataset, there are data for subject BMI, Pulse, BPSysAve, BPDiaAve, TotalChol.\nCreate a series of plots/plot to show the relationship between these variables with each other."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nIf you wanted to make a correlation plot for all variables below.\n\nNHANES_trimmed &lt;- NHANES %&gt;%\n  select(Age, BMI, Pulse, starts_with(\"BP\"), TotChol) %&gt;%\n  drop_na()\n\nNHANES_cor &lt;- cor(NHANES_trimmed)"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#introduction",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nIf you wanted to make a correlation plot for all variables below.\n\nNHANES_trimmed &lt;- NHANES %&gt;%\n  select(Age, BMI, Pulse, starts_with(\"BP\"), TotChol) %&gt;%\n  drop_na()\n\nNHANES_cor &lt;- cor(NHANES_trimmed)"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-measures-of-blood-pressure",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-measures-of-blood-pressure",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "1. How correlated are different measures of blood pressure?",
    "text": "1. How correlated are different measures of blood pressure?\nIn the NHANES dataset, there are 3 measurements for each systolic (the first/top number) and diastolic blood (the second/bottom number) pressure. How reproducible is each type of blood pressure measurement over the 3 samplings? Make visualizations to convey your findings.\n\nWrangling, creating two dataframes\n\nIncludes the 4 measures for systolic BP BPSysAve, BPSys1, BPSys2, BPSys3\nIncludes the 4 measures for diastolic BP BPDiaAve, BPDia1, BPDia2, BPDia3\n\n\n# create df with all of the BP measurements\n# remove missing values\nNHANES_BP &lt;- NHANES %&gt;%\n  select(starts_with(\"BP\")) %&gt;%\n  drop_na()\n\n# create df with all systolic data\nNHANES_systolic &lt;- NHANES_BP %&gt;%\n  select(contains(\"Sys\"))\n\n# create df with all diastolic data\nNHANES_diastolic &lt;- NHANES_BP %&gt;%\n  select(contains(\"Dia\"))\n\n\n\nLooking at relationships using scatteplots\nWe can look quickly at the relationship betwen all the diastolic BP measurements, and all of the systolic BP measurements using ggpairs().\n\nNHANES_diastolic %&gt;%\n  ggpairs(title = \"Diastolic Blood Pressure Relationships\")\n\n\n\n\nFrom the diastolic data, we can see some values that are zero. Those are biologically implausible so I am going to elect to remove those observations.\n\nNHANES_diastolic_no0 &lt;- NHANES_diastolic %&gt;%\n  filter(BPDiaAve &gt; 0 & BPDia1 &gt; 0 & BPDia2 &gt; 0 & BPDia3 &gt; 0)\n\n# how many observations are there?\nnrow(NHANES_diastolic)\n\n[1] 7971\n\n# how many obesrvations after removing zero diastolic\nnrow(NHANES_diastolic_no0)\n\n[1] 7803\n\n# how many observations did we remove?\nnrow(NHANES_diastolic) - nrow(NHANES_diastolic_no0)\n\n[1] 168\n\n\nTry again now that we‚Äôve removed diastolic BP values that are zero.\n\nNHANES_diastolic_no0 %&gt;%\n  ggpairs(title = \"Diastolic Blood Pressure Relationships\")\n\n\n\n\nThis looks better.\n\nNHANES_systolic %&gt;%\n  ggpairs(title = \"Systolic Blood Pressure Relationships\")\n\n\n\n\n\n\nRun correlation analysis with cor() and rcorr()\n\n# run systolic correlation analysis\nNHANES_sys_cor &lt;- cor(NHANES_systolic)\n\n# could also use rcorr()\nNHANES_sys_rcorr &lt;- rcorr(as.matrix(NHANES_systolic))\n\n# run diastolic correlation analysis\nNHANES_dia_cor &lt;- cor(NHANES_diastolic_no0)\n\n# could also use rcorr()\nNHANES_dia_rcorr &lt;- rcorr(as.matrix(NHANES_diastolic_no0))\n\n\n\nPrepare to plot with corrplot()\n\n# create a vector of the systolic names for labeling\nsys_labels &lt;- c(\"Systolic BP, Average\",\n                \"Systolic BP 1\",\n                \"Systolic BP 2\",\n                \"Systolic BP 3\")\n\ndia_labels &lt;- c(\"Diastolic BP, Average\",\n                \"Diastolic BP 1\",\n                \"Diastolic BP 2\",\n                \"Diastolic BP 3\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(NHANES_sys_rcorr$r) &lt;- sys_labels\nrownames(NHANES_sys_rcorr$r) &lt;- sys_labels\ncolnames(NHANES_dia_rcorr$r) &lt;- dia_labels\nrownames(NHANES_dia_rcorr$r) &lt;- dia_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(NHANES_sys_rcorr$P) &lt;- sys_labels\nrownames(NHANES_sys_rcorr$P) &lt;- sys_labels\ncolnames(NHANES_dia_rcorr$P) &lt;- dia_labels\nrownames(NHANES_dia_rcorr$P) &lt;- dia_labels\n\nPlot\n\ncorrplot(NHANES_sys_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = NHANES_sys_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"white\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 1.0) # size of correlation font\n\n\n\n\n\ncorrplot(NHANES_dia_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = NHANES_dia_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"white\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 1.0) # size of correlation font\n\n\n\n\n\n\nPrepare to plot with melt() and ggplot()\nCreate a lower triangle object to plot.\n\n# \"save as\"\nsys_lower &lt;- NHANES_sys_cor\ndia_lower &lt;- NHANES_dia_cor\n\n# use function upper.tri() and set the upper triangle all to NA\n# then we can keep only the lower triangle\nsys_lower[upper.tri(sys_lower)] &lt;- NA\ndia_lower[upper.tri(dia_lower)] &lt;- NA\n\n# melt to go back to long format\nmelted_sys_lower &lt;- melt(sys_lower, na.rm = TRUE)\nmelted_dia_lower &lt;- melt(dia_lower, na.rm = TRUE)\n\n# did it work?\nhead(melted_sys_lower) \n\n      Var1     Var2     value\n1 BPSysAve BPSysAve 1.0000000\n2   BPSys1 BPSysAve 0.9526899\n3   BPSys2 BPSysAve 0.9881045\n4   BPSys3 BPSysAve 0.9876269\n6   BPSys1   BPSys1 1.0000000\n7   BPSys2   BPSys1 0.9468706\n\nhead(melted_dia_lower) \n\n      Var1     Var2     value\n1 BPDiaAve BPDiaAve 1.0000000\n2   BPDia1 BPDiaAve 0.9091983\n3   BPDia2 BPDiaAve 0.9769738\n4   BPDia3 BPDiaAve 0.9770299\n6   BPDia1   BPDia1 1.0000000\n7   BPDia2   BPDia1 0.8979354\n\n\nPlot systolic\n\n# create a vector of the systolic names for labeling\nsys_labels &lt;- c(\"Systolic BP, Average\",\n                \"Systolic BP 1\",\n                \"Systolic BP 2\",\n                \"Systolic BP 3\")\n\nmelted_sys_lower %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#ef8a62\",\n                       mid = \"#f7f7f7\",\n                       high = \"#67a9cf\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = sys_labels) +\n  scale_y_discrete(labels = sys_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y = \"\",\n       title = \"Correlation measures of systolic blood pressure at 3 times\",\n       subtitle = \"Data collected from NHANES 2009-2012\",\n       caption = \"Number presents correlation coefficient \\nAll correlations are statistically significant (p &lt; 0.05)\")\n\n\n\n\nPlot diastolic\n\n# create a vector of the systolic names for labeling\ndia_labels &lt;- c(\"Diastolic BP, Average\",\n                \"Diastolic BP 1\",\n                \"Diastolic BP 2\",\n                \"Diastolic BP 3\")\n\nmelted_dia_lower %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#f1a340\",\n                       mid = \"#f7f7f7\",\n                       high = \"#998ec3\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = dia_labels) +\n  scale_y_discrete(labels = dia_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y =\"\",\n       title = \"Correlation measures of diastolic blood pressure at 3 times\",\n       subtitle = \"Data collected from NHANES 2009-2012\",\n       caption = \"Number presents correlation coefficient \\nAll correlations are statistically significant (p &lt; 0.05)\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "2. How correlated are different physical measurements, health, and lifestyle variables?",
    "text": "2. How correlated are different physical measurements, health, and lifestyle variables?\nIn the NHANES dataset, there are data for subject BMI, Pulse, BPSysAve, BPDiaAve, TotalChol.\nCreate a series of plots to show the relationship between these variables with each other.\n\nWrangle\nCreate a dataframe that includes only the variables we want to correlate, and drop the observations with missing values.\n\nnhanes_trimmed &lt;- NHANES %&gt;%\n  select(BMI, Pulse, BPSysAve, BPDiaAve, TotChol) %&gt;%\n  drop_na()\n\n\n\nVisualize with ggpairs()\nHere, we don‚Äôt have to specify columns since we‚Äôre using them all.\n\nnhanes_trimmed %&gt;%\n    ggpairs(aes(alpha = 0.01), # note alpha inside aes which is weird idk why\n            lower = list(continuous = \"smooth\"),\n            columnLabels = c(\"BMI\", \"Pulse\", \"Systolic BP\", \"Diastolic BP\", \"Total Cholesterol\"))\n\n\n\n\n\n\nCreate a correlation plot with corrplot()\nFirst we will make our trimmed df a matrix.\n\n# convert into a matrix as this is what corrplot takes\nnhanes_trimmed_matrix &lt;- nhanes_trimmed %&gt;%\n  as.matrix() \n\nnhanes_rcorr &lt;- rcorr(nhanes_trimmed_matrix, type = \"pearson\")\n\n# correlation matrix\nnhanes_rcorr$r\n\n                BMI         Pulse    BPSysAve   BPDiaAve       TotChol\nBMI      1.00000000  1.514764e-02  0.24526330 0.23705522  1.213842e-01\nPulse    0.01514764  1.000000e+00 -0.09670785 0.01217557 -9.486773e-05\nBPSysAve 0.24526330 -9.670785e-02  1.00000000 0.40476431  2.174280e-01\nBPDiaAve 0.23705522  1.217557e-02  0.40476431 1.00000000  2.620035e-01\nTotChol  0.12138419 -9.486773e-05  0.21742801 0.26200353  1.000000e+00\n\n# pvalue matrix\nnhanes_rcorr$P\n\n               BMI     Pulse BPSysAve  BPDiaAve  TotChol\nBMI             NA 0.1772467        0 0.0000000 0.000000\nPulse    0.1772467        NA        0 0.2781342 0.993258\nBPSysAve 0.0000000 0.0000000       NA 0.0000000 0.000000\nBPDiaAve 0.0000000 0.2781342        0        NA 0.000000\nTotChol  0.0000000 0.9932580        0 0.0000000       NA\n\n\nWrangle labels\n\n# create a vector of how i want the labels to look\nnhanes_labels &lt;- c(\"BMI\",\n                   \"Pulse\",\n                   \"Systolic \\nBlood Pressure\",\n                   \"Diastolic \\nBlood Pressure\",\n                   \"Total Cholesterol\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(nhanes_rcorr$r) &lt;- nhanes_labels\nrownames(nhanes_rcorr$r) &lt;- nhanes_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(nhanes_rcorr$P) &lt;- nhanes_labels\nrownames(nhanes_rcorr$P) &lt;- nhanes_labels\n\nMake the correlation plot. The numbers are the correlation coefficients for relationships that are significant based on our criteria.\n\ncorrplot(nhanes_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = nhanes_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"black\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 0.6) # size of correlation font"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\nlibrary(tidyverse)\n\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class.\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years\n\n\n# read in happiness data from your computer\n# mine has the path below since i have a subfolder called data where\n# the happiness data is living\nhappiness &lt;- read_csv(\"data/hapiscore_whr.csv\")\n\nRows: 163 Columns: 19\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (1): country\ndbl (18): 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in life expectancy data from your computer\nlife_expectancy &lt;- read_csv(\"data/life_expectancy.csv\")\n\nRows: 195 Columns: 302\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (1): country\ndbl (301): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#introduction",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\nlibrary(tidyverse)\n\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class.\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years\n\n\n# read in happiness data from your computer\n# mine has the path below since i have a subfolder called data where\n# the happiness data is living\nhappiness &lt;- read_csv(\"data/hapiscore_whr.csv\")\n\nRows: 163 Columns: 19\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (1): country\ndbl (18): 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in life expectancy data from your computer\nlife_expectancy &lt;- read_csv(\"data/life_expectancy.csv\")\n\nRows: 195 Columns: 302\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (1): country\ndbl (301): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#explore-your-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#explore-your-data",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Explore your data",
    "text": "Explore your data\nWrite some code that lets you explore that is in these two datasets.\n\n# see data structure with glimpse\nglimpse(happiness)\n\nRows: 163\nColumns: 19\n$ country &lt;chr&gt; \"Afghanistan\", \"Angola\", \"Albania\", \"United Arab Emirates\", \"A‚Ä¶\n$ `2005`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 73.4, NA, NA, NA, 72.6, NA, NA, NA, NA‚Ä¶\n$ `2006`  &lt;dbl&gt; NA, NA, NA, 67.3, 63.1, 42.9, NA, 71.2, 47.3, NA, NA, 33.3, 38‚Ä¶\n$ `2007`  &lt;dbl&gt; NA, NA, 46.3, NA, 60.7, 48.8, 72.8, NA, 45.7, NA, 72.2, NA, 40‚Ä¶\n$ `2008`  &lt;dbl&gt; 37.2, NA, NA, NA, 59.6, 46.5, 72.5, 71.8, 48.2, 35.6, 71.2, 36‚Ä¶\n$ `2009`  &lt;dbl&gt; 44.0, NA, 54.9, 68.7, 64.2, 41.8, NA, NA, 45.7, 37.9, NA, NA, ‚Ä¶\n$ `2010`  &lt;dbl&gt; 47.6, NA, 52.7, 71.0, 64.4, 43.7, 74.5, 73.0, 42.2, NA, 68.5, ‚Ä¶\n$ `2011`  &lt;dbl&gt; 38.3, 55.9, 58.7, 71.2, 67.8, 42.6, 74.1, 74.7, 46.8, 37.1, 71‚Ä¶\n$ `2012`  &lt;dbl&gt; 37.8, 43.6, 55.1, 72.2, 64.7, 43.2, 72.0, 74.0, 49.1, NA, 69.3‚Ä¶\n$ `2013`  &lt;dbl&gt; 35.7, 39.4, 45.5, 66.2, 65.8, 42.8, 73.6, 75.0, 54.8, NA, 71.0‚Ä¶\n$ `2014`  &lt;dbl&gt; 31.3, 38.0, 48.1, 65.4, 66.7, 44.5, 72.9, 69.5, 52.5, 29.1, 68‚Ä¶\n$ `2015`  &lt;dbl&gt; 39.8, NA, 46.1, 65.7, 67.0, 43.5, 73.1, 70.8, 51.5, NA, 69.0, ‚Ä¶\n$ `2016`  &lt;dbl&gt; 42.2, NA, 45.1, 68.3, 64.3, 43.3, 72.5, 70.5, 53.0, NA, 69.5, ‚Ä¶\n$ `2017`  &lt;dbl&gt; 26.6, NA, 46.4, 70.4, 60.4, 42.9, 72.6, 72.9, 51.5, NA, 69.3, ‚Ä¶\n$ `2018`  &lt;dbl&gt; 26.9, NA, 50.0, 66.0, 57.9, 50.6, 71.8, 74.0, 51.7, 37.8, 68.9‚Ä¶\n$ `2019`  &lt;dbl&gt; 23.8, NA, 50.0, 67.1, 60.9, 54.9, 72.3, 72.0, 51.7, NA, 67.7, ‚Ä¶\n$ `2020`  &lt;dbl&gt; NA, NA, 53.6, 64.6, 59.0, NA, 71.4, 72.1, NA, NA, 68.4, 44.1, ‚Ä¶\n$ `2021`  &lt;dbl&gt; 24.4, NA, 52.5, 67.3, 59.1, 53.0, 71.1, 70.8, NA, NA, 68.8, 44‚Ä¶\n$ `2022`  &lt;dbl&gt; 18.6, NA, 52.8, 65.7, 60.2, 53.4, 71.0, 71.0, NA, NA, 68.6, 43‚Ä¶\n\n# look at all columns and first 6 rows with head\nhead(happiness)\n\n# A tibble: 6 √ó 19\n  country  `2005` `2006` `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014`\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghani‚Ä¶     NA   NA     NA     37.2   44     47.6   38.3   37.8   35.7   31.3\n2 Angola       NA   NA     NA     NA     NA     NA     55.9   43.6   39.4   38  \n3 Albania      NA   NA     46.3   NA     54.9   52.7   58.7   55.1   45.5   48.1\n4 United ‚Ä¶     NA   67.3   NA     NA     68.7   71     71.2   72.2   66.2   65.4\n5 Argenti‚Ä¶     NA   63.1   60.7   59.6   64.2   64.4   67.8   64.7   65.8   66.7\n6 Armenia      NA   42.9   48.8   46.5   41.8   43.7   42.6   43.2   42.8   44.5\n# ‚Ñπ 8 more variables: `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n# this will open the file for you to look through in R\n# View(happiness)\n\n\n# see data structure with glimpse\nglimpse(life_expectancy)\n\nRows: 195\nColumns: 302\n$ country &lt;chr&gt; \"Afghanistan\", \"Angola\", \"Albania\", \"Andorra\", \"United Arab Em‚Ä¶\n$ `1800`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1801`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1802`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1803`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1804`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1805`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1806`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1807`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1808`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1809`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1810`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1811`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1812`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1813`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1814`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1815`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1816`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1817`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1818`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1819`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1820`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1821`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1822`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1823`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1824`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1825`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1826`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1827`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1828`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1829`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1830`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1831`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1832`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1833`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1834`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1835`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1836`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1837`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1838`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1839`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1840`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1841`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1842`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1843`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1844`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1845`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1846`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1847`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1848`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1849`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1850`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1851`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1852`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1853`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1854`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1855`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1856`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1857`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1858`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1859`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1860`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1861`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1862`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1863`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1864`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 33.5, 33.5, 34.0, 34.4, 28.6‚Ä¶\n$ `1865`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 33.0, 33.5, 34.0, 34.4, 28.1‚Ä¶\n$ `1866`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 32.4, 33.5, 34.0, 34.4, 27.6‚Ä¶\n$ `1867`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.9, 33.5, 34.0, 34.4, 27.1‚Ä¶\n$ `1868`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.4, 33.5, 34.0, 34.4, 26.6‚Ä¶\n$ `1869`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.4, 33.5, 34.0, 34.4, 26.6‚Ä¶\n$ `1870`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.5, 33.5, 34.0, 34.4, 26.6‚Ä¶\n$ `1871`  &lt;dbl&gt; 27.7, 27.2, 35.4, NA, 30.9, 33.2, 31.5, 33.5, 34.6, 34.5, 26.6‚Ä¶\n$ `1872`  &lt;dbl&gt; 27.9, 27.4, 35.4, NA, 31.0, 33.2, 31.6, 33.5, 35.1, 34.5, 26.5‚Ä¶\n$ `1873`  &lt;dbl&gt; 28.1, 27.6, 35.4, NA, 31.2, 33.2, 31.6, 33.5, 35.6, 34.6, 26.4‚Ä¶\n$ `1874`  &lt;dbl&gt; 28.3, 27.7, 35.3, NA, 31.3, 33.3, 31.8, 33.5, 36.2, 34.6, 26.5‚Ä¶\n$ `1875`  &lt;dbl&gt; 28.5, 27.9, 35.3, NA, 31.5, 33.3, 32.0, 33.6, 36.7, 34.7, 26.6‚Ä¶\n$ `1876`  &lt;dbl&gt; 28.7, 28.1, 35.3, NA, 31.6, 33.3, 32.2, 33.6, 37.2, 34.7, 26.7‚Ä¶\n$ `1877`  &lt;dbl&gt; 28.9, 28.3, 35.3, NA, 31.8, 33.3, 32.4, 33.6, 37.8, 34.8, 26.8‚Ä¶\n$ `1878`  &lt;dbl&gt; 29.1, 28.5, 35.3, NA, 32.0, 33.2, 32.6, 33.6, 38.3, 34.8, 26.8‚Ä¶\n$ `1879`  &lt;dbl&gt; 29.3, 28.7, 35.3, NA, 32.1, 33.2, 32.8, 33.6, 38.8, 34.9, 26.9‚Ä¶\n$ `1880`  &lt;dbl&gt; 29.4, 28.9, 35.2, NA, 32.3, 33.2, 33.0, 33.6, 39.4, 34.9, 27.0‚Ä¶\n$ `1881`  &lt;dbl&gt; 29.6, 29.1, 35.2, NA, 32.4, 33.1, 33.2, 33.6, 39.9, 35.0, 27.0‚Ä¶\n$ `1882`  &lt;dbl&gt; 29.8, 29.3, 35.2, NA, 32.6, 33.0, 33.4, 33.6, 40.4, 35.2, 27.1‚Ä¶\n$ `1883`  &lt;dbl&gt; 30.0, 29.4, 35.2, NA, 32.8, 33.0, 33.6, 33.6, 41.0, 35.5, 27.2‚Ä¶\n$ `1884`  &lt;dbl&gt; 30.2, 29.6, 35.2, NA, 32.9, 32.9, 33.8, 33.6, 41.5, 35.7, 27.3‚Ä¶\n$ `1885`  &lt;dbl&gt; 30.4, 29.8, 35.2, NA, 33.1, 32.8, 34.0, 33.6, 42.0, 35.9, 27.3‚Ä¶\n$ `1886`  &lt;dbl&gt; 30.6, 30.0, 35.1, NA, 33.2, 33.1, 34.2, 33.6, 42.6, 36.1, 27.4‚Ä¶\n$ `1887`  &lt;dbl&gt; 30.8, 30.2, 35.1, NA, 33.4, 33.4, 34.4, 33.6, 43.1, 36.4, 27.4‚Ä¶\n$ `1888`  &lt;dbl&gt; 31.0, 30.4, 35.1, NA, 33.5, 33.7, 34.6, 33.6, 43.6, 36.6, 27.5‚Ä¶\n$ `1889`  &lt;dbl&gt; 31.2, 30.6, 35.1, NA, 33.7, 34.0, 34.8, 33.6, 44.2, 36.8, 27.6‚Ä¶\n$ `1890`  &lt;dbl&gt; 31.4, 30.8, 35.1, NA, 33.9, 34.3, 35.0, 33.6, 44.7, 37.1, 27.6‚Ä¶\n$ `1891`  &lt;dbl&gt; 31.6, 30.9, 35.1, NA, 34.0, 34.1, 35.2, 33.6, 45.2, 37.3, 27.7‚Ä¶\n$ `1892`  &lt;dbl&gt; 31.8, 31.1, 35.0, NA, 34.2, 34.0, 35.4, 33.6, 45.8, 37.8, 27.7‚Ä¶\n$ `1893`  &lt;dbl&gt; 32.0, 31.3, 35.0, NA, 34.3, 33.9, 35.6, 33.6, 46.3, 38.2, 27.8‚Ä¶\n$ `1894`  &lt;dbl&gt; 32.2, 31.5, 35.0, NA, 34.5, 33.8, 35.8, 33.6, 46.8, 38.7, 27.8‚Ä¶\n$ `1895`  &lt;dbl&gt; 32.4, 31.7, 35.0, NA, 34.6, 33.6, 36.0, 33.6, 47.4, 39.2, 27.9‚Ä¶\n$ `1896`  &lt;dbl&gt; 32.5, 31.9, 35.0, NA, 34.8, 34.3, 36.2, 33.6, 47.9, 39.6, 27.9‚Ä¶\n$ `1897`  &lt;dbl&gt; 32.7, 32.1, 35.0, NA, 35.0, 35.1, 36.4, 33.6, 48.4, 40.1, 28.0‚Ä¶\n$ `1898`  &lt;dbl&gt; 32.9, 32.3, 35.0, NA, 35.1, 35.8, 36.2, 33.7, 49.0, 40.6, 27.7‚Ä¶\n$ `1899`  &lt;dbl&gt; 33.1, 32.5, 34.9, NA, 35.3, 36.5, 36.1, 33.7, 49.5, 41.0, 27.4‚Ä¶\n$ `1900`  &lt;dbl&gt; 33.3, 32.6, 34.9, NA, 35.4, 37.2, 35.9, 33.7, 50.0, 41.5, 27.1‚Ä¶\n$ `1901`  &lt;dbl&gt; 33.5, 32.8, 34.9, NA, 35.6, 37.8, 36.1, 33.7, 50.6, 42.0, 27.2‚Ä¶\n$ `1902`  &lt;dbl&gt; 33.7, 33.0, 34.9, NA, 35.7, 38.4, 36.4, 33.7, 51.1, 41.0, 27.3‚Ä¶\n$ `1903`  &lt;dbl&gt; 33.9, 33.2, 34.9, NA, 35.9, 39.0, 36.6, 33.7, 51.6, 40.1, 27.4‚Ä¶\n$ `1904`  &lt;dbl&gt; 34.1, 33.4, 34.9, NA, 36.1, 39.6, 36.9, 33.7, 52.2, 40.7, 27.4‚Ä¶\n$ `1905`  &lt;dbl&gt; 34.3, 33.6, 34.8, NA, 36.2, 40.2, 37.1, 33.7, 52.7, 41.3, 27.5‚Ä¶\n$ `1906`  &lt;dbl&gt; 34.5, 33.8, 34.8, NA, 36.4, 41.0, 37.4, 33.7, 53.2, 42.0, 27.6‚Ä¶\n$ `1907`  &lt;dbl&gt; 34.7, 34.0, 34.8, NA, 36.5, 41.8, 37.6, 33.7, 53.8, 42.6, 27.7‚Ä¶\n$ `1908`  &lt;dbl&gt; 34.9, 34.2, 34.8, NA, 36.7, 42.6, 37.9, 33.7, 54.3, 43.2, 27.8‚Ä¶\n$ `1909`  &lt;dbl&gt; 35.0, 34.4, 34.8, NA, 36.8, 43.4, 38.1, 33.7, 54.8, 43.8, 27.8‚Ä¶\n$ `1910`  &lt;dbl&gt; 35.2, 34.5, 34.8, NA, 37.0, 44.2, 38.4, 33.7, 55.4, 44.5, 27.9‚Ä¶\n$ `1911`  &lt;dbl&gt; 35.4, 34.7, 34.7, NA, 37.2, 44.7, 41.4, 33.7, 55.9, 45.1, 30.2‚Ä¶\n$ `1912`  &lt;dbl&gt; 35.6, 34.9, 34.7, NA, 37.3, 45.3, 41.8, 33.7, 56.4, 45.6, 30.4‚Ä¶\n$ `1913`  &lt;dbl&gt; 35.8, 35.1, 34.7, NA, 37.5, 45.9, 39.6, 33.7, 57.0, 46.2, 28.4‚Ä¶\n$ `1914`  &lt;dbl&gt; 36.0, 35.3, 34.7, NA, 37.6, 46.4, 39.2, 33.7, 57.5, 46.8, 28.0‚Ä¶\n$ `1915`  &lt;dbl&gt; 36.2, 35.5, 34.7, NA, 37.8, 47.0, 38.8, 33.7, 58.0, 47.3, 27.5‚Ä¶\n$ `1916`  &lt;dbl&gt; 36.4, 35.7, 34.7, NA, 38.0, 47.8, 38.8, 33.7, 58.6, 47.9, 27.4‚Ä¶\n$ `1917`  &lt;dbl&gt; 36.6, 35.9, 34.6, NA, 38.1, 48.7, 35.8, 33.7, 59.1, 48.5, 24.8‚Ä¶\n$ `1918`  &lt;dbl&gt; 9.59, 13.90, 19.00, NA, 31.70, 42.50, 27.00, 21.90, 55.00, 32.‚Ä¶\n$ `1919`  &lt;dbl&gt; 36.9, 36.2, 34.6, NA, 38.4, 50.3, 37.0, 33.8, 60.2, 49.6, 25.4‚Ä¶\n$ `1920`  &lt;dbl&gt; 37.1, 36.4, 34.6, NA, 38.6, 51.2, 28.0, 33.8, 60.7, 50.2, 23.6‚Ä¶\n$ `1921`  &lt;dbl&gt; 37.3, 36.6, 34.6, NA, 38.7, 51.7, 38.0, 33.8, 61.3, 50.7, 25.9‚Ä¶\n$ `1922`  &lt;dbl&gt; 37.5, 36.8, 34.6, NA, 38.9, 52.2, 39.0, 34.6, 63.1, 51.3, 26.5‚Ä¶\n$ `1923`  &lt;dbl&gt; 37.7, 37.0, 34.5, NA, 39.0, 52.7, 39.9, 35.4, 62.0, 51.9, 27.2‚Ä¶\n$ `1924`  &lt;dbl&gt; 37.9, 37.2, 34.5, NA, 39.2, 53.2, 42.4, 36.3, 62.8, 52.4, 29.0‚Ä¶\n$ `1925`  &lt;dbl&gt; 38.1, 37.4, 34.5, NA, 39.4, 53.7, 41.5, 37.1, 63.5, 53.0, 28.1‚Ä¶\n$ `1926`  &lt;dbl&gt; 38.3, 37.6, 34.5, NA, 39.5, 54.1, 44.7, 38.0, 63.2, 53.6, 30.4‚Ä¶\n$ `1927`  &lt;dbl&gt; 38.4, 37.8, 34.5, NA, 39.7, 54.4, 44.0, 38.8, 63.2, 54.1, 29.7‚Ä¶\n$ `1928`  &lt;dbl&gt; 38.6, 37.9, 34.5, NA, 39.8, 54.8, 45.5, 39.6, 63.2, 54.7, 30.7‚Ä¶\n$ `1929`  &lt;dbl&gt; 38.8, 38.1, 34.5, NA, 40.0, 55.2, 44.3, 40.5, 63.4, 55.3, 29.6‚Ä¶\n$ `1930`  &lt;dbl&gt; 39.0, 38.3, 35.3, NA, 40.1, 55.5, 43.6, 41.3, 65.2, 55.8, 28.9‚Ä¶\n$ `1931`  &lt;dbl&gt; 39.2, 38.5, 36.1, NA, 40.3, 55.6, 42.0, 42.1, 65.7, 56.4, 27.4‚Ä¶\n$ `1932`  &lt;dbl&gt; 39.4, 38.7, 37.0, NA, 40.5, 55.6, 39.5, 43.0, 66.0, 56.7, 25.4‚Ä¶\n$ `1933`  &lt;dbl&gt; 39.6, 38.9, 37.9, NA, 40.6, 55.6, 33.0, 43.8, 65.8, 57.0, 19.8‚Ä¶\n$ `1934`  &lt;dbl&gt; 39.8, 39.1, 38.7, NA, 40.8, 55.6, 46.0, 44.7, 65.2, 57.3, 30.0‚Ä¶\n$ `1935`  &lt;dbl&gt; 39.9, 39.3, 39.6, NA, 40.9, 55.6, 47.5, 45.5, 65.4, 57.6, 30.9‚Ä¶\n$ `1936`  &lt;dbl&gt; 40.1, 39.5, 40.4, NA, 41.1, 56.6, 49.2, 46.3, 65.6, 57.9, 32.0‚Ä¶\n$ `1937`  &lt;dbl&gt; 40.3, 39.6, 41.3, NA, 41.3, 57.7, 48.2, 47.2, 66.1, 58.2, 31.1‚Ä¶\n$ `1938`  &lt;dbl&gt; 40.5, 39.8, 42.1, NA, 41.4, 58.7, 49.9, 48.0, 66.2, 58.5, 32.1‚Ä¶\n$ `1939`  &lt;dbl&gt; 40.7, 40.0, 41.6, NA, 41.6, 59.7, 52.3, 48.9, 66.1, 58.0, 33.6‚Ä¶\n$ `1940`  &lt;dbl&gt; 40.9, 40.2, 40.7, NA, 41.7, 60.7, 49.8, 49.7, 66.6, 57.7, 31.6‚Ä¶\n$ `1941`  &lt;dbl&gt; 41.0, 40.7, 40.1, NA, 41.9, 61.3, 27.4, 50.5, 66.5, 56.4, 21.2‚Ä¶\n$ `1942`  &lt;dbl&gt; 41.2, 41.3, 38.7, NA, 42.0, 61.9, 23.5, 51.4, 66.2, 54.0, 18.6‚Ä¶\n$ `1943`  &lt;dbl&gt; 41.4, 41.8, 35.8, NA, 42.2, 62.5, 21.1, 52.2, 66.7, 50.1, 17.1‚Ä¶\n$ `1944`  &lt;dbl&gt; 41.6, 42.3, 32.9, NA, 42.4, 63.2, 27.5, 53.1, 68.4, 39.1, 22.0‚Ä¶\n$ `1945`  &lt;dbl&gt; 41.8, 42.9, 45.4, NA, 42.5, 63.8, 35.4, 53.9, 68.8, 31.4, 28.7‚Ä¶\n$ `1946`  &lt;dbl&gt; 42.0, 43.4, 48.3, NA, 45.6, 63.7, 49.6, 54.8, 68.3, 55.9, 35.3‚Ä¶\n$ `1947`  &lt;dbl&gt; 42.2, 43.9, 49.7, NA, 48.8, 63.6, 41.5, 55.6, 69.0, 61.2, 29.6‚Ä¶\n$ `1948`  &lt;dbl&gt; 42.4, 44.5, 50.5, NA, 52.0, 63.5, 47.0, 56.5, 68.9, 63.2, 34.9‚Ä¶\n$ `1949`  &lt;dbl&gt; 42.5, 45.0, 51.4, NA, 55.2, 63.4, 47.9, 57.3, 69.5, 63.4, 36.5‚Ä¶\n$ `1950`  &lt;dbl&gt; 42.7, 45.6, 52.2, 74.6, 58.4, 63.3, 48.2, 58.1, 69.4, 64.8, 37‚Ä¶\n$ `1951`  &lt;dbl&gt; 42.9, 45.6, 53.6, 74.7, 58.5, 63.5, 49.0, 58.7, 69.2, 65.5, 38‚Ä¶\n$ `1952`  &lt;dbl&gt; 43.1, 45.6, 54.5, 74.8, 58.6, 64.2, 50.0, 59.3, 69.5, 66.7, 39‚Ä¶\n$ `1953`  &lt;dbl&gt; 43.5, 45.6, 55.4, 75.0, 58.7, 64.1, 51.1, 59.8, 69.9, 67.2, 40‚Ä¶\n$ `1954`  &lt;dbl&gt; 43.3, 45.6, 56.1, 75.1, 58.8, 64.7, 52.1, 60.3, 70.2, 67.3, 41‚Ä¶\n$ `1955`  &lt;dbl&gt; 43.9, 45.5, 56.3, 75.2, 58.9, 64.5, 53.3, 60.9, 70.3, 67.7, 42‚Ä¶\n$ `1956`  &lt;dbl&gt; 44.1, 45.7, 58.0, 75.3, 58.8, 65.2, 54.5, 61.5, 70.4, 67.8, 43‚Ä¶\n$ `1957`  &lt;dbl&gt; 44.3, 45.8, 59.3, 75.4, 59.3, 65.2, 55.7, 61.9, 70.6, 67.8, 44‚Ä¶\n$ `1958`  &lt;dbl&gt; 44.5, 45.9, 61.0, 75.5, 59.6, 65.4, 56.5, 62.6, 71.0, 68.4, 45‚Ä¶\n$ `1959`  &lt;dbl&gt; 44.7, 46.1, 61.7, 75.6, 59.7, 65.4, 58.0, 63.3, 70.9, 68.5, 46‚Ä¶\n$ `1960`  &lt;dbl&gt; 45.0, 46.3, 62.5, 75.7, 60.3, 65.3, 59.2, 63.8, 71.1, 69.0, 48‚Ä¶\n$ `1961`  &lt;dbl&gt; 45.3, 44.8, 63.3, 75.8, 60.8, 65.7, 60.4, 64.8, 71.3, 69.6, 50‚Ä¶\n$ `1962`  &lt;dbl&gt; 45.5, 45.0, 63.3, 75.9, 61.3, 65.8, 61.4, 65.5, 71.2, 69.7, 51‚Ä¶\n$ `1963`  &lt;dbl&gt; 45.7, 45.2, 63.8, 76.0, 61.6, 65.8, 62.0, 65.8, 71.2, 69.8, 53‚Ä¶\n$ `1964`  &lt;dbl&gt; 45.9, 45.4, 64.4, 76.2, 62.1, 65.8, 62.9, 66.1, 71.0, 70.0, 54‚Ä¶\n$ `1965`  &lt;dbl&gt; 46.1, 45.6, 64.8, 76.3, 62.6, 66.1, 63.6, 66.7, 71.1, 70.1, 55‚Ä¶\n$ `1966`  &lt;dbl&gt; 46.3, 45.8, 65.5, 76.4, 63.0, 66.6, 64.2, 67.1, 71.1, 70.2, 56‚Ä¶\n$ `1967`  &lt;dbl&gt; 46.5, 46.0, 66.1, 76.5, 63.4, 66.5, 64.7, 67.2, 71.1, 70.2, 57‚Ä¶\n$ `1968`  &lt;dbl&gt; 46.7, 46.2, 66.5, 76.7, 63.8, 66.0, 65.1, 67.6, 71.0, 70.3, 58‚Ä¶\n$ `1969`  &lt;dbl&gt; 46.9, 46.4, 67.1, 76.8, 64.2, 65.9, 65.3, 68.0, 71.2, 70.2, 58‚Ä¶\n$ `1970`  &lt;dbl&gt; 47.1, 46.6, 67.8, 77.0, 64.0, 66.1, 65.9, 68.3, 71.2, 70.2, 59‚Ä¶\n$ `1971`  &lt;dbl&gt; 47.3, 46.8, 68.3, 77.1, 64.9, 66.9, 66.4, 68.9, 71.4, 70.4, 61‚Ä¶\n$ `1972`  &lt;dbl&gt; 47.3, 47.0, 68.8, 77.2, 65.1, 67.3, 66.8, 69.4, 71.8, 70.7, 61‚Ä¶\n$ `1973`  &lt;dbl&gt; 47.3, 47.2, 69.3, 77.4, 65.4, 67.7, 67.2, 69.8, 71.9, 71.1, 62‚Ä¶\n$ `1974`  &lt;dbl&gt; 47.4, 47.4, 69.8, 77.5, 65.7, 67.9, 67.9, 70.0, 72.0, 71.2, 63‚Ä¶\n$ `1975`  &lt;dbl&gt; 47.5, 47.5, 70.2, 77.7, 66.0, 68.0, 68.2, 70.2, 72.5, 71.4, 64‚Ä¶\n$ `1976`  &lt;dbl&gt; 47.7, 47.5, 70.7, 77.8, 66.3, 67.0, 69.0, 70.3, 72.8, 71.8, 65‚Ä¶\n$ `1977`  &lt;dbl&gt; 47.9, 47.7, 71.1, 78.0, 66.6, 67.7, 69.5, 70.6, 73.3, 72.2, 65‚Ä¶\n$ `1978`  &lt;dbl&gt; 46.4, 47.8, 71.7, 78.1, 67.0, 69.0, 69.7, 71.0, 73.8, 72.4, 64‚Ä¶\n$ `1979`  &lt;dbl&gt; 44.7, 48.0, 71.3, 78.2, 67.3, 69.8, 70.2, 71.6, 74.2, 72.6, 65‚Ä¶\n$ `1980`  &lt;dbl&gt; 43.7, 48.1, 71.3, 78.3, 67.6, 70.2, 70.2, 72.1, 74.5, 72.8, 66‚Ä¶\n$ `1981`  &lt;dbl&gt; 44.3, 48.2, 71.3, 78.4, 68.0, 70.3, 70.5, 72.6, 74.8, 73.1, 66‚Ä¶\n$ `1982`  &lt;dbl&gt; 44.1, 48.2, 71.4, 78.5, 68.1, 70.9, 70.8, 73.2, 74.9, 73.3, 66‚Ä¶\n$ `1983`  &lt;dbl&gt; 42.3, 48.2, 71.2, 78.5, 67.9, 70.7, 70.8, 73.8, 75.3, 73.5, 66‚Ä¶\n$ `1984`  &lt;dbl&gt; 39.9, 48.4, 71.4, 78.6, 68.4, 70.8, 71.1, 73.5, 75.5, 73.9, 66‚Ä¶\n$ `1985`  &lt;dbl&gt; 42.0, 48.6, 71.9, 78.7, 68.5, 71.7, 71.3, 73.8, 75.6, 74.3, 66‚Ä¶\n$ `1986`  &lt;dbl&gt; 43.3, 48.6, 72.3, 78.8, 68.5, 72.0, 71.7, 74.0, 76.0, 74.7, 66‚Ä¶\n$ `1987`  &lt;dbl&gt; 45.9, 48.6, 72.2, 78.8, 68.5, 72.1, 71.8, 74.2, 76.2, 75.1, 66‚Ä¶\n$ `1988`  &lt;dbl&gt; 48.5, 48.6, 72.4, 78.9, 68.5, 72.1, 55.3, 74.3, 76.4, 75.5, 66‚Ä¶\n$ `1989`  &lt;dbl&gt; 52.7, 49.4, 72.5, 79.0, 68.6, 72.3, 71.0, 74.3, 76.5, 75.7, 67‚Ä¶\n$ `1990`  &lt;dbl&gt; 53.8, 49.7, 72.8, 79.0, 68.7, 72.5, 70.6, 74.0, 77.0, 76.0, 66‚Ä¶\n$ `1991`  &lt;dbl&gt; 53.8, 50.3, 72.6, 79.1, 68.7, 72.7, 70.3, 74.0, 77.4, 76.1, 66‚Ä¶\n$ `1992`  &lt;dbl&gt; 54.2, 50.3, 73.2, 79.2, 68.8, 72.8, 69.4, 74.0, 77.6, 76.3, 64‚Ä¶\n$ `1993`  &lt;dbl&gt; 54.4, 49.0, 73.8, 79.3, 68.8, 73.0, 69.0, 73.4, 77.9, 76.5, 64‚Ä¶\n$ `1994`  &lt;dbl&gt; 53.9, 50.3, 74.6, 79.5, 68.7, 73.4, 69.5, 73.3, 78.0, 76.7, 63‚Ä¶\n$ `1995`  &lt;dbl&gt; 54.3, 51.2, 74.6, 79.8, 68.8, 73.4, 70.1, 73.3, 78.3, 77.0, 64‚Ä¶\n$ `1996`  &lt;dbl&gt; 54.7, 51.7, 74.5, 80.0, 68.9, 73.5, 70.4, 74.0, 78.5, 77.3, 65‚Ä¶\n$ `1997`  &lt;dbl&gt; 54.5, 51.6, 72.9, 80.2, 69.0, 73.6, 71.1, 73.5, 78.8, 77.7, 65‚Ä¶\n$ `1998`  &lt;dbl&gt; 53.3, 50.6, 74.8, 80.4, 69.2, 73.7, 71.6, 74.5, 79.1, 77.9, 66‚Ä¶\n$ `1999`  &lt;dbl&gt; 54.7, 51.9, 75.1, 80.6, 69.2, 73.8, 71.9, 74.7, 79.4, 78.2, 66‚Ä¶\n$ `2000`  &lt;dbl&gt; 54.7, 52.8, 75.4, 80.8, 69.1, 74.2, 72.4, 74.8, 79.7, 78.5, 66‚Ä¶\n$ `2001`  &lt;dbl&gt; 54.8, 53.4, 76.0, 80.9, 69.2, 74.3, 72.5, 75.1, 80.1, 78.9, 67‚Ä¶\n$ `2002`  &lt;dbl&gt; 55.5, 54.5, 75.9, 81.1, 69.4, 74.3, 72.7, 75.4, 80.3, 79.0, 67‚Ä¶\n$ `2003`  &lt;dbl&gt; 56.5, 55.1, 75.6, 81.2, 69.3, 74.4, 72.8, 75.6, 80.6, 79.1, 67‚Ä¶\n$ `2004`  &lt;dbl&gt; 57.1, 55.5, 75.8, 81.3, 69.1, 74.9, 73.0, 75.7, 80.9, 79.5, 67‚Ä¶\n$ `2005`  &lt;dbl&gt; 57.6, 56.4, 76.2, 81.4, 69.2, 75.3, 73.0, 75.9, 81.2, 79.8, 67‚Ä¶\n$ `2006`  &lt;dbl&gt; 58.0, 57.0, 76.9, 81.5, 69.5, 75.4, 73.1, 75.9, 81.5, 80.1, 67‚Ä¶\n$ `2007`  &lt;dbl&gt; 58.5, 58.0, 77.5, 81.7, 70.0, 75.3, 73.5, 75.1, 81.5, 80.3, 68‚Ä¶\n$ `2008`  &lt;dbl&gt; 59.2, 58.8, 77.6, 81.8, 70.4, 75.7, 73.5, 75.2, 81.7, 80.5, 68‚Ä¶\n$ `2009`  &lt;dbl&gt; 59.9, 59.5, 78.0, 81.8, 70.6, 75.8, 73.6, 75.8, 81.9, 80.5, 68‚Ä¶\n$ `2010`  &lt;dbl&gt; 60.5, 60.2, 78.1, 81.8, 70.8, 75.9, 73.9, 75.9, 82.1, 80.8, 69‚Ä¶\n$ `2011`  &lt;dbl&gt; 61.0, 60.8, 78.1, 81.9, 71.0, 76.0, 74.2, 76.0, 82.3, 81.0, 69‚Ä¶\n$ `2012`  &lt;dbl&gt; 61.4, 61.4, 78.2, 81.9, 71.2, 76.2, 74.6, 76.0, 82.6, 81.2, 69‚Ä¶\n$ `2013`  &lt;dbl&gt; 61.9, 62.1, 78.3, 82.0, 71.6, 76.3, 75.1, 76.1, 82.7, 81.3, 69‚Ä¶\n$ `2014`  &lt;dbl&gt; 61.9, 63.0, 78.2, 82.0, 73.0, 76.5, 75.2, 76.0, 82.7, 81.5, 69‚Ä¶\n$ `2015`  &lt;dbl&gt; 61.9, 63.5, 78.1, 82.0, 73.2, 76.5, 75.1, 76.0, 82.7, 81.6, 70‚Ä¶\n$ `2016`  &lt;dbl&gt; 62.0, 63.9, 78.2, 82.1, 73.4, 76.2, 75.3, 76.0, 83.0, 81.8, 70‚Ä¶\n$ `2017`  &lt;dbl&gt; 62.9, 64.2, 78.3, 82.1, 73.5, 76.3, 75.5, 76.1, 83.0, 82.0, 70‚Ä¶\n$ `2018`  &lt;dbl&gt; 62.7, 64.6, 78.4, 82.1, 73.7, 76.5, 75.6, 76.2, 82.9, 82.1, 70‚Ä¶\n$ `2019`  &lt;dbl&gt; 63.3, 65.1, 78.5, 82.2, 73.9, 76.6, 75.7, 76.3, 82.9, 82.2, 71‚Ä¶\n$ `2020`  &lt;dbl&gt; 63.4, 65.2, 77.9, NA, 74.0, 74.6, 74.0, 76.3, 82.9, 81.5, 70.4‚Ä¶\n$ `2021`  &lt;dbl&gt; 64.0, 65.8, 78.7, NA, 74.2, 76.9, 76.0, 76.5, 83.2, 82.4, 71.2‚Ä¶\n$ `2022`  &lt;dbl&gt; 64.3, 66.1, 78.8, NA, 74.3, 77.0, 76.1, 76.7, 83.3, 82.6, 71.3‚Ä¶\n$ `2023`  &lt;dbl&gt; 64.6, 66.4, 79.0, NA, 74.4, 77.2, 76.3, 76.8, 83.5, 82.8, 71.4‚Ä¶\n$ `2024`  &lt;dbl&gt; 64.9, 66.8, 79.1, NA, 74.6, 77.3, 76.4, 76.9, 83.6, 82.9, 71.5‚Ä¶\n$ `2025`  &lt;dbl&gt; 65.2, 67.1, 79.2, NA, 74.7, 77.5, 76.5, 77.1, 83.7, 83.1, 71.6‚Ä¶\n$ `2026`  &lt;dbl&gt; 65.4, 67.4, 79.4, NA, 74.8, 77.6, 76.7, 77.2, 83.8, 83.2, 71.7‚Ä¶\n$ `2027`  &lt;dbl&gt; 65.7, 67.7, 79.5, NA, 75.0, 77.8, 76.8, 77.3, 84.0, 83.4, 71.8‚Ä¶\n$ `2028`  &lt;dbl&gt; 66.0, 68.0, 79.7, NA, 75.1, 77.9, 77.0, 77.4, 84.1, 83.6, 71.9‚Ä¶\n$ `2029`  &lt;dbl&gt; 66.2, 68.3, 79.8, NA, 75.2, 78.0, 77.1, 77.6, 84.2, 83.7, 72.0‚Ä¶\n$ `2030`  &lt;dbl&gt; 66.4, 68.6, 80.0, NA, 75.4, 78.2, 77.2, 77.7, 84.3, 83.9, 72.1‚Ä¶\n$ `2031`  &lt;dbl&gt; 66.6, 68.9, 80.2, NA, 75.5, 78.3, 77.4, 77.8, 84.4, 84.1, 72.2‚Ä¶\n$ `2032`  &lt;dbl&gt; 66.9, 69.2, 80.3, NA, 75.6, 78.5, 77.5, 78.0, 84.6, 84.2, 72.3‚Ä¶\n$ `2033`  &lt;dbl&gt; 67.1, 69.4, 80.5, NA, 75.8, 78.6, 77.7, 78.1, 84.7, 84.4, 72.4‚Ä¶\n$ `2034`  &lt;dbl&gt; 67.3, 69.7, 80.7, NA, 75.9, 78.8, 77.8, 78.2, 84.8, 84.5, 72.5‚Ä¶\n$ `2035`  &lt;dbl&gt; 67.5, 70.0, 80.8, NA, 76.0, 78.9, 77.9, 78.3, 84.9, 84.7, 72.7‚Ä¶\n$ `2036`  &lt;dbl&gt; 67.7, 70.2, 80.9, NA, 76.2, 79.1, 78.1, 78.5, 85.0, 84.8, 72.8‚Ä¶\n$ `2037`  &lt;dbl&gt; 67.9, 70.5, 81.1, NA, 76.3, 79.2, 78.2, 78.6, 85.2, 84.9, 72.9‚Ä¶\n$ `2038`  &lt;dbl&gt; 68.0, 70.7, 81.2, NA, 76.4, 79.4, 78.3, 78.7, 85.3, 85.0, 73.0‚Ä¶\n$ `2039`  &lt;dbl&gt; 68.2, 70.9, 81.3, NA, 76.6, 79.5, 78.5, 78.8, 85.4, 85.2, 73.1‚Ä¶\n$ `2040`  &lt;dbl&gt; 68.4, 71.1, 81.5, NA, 76.7, 79.7, 78.6, 79.0, 85.5, 85.3, 73.2‚Ä¶\n$ `2041`  &lt;dbl&gt; 68.6, 71.4, 81.6, NA, 76.8, 79.8, 78.7, 79.1, 85.6, 85.4, 73.3‚Ä¶\n$ `2042`  &lt;dbl&gt; 68.8, 71.6, 81.8, NA, 77.0, 80.0, 78.9, 79.2, 85.7, 85.5, 73.5‚Ä¶\n$ `2043`  &lt;dbl&gt; 68.9, 71.8, 81.9, NA, 77.1, 80.1, 79.0, 79.3, 85.8, 85.6, 73.6‚Ä¶\n$ `2044`  &lt;dbl&gt; 69.1, 72.0, 82.0, NA, 77.2, 80.3, 79.1, 79.5, 86.0, 85.8, 73.7‚Ä¶\n$ `2045`  &lt;dbl&gt; 69.2, 72.2, 82.2, NA, 77.3, 80.4, 79.3, 79.6, 86.1, 85.9, 73.8‚Ä¶\n$ `2046`  &lt;dbl&gt; 69.4, 72.4, 82.3, NA, 77.5, 80.6, 79.4, 79.7, 86.2, 86.0, 73.9‚Ä¶\n$ `2047`  &lt;dbl&gt; 69.5, 72.5, 82.4, NA, 77.6, 80.7, 79.5, 79.8, 86.3, 86.1, 74.0‚Ä¶\n$ `2048`  &lt;dbl&gt; 69.7, 72.7, 82.6, NA, 77.7, 80.8, 79.7, 80.0, 86.4, 86.2, 74.2‚Ä¶\n$ `2049`  &lt;dbl&gt; 69.8, 72.9, 82.7, NA, 77.9, 81.0, 79.8, 80.1, 86.5, 86.3, 74.3‚Ä¶\n$ `2050`  &lt;dbl&gt; 70.0, 73.1, 82.8, NA, 78.0, 81.2, 79.9, 80.2, 86.6, 86.5, 74.4‚Ä¶\n$ `2051`  &lt;dbl&gt; 70.2, 73.3, 83.0, NA, 78.1, 81.3, 80.0, 80.3, 86.8, 86.6, 74.5‚Ä¶\n$ `2052`  &lt;dbl&gt; 70.3, 73.4, 83.1, NA, 78.2, 81.4, 80.2, 80.5, 86.9, 86.7, 74.6‚Ä¶\n$ `2053`  &lt;dbl&gt; 70.4, 73.6, 83.3, NA, 78.4, 81.6, 80.3, 80.6, 87.0, 86.8, 74.7‚Ä¶\n$ `2054`  &lt;dbl&gt; 70.6, 73.8, 83.4, NA, 78.5, 81.7, 80.5, 80.7, 87.1, 86.9, 74.9‚Ä¶\n$ `2055`  &lt;dbl&gt; 70.7, 73.9, 83.5, NA, 78.6, 81.9, 80.6, 80.8, 87.2, 87.0, 75.0‚Ä¶\n$ `2056`  &lt;dbl&gt; 70.9, 74.1, 83.7, NA, 78.7, 82.0, 80.7, 81.0, 87.3, 87.1, 75.1‚Ä¶\n$ `2057`  &lt;dbl&gt; 71.0, 74.2, 83.8, NA, 78.8, 82.2, 80.9, 81.1, 87.4, 87.3, 75.2‚Ä¶\n$ `2058`  &lt;dbl&gt; 71.1, 74.4, 83.9, NA, 79.0, 82.3, 81.0, 81.2, 87.5, 87.4, 75.3‚Ä¶\n$ `2059`  &lt;dbl&gt; 71.3, 74.5, 84.0, NA, 79.1, 82.4, 81.1, 81.3, 87.7, 87.5, 75.5‚Ä¶\n$ `2060`  &lt;dbl&gt; 71.4, 74.7, 84.1, NA, 79.2, 82.6, 81.3, 81.5, 87.8, 87.6, 75.6‚Ä¶\n$ `2061`  &lt;dbl&gt; 71.5, 74.8, 84.3, NA, 79.3, 82.7, 81.4, 81.6, 87.9, 87.7, 75.7‚Ä¶\n$ `2062`  &lt;dbl&gt; 71.7, 75.0, 84.4, NA, 79.4, 82.8, 81.5, 81.7, 88.0, 87.8, 75.8‚Ä¶\n$ `2063`  &lt;dbl&gt; 71.8, 75.1, 84.5, NA, 79.5, 83.0, 81.7, 81.8, 88.1, 87.9, 75.9‚Ä¶\n$ `2064`  &lt;dbl&gt; 72.0, 75.3, 84.6, NA, 79.7, 83.1, 81.8, 82.0, 88.2, 88.0, 76.1‚Ä¶\n$ `2065`  &lt;dbl&gt; 72.1, 75.4, 84.7, NA, 79.8, 83.2, 81.9, 82.1, 88.3, 88.1, 76.2‚Ä¶\n$ `2066`  &lt;dbl&gt; 72.2, 75.5, 84.8, NA, 79.9, 83.4, 82.1, 82.2, 88.5, 88.3, 76.3‚Ä¶\n$ `2067`  &lt;dbl&gt; 72.3, 75.7, 84.9, NA, 80.0, 83.5, 82.2, 82.3, 88.6, 88.4, 76.4‚Ä¶\n$ `2068`  &lt;dbl&gt; 72.5, 75.8, 85.0, NA, 80.1, 83.6, 82.3, 82.5, 88.7, 88.5, 76.6‚Ä¶\n$ `2069`  &lt;dbl&gt; 72.6, 76.0, 85.2, NA, 80.2, 83.7, 82.5, 82.6, 88.8, 88.6, 76.7‚Ä¶\n$ `2070`  &lt;dbl&gt; 72.7, 76.1, 85.3, NA, 80.3, 83.9, 82.6, 82.7, 88.9, 88.7, 76.8‚Ä¶\n$ `2071`  &lt;dbl&gt; 72.9, 76.2, 85.4, NA, 80.4, 84.0, 82.7, 82.8, 89.0, 88.8, 76.9‚Ä¶\n$ `2072`  &lt;dbl&gt; 73.0, 76.4, 85.5, NA, 80.5, 84.1, 82.9, 82.9, 89.1, 88.9, 77.1‚Ä¶\n$ `2073`  &lt;dbl&gt; 73.1, 76.5, 85.6, NA, 80.7, 84.2, 83.0, 83.0, 89.2, 89.0, 77.2‚Ä¶\n$ `2074`  &lt;dbl&gt; 73.3, 76.6, 85.7, NA, 80.8, 84.3, 83.1, 83.2, 89.3, 89.1, 77.3‚Ä¶\n$ `2075`  &lt;dbl&gt; 73.4, 76.8, 85.8, NA, 80.9, 84.5, 83.3, 83.3, 89.5, 89.3, 77.5‚Ä¶\n$ `2076`  &lt;dbl&gt; 73.5, 76.9, 85.9, NA, 81.0, 84.6, 83.4, 83.4, 89.6, 89.4, 77.6‚Ä¶\n$ `2077`  &lt;dbl&gt; 73.7, 77.0, 86.0, NA, 81.0, 84.7, 83.5, 83.5, 89.7, 89.5, 77.7‚Ä¶\n$ `2078`  &lt;dbl&gt; 73.8, 77.2, 86.1, NA, 81.2, 84.8, 83.6, 83.6, 89.8, 89.6, 77.8‚Ä¶\n$ `2079`  &lt;dbl&gt; 74.0, 77.3, 86.2, NA, 81.3, 84.9, 83.8, 83.7, 89.9, 89.7, 78.0‚Ä¶\n$ `2080`  &lt;dbl&gt; 74.1, 77.4, 86.3, NA, 81.3, 85.0, 83.9, 83.8, 90.0, 89.8, 78.1‚Ä¶\n$ `2081`  &lt;dbl&gt; 74.2, 77.5, 86.4, NA, 81.4, 85.1, 84.0, 84.0, 90.1, 89.9, 78.3‚Ä¶\n$ `2082`  &lt;dbl&gt; 74.3, 77.7, 86.5, NA, 81.5, 85.3, 84.1, 84.1, 90.2, 90.0, 78.4‚Ä¶\n$ `2083`  &lt;dbl&gt; 74.5, 77.8, 86.6, NA, 81.6, 85.4, 84.3, 84.2, 90.3, 90.1, 78.5‚Ä¶\n$ `2084`  &lt;dbl&gt; 74.6, 77.9, 86.7, NA, 81.7, 85.5, 84.4, 84.3, 90.5, 90.2, 78.7‚Ä¶\n$ `2085`  &lt;dbl&gt; 74.8, 78.0, 86.8, NA, 81.8, 85.6, 84.5, 84.4, 90.6, 90.3, 78.8‚Ä¶\n$ `2086`  &lt;dbl&gt; 74.9, 78.2, 86.9, NA, 81.9, 85.7, 84.6, 84.5, 90.7, 90.5, 78.9‚Ä¶\n$ `2087`  &lt;dbl&gt; 75.0, 78.3, 87.0, NA, 82.0, 85.8, 84.7, 84.6, 90.8, 90.6, 79.1‚Ä¶\n$ `2088`  &lt;dbl&gt; 75.2, 78.4, 87.1, NA, 82.1, 85.9, 84.8, 84.7, 90.9, 90.7, 79.2‚Ä¶\n$ `2089`  &lt;dbl&gt; 75.3, 78.6, 87.2, NA, 82.2, 86.0, 85.0, 84.8, 91.0, 90.8, 79.3‚Ä¶\n$ `2090`  &lt;dbl&gt; 75.4, 78.7, 87.3, NA, 82.3, 86.1, 85.1, 84.9, 91.1, 90.9, 79.5‚Ä¶\n$ `2091`  &lt;dbl&gt; 75.5, 78.8, 87.4, NA, 82.4, 86.2, 85.2, 85.0, 91.3, 91.0, 79.6‚Ä¶\n$ `2092`  &lt;dbl&gt; 75.7, 79.0, 87.5, NA, 82.5, 86.3, 85.3, 85.1, 91.4, 91.1, 79.7‚Ä¶\n$ `2093`  &lt;dbl&gt; 75.8, 79.1, 87.6, NA, 82.6, 86.5, 85.4, 85.2, 91.5, 91.2, 79.9‚Ä¶\n$ `2094`  &lt;dbl&gt; 76.0, 79.2, 87.7, NA, 82.7, 86.5, 85.5, 85.3, 91.6, 91.3, 80.0‚Ä¶\n$ `2095`  &lt;dbl&gt; 76.1, 79.3, 87.8, NA, 82.8, 86.7, 85.6, 85.4, 91.7, 91.5, 80.1‚Ä¶\n$ `2096`  &lt;dbl&gt; 76.2, 79.5, 87.9, NA, 82.9, 86.8, 85.7, 85.5, 91.8, 91.6, 80.3‚Ä¶\n$ `2097`  &lt;dbl&gt; 76.4, 79.6, 88.0, NA, 83.0, 86.9, 85.8, 85.6, 91.9, 91.7, 80.4‚Ä¶\n$ `2098`  &lt;dbl&gt; 76.5, 79.7, 88.2, NA, 83.1, 87.0, 86.0, 85.7, 92.0, 91.8, 80.5‚Ä¶\n$ `2099`  &lt;dbl&gt; 76.6, 79.9, 88.3, NA, 83.2, 87.1, 86.1, 85.8, 92.1, 91.9, 80.7‚Ä¶\n$ `2100`  &lt;dbl&gt; 76.8, 80.0, 88.4, NA, 83.3, 87.2, 86.2, 85.9, 92.3, 92.0, 80.8‚Ä¶\n\n# look at all columns and first 6 rows with head\nhead(life_expectancy)\n\n# A tibble: 6 √ó 302\n  country  `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghani‚Ä¶   28.2   28.2   28.2   28.2   28.2   28.2   28.1   28.1   28.1   28.1\n2 Angola     27     27     27     27     27     27     27     27     27     27  \n3 Albania    35.4   35.4   35.4   35.4   35.4   35.4   35.4   35.4   35.4   35.4\n4 Andorra    NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  \n5 United ‚Ä¶   30.7   30.7   30.7   30.7   30.7   30.7   30.7   30.7   30.7   30.7\n6 Argenti‚Ä¶   33.2   33.2   33.2   33.2   33.2   33.2   33.2   33.2   33.2   33.2\n# ‚Ñπ 291 more variables: `1810` &lt;dbl&gt;, `1811` &lt;dbl&gt;, `1812` &lt;dbl&gt;, `1813` &lt;dbl&gt;,\n#   `1814` &lt;dbl&gt;, `1815` &lt;dbl&gt;, `1816` &lt;dbl&gt;, `1817` &lt;dbl&gt;, `1818` &lt;dbl&gt;,\n#   `1819` &lt;dbl&gt;, `1820` &lt;dbl&gt;, `1821` &lt;dbl&gt;, `1822` &lt;dbl&gt;, `1823` &lt;dbl&gt;,\n#   `1824` &lt;dbl&gt;, `1825` &lt;dbl&gt;, `1826` &lt;dbl&gt;, `1827` &lt;dbl&gt;, `1828` &lt;dbl&gt;,\n#   `1829` &lt;dbl&gt;, `1830` &lt;dbl&gt;, `1831` &lt;dbl&gt;, `1832` &lt;dbl&gt;, `1833` &lt;dbl&gt;,\n#   `1834` &lt;dbl&gt;, `1835` &lt;dbl&gt;, `1836` &lt;dbl&gt;, `1837` &lt;dbl&gt;, `1838` &lt;dbl&gt;,\n#   `1839` &lt;dbl&gt;, `1840` &lt;dbl&gt;, `1841` &lt;dbl&gt;, `1842` &lt;dbl&gt;, `1843` &lt;dbl&gt;, ‚Ä¶\n\n# this will open the file for you to look through in R\n# View(life_expectancy)\n\nHow many observations there in each dataset?\n\ndim(happiness)\n\n[1] 163  19\n\ndim(life_expectancy)\n\n[1] 195 302\n\n\nWhat years do the data contain information for?\n\nhappiness_long &lt;- happiness |&gt; \n  pivot_longer(cols = !country,\n               names_to = \"year\",\n               values_to = \"happy_value\")\n\nhappiness_long |&gt; \n  summarize(min_year = min(year),\n            max_year = max(year))\n\n# A tibble: 1 √ó 2\n  min_year max_year\n  &lt;chr&gt;    &lt;chr&gt;   \n1 2005     2022    \n\n\n\nlife_expectancy_long &lt;- life_expectancy |&gt; \n  pivot_longer(cols = !country,\n               names_to = \"year\",\n               values_to = \"life_exp\")\n\nlife_expectancy_long |&gt; \n  summarize(min_year = min(year),\n            max_year = max(year))\n\n# A tibble: 1 √ó 2\n  min_year max_year\n  &lt;chr&gt;    &lt;chr&gt;   \n1 1800     2100"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#modifying-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#modifying-data",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Modifying data",
    "text": "Modifying data\nCreate a new dataset for life_expectancy that only includes observed data (i.e., remove the projected data after 2022).\n\nlife_expectancy_actual &lt;- life_expectancy_long |&gt; \n  filter(year &lt;= 2022)\n\nlife_expectancy_actual &lt;- life_expectancy |&gt; \n  select(country, num_range(prefix = \"\", # since there is no prefix here\n                            range = 1800:2022))"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#calculating-summaries",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#calculating-summaries",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Calculating summaries",
    "text": "Calculating summaries\nWhat country has the highest average happiness index in 2022?\n\n# highest happiness is 2022\n# note you can have columns that use non-standard R naming convention\n# like here where columns are numbers\n# but you need to refer to them surrounded by backticks\nhappiness |&gt; \n  select(country, `2022`) |&gt; \n  arrange(desc(`2022`))\n\n# A tibble: 163 √ó 2\n   country     `2022`\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Finland       78  \n 2 Denmark       75.9\n 3 Iceland       75.3\n 4 Israel        74.7\n 5 Netherlands   74  \n 6 Sweden        74  \n 7 Norway        73.2\n 8 Switzerland   72.4\n 9 Luxembourg    72.3\n10 New Zealand   71.2\n# ‚Ñπ 153 more rows\n\n# or we could use clean_names\nhappiness |&gt; \n  janitor::clean_names() |&gt; \n  select(country, x2022) |&gt; \n  arrange(desc(x2022))\n\n# A tibble: 163 √ó 2\n   country     x2022\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 Finland      78  \n 2 Denmark      75.9\n 3 Iceland      75.3\n 4 Israel       74.7\n 5 Netherlands  74  \n 6 Sweden       74  \n 7 Norway       73.2\n 8 Switzerland  72.4\n 9 Luxembourg   72.3\n10 New Zealand  71.2\n# ‚Ñπ 153 more rows\n\n\nWhat about overall average highest index?\n\n# if you want to calculate and have missing values\n# you need to remove them as the default is to not\n\n# using select in rowMeans\nhappiness |&gt; \n  mutate(mean_happiness = rowMeans(select(happiness, -country),\n                                          na.rm = TRUE)) |&gt; \n  arrange(desc(mean_happiness))\n\n# A tibble: 163 √ó 20\n   country `2005` `2006` `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014`\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Denmark   80.2   NA     78.3   79.7   76.8   77.7   77.9   75.2   75.9   75.1\n 2 Finland   NA     76.7   NA     76.7   NA     73.9   73.5   74.2   74.5   73.8\n 3 Switze‚Ä¶   NA     74.7   NA     NA     75.3   NA     NA     77.8   NA     74.9\n 4 Norway    NA     74.2   NA     76.3   NA     NA     NA     76.8   NA     74.4\n 5 Iceland   NA     NA     NA     68.9   NA     NA     NA     75.9   75     NA  \n 6 Nether‚Ä¶   74.6   NA     74.5   76.3   NA     75     75.6   74.7   74.1   73.2\n 7 Sweden    73.8   NA     72.4   75.2   72.7   75     73.8   75.6   74.3   72.4\n 8 Canada    74.2   NA     74.8   74.9   74.9   76.5   74.3   74.2   75.9   73  \n 9 New Ze‚Ä¶   NA     73     76     73.8   NA     72.2   71.9   72.5   72.8   73.1\n10 Austra‚Ä¶   73.4   NA     72.8   72.5   NA     74.5   74.1   72     73.6   72.9\n# ‚Ñπ 153 more rows\n# ‚Ñπ 9 more variables: `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;,\n#   mean_happiness &lt;dbl&gt;\n\nrowMeans(happiness[,-1], na.rm = TRUE)\n\n  [1] 33.87143 44.22500 50.52000 67.80667 62.69412 46.15000 72.60000 72.22000\n  [9] 49.40714 35.50000 69.66250 41.03571 42.58125 46.89412 46.98571 60.15833\n [17] 52.58000 55.71429 62.05000 57.20588 65.56471 51.96667 39.53077 35.16000\n [25] 73.27647 75.04167 63.60000 51.41176 46.88182 46.62353 42.21111 46.09231\n [33] 61.56471 38.88571 70.51765 54.20000 61.18667 66.35000 68.40000 48.25000\n [41] 76.75882 52.84706 53.72727 57.12353 44.79412 65.03529 57.41875 43.69000\n [49] 76.23333 66.68824 45.75833 69.08235 43.67647 47.80588 43.32500 46.20000\n [57] 56.08750 62.47333 59.90000 54.26923 54.77500 57.06667 39.53636 54.10000\n [65] 52.19412 43.94706 70.43125 48.61333 47.25714 74.68000 72.54118 63.02941\n [73] 57.48889 50.74118 60.27647 58.52941 44.12353 52.32353 42.76471 58.92941\n [81] 62.74545 50.05455 45.25294 40.46000 55.43333 43.33125 40.05000 59.42353\n [89] 70.61667 55.35000 50.35833 56.46471 39.84167 52.00000 66.34118 49.78667\n [97] 42.39375 63.90000 43.95455 53.40000 51.54000 47.59000 44.32857 58.76667\n[105] 39.87143 58.38667 45.92500 42.72000 49.69333 57.30588 74.51250 74.83333\n[113] 48.01176 72.87500 68.50000 51.21176 66.49375 55.60588 52.76471 59.54375\n[121] 55.26667 55.68125 46.38000 65.70000 57.35000 55.25294 36.54167 65.29412\n[129] 43.78000 45.12353 65.15333 40.14286 59.90000 51.83333 53.70667 34.02500\n[137] 62.70000 61.18571 61.41333 73.76471 44.93333 40.15714 40.68667 36.52727\n[145] 60.46471 49.81765 56.00000 62.82000 47.18571 51.50588 63.31333 36.96471\n[153] 43.07059 48.16471 62.88824 70.70000 58.32500 59.19412 53.56471 39.13333\n[161] 49.39412 44.23750 38.00000\n\n# using indexing in rowMeans\nhappiness |&gt; \n  mutate(mean_happiness = rowMeans(happiness[-1], na.rm = TRUE)) |&gt; \n  arrange(desc(mean_happiness)) |&gt; \n  select(country, mean_happiness, everything())\n\n# A tibble: 163 √ó 20\n   country     mean_happiness `2005` `2006` `2007` `2008` `2009` `2010` `2011`\n   &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Denmark               76.8   80.2   NA     78.3   79.7   76.8   77.7   77.9\n 2 Finland               76.2   NA     76.7   NA     76.7   NA     73.9   73.5\n 3 Switzerland           75.0   NA     74.7   NA     NA     75.3   NA     NA  \n 4 Norway                74.8   NA     74.2   NA     76.3   NA     NA     NA  \n 5 Iceland               74.7   NA     NA     NA     68.9   NA     NA     NA  \n 6 Netherlands           74.5   74.6   NA     74.5   76.3   NA     75     75.6\n 7 Sweden                73.8   73.8   NA     72.4   75.2   72.7   75     73.8\n 8 Canada                73.3   74.2   NA     74.8   74.9   74.9   76.5   74.3\n 9 New Zealand           72.9   NA     73     76     73.8   NA     72.2   71.9\n10 Australia             72.6   73.4   NA     72.8   72.5   NA     74.5   74.1\n# ‚Ñπ 153 more rows\n# ‚Ñπ 11 more variables: `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;,\n#   `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n# pivoting and then calculating mean after group_by country\nhappiness_long |&gt; \n  group_by(country) |&gt; \n  summarize(mean_happiness = mean(happy_value, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_happiness))\n\n# A tibble: 163 √ó 2\n   country     mean_happiness\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Denmark               76.8\n 2 Finland               76.2\n 3 Switzerland           75.0\n 4 Norway                74.8\n 5 Iceland               74.7\n 6 Netherlands           74.5\n 7 Sweden                73.8\n 8 Canada                73.3\n 9 New Zealand           72.9\n10 Australia             72.6\n# ‚Ñπ 153 more rows\n\n\nHow many countries had an average life expectancy over 80 years in 2022?\n\n# with long data\n# to see what the countries are\nlife_expectancy_long |&gt; # long data\n  filter(year == 2022) |&gt;  # only 2022\n  filter(life_exp &gt; 80) # filter for over 80 years\n\n# A tibble: 36 √ó 3\n   country     year  life_exp\n   &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n 1 Australia   2022      83.3\n 2 Austria     2022      82.6\n 3 Belgium     2022      81.9\n 4 Canada      2022      82.6\n 5 Switzerland 2022      84.4\n 6 Chile       2022      80.6\n 7 Colombia    2022      80.7\n 8 Costa Rica  2022      80.6\n 9 Cyprus      2022      81.3\n10 Germany     2022      81.7\n# ‚Ñπ 26 more rows\n\n# to see how many there are\nlife_expectancy_long |&gt; \n  filter(year == 2022) |&gt; \n  filter(life_exp &gt; 80) |&gt; \n  nrow() # counts rows\n\n[1] 36\n\n# with wide data\n# to see what the countries are\nlife_expectancy |&gt; \n  select(country, `2022`) |&gt; # pick the columns country and 2022\n  filter(`2022` &gt; 80) # filter for 2022 &gt; 80\n\n# A tibble: 36 √ó 2\n   country     `2022`\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Australia     83.3\n 2 Austria       82.6\n 3 Belgium       81.9\n 4 Canada        82.6\n 5 Switzerland   84.4\n 6 Chile         80.6\n 7 Colombia      80.7\n 8 Costa Rica    80.6\n 9 Cyprus        81.3\n10 Germany       81.7\n# ‚Ñπ 26 more rows\n\n# to see how many there are\nlife_expectancy |&gt; \n  select(country, `2022`) |&gt; \n  filter(`2022` &gt; 80) |&gt; \n  nrow() \n\n[1] 36\n\n\nWhat countries are in the top 10 percentile for happiness? What about the bottom? What about for life expectancy? You can calculate this for the most recent data, for the mean, or really for whatever you want. Remember there are lots of ways to do this. Hint - try using the functions in the slice_() family.\n\n# happiness\n# top 10th percentile\nhappiness_long |&gt; \n  group_by(country) |&gt; \n  summarize(mean_happiness = mean(happy_value, na.rm = TRUE)) |&gt; \n  slice_max(order_by = mean_happiness, prop = 0.1)\n\n# A tibble: 16 √ó 2\n   country       mean_happiness\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Denmark                 76.8\n 2 Finland                 76.2\n 3 Switzerland             75.0\n 4 Norway                  74.8\n 5 Iceland                 74.7\n 6 Netherlands             74.5\n 7 Sweden                  73.8\n 8 Canada                  73.3\n 9 New Zealand             72.9\n10 Australia               72.6\n11 Israel                  72.5\n12 Austria                 72.2\n13 United States           70.7\n14 Luxembourg              70.6\n15 Costa Rica              70.5\n16 Ireland                 70.4\n\n# how many countries do we have?\nnrow(happiness)\n\n[1] 163\n\n# how many countries are in the each decile?\nnrow(happiness) * 0.1\n\n[1] 16.3\n\n# we want to pick the top 16 countries\nhappiness_long |&gt; \n  group_by(country) |&gt; \n  summarize(mean_happiness = mean(happy_value, na.rm = TRUE)) |&gt; \n  arrange(-mean_happiness) |&gt; \n  top_n(16)\n\nSelecting by mean_happiness\n\n\n# A tibble: 16 √ó 2\n   country       mean_happiness\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Denmark                 76.8\n 2 Finland                 76.2\n 3 Switzerland             75.0\n 4 Norway                  74.8\n 5 Iceland                 74.7\n 6 Netherlands             74.5\n 7 Sweden                  73.8\n 8 Canada                  73.3\n 9 New Zealand             72.9\n10 Australia               72.6\n11 Israel                  72.5\n12 Austria                 72.2\n13 United States           70.7\n14 Luxembourg              70.6\n15 Costa Rica              70.5\n16 Ireland                 70.4\n\n\n\n# life expectancy in 2022\n# top 10th percentile\nlife_expectancy_long |&gt; \n  filter(year == 2022) |&gt; \n  slice_max(order_by = life_exp, prop = 0.1)\n\n# A tibble: 19 √ó 3\n   country          year  life_exp\n   &lt;chr&gt;            &lt;chr&gt;    &lt;dbl&gt;\n 1 Singapore        2022      85.3\n 2 Japan            2022      85.2\n 3 Hong Kong, China 2022      84.8\n 4 Iceland          2022      84.5\n 5 Switzerland      2022      84.4\n 6 Spain            2022      83.5\n 7 Israel           2022      83.5\n 8 Italy            2022      83.5\n 9 Luxembourg       2022      83.4\n10 Norway           2022      83.4\n11 Australia        2022      83.3\n12 France           2022      83.3\n13 South Korea      2022      83.3\n14 Sweden           2022      83.3\n15 Malta            2022      83  \n16 Austria          2022      82.6\n17 Canada           2022      82.6\n18 Ireland          2022      82.5\n19 Finland          2022      82.4\n\n# bottom 10th percentile\n# top 10th percentile\nlife_expectancy_long |&gt; \n  filter(year == 2022) |&gt; \n  slice_max(order_by = -life_exp, prop = 0.1)\n\n# A tibble: 19 √ó 3\n   country                  year  life_exp\n   &lt;chr&gt;                    &lt;chr&gt;    &lt;dbl&gt;\n 1 Lesotho                  2022      53  \n 2 Central African Republic 2022      53.4\n 3 Eswatini                 2022      59.3\n 4 Somalia                  2022      59.4\n 5 Solomon Islands          2022      59.5\n 6 Mozambique               2022      59.6\n 7 Chad                     2022      61.3\n 8 Kiribati                 2022      61.4\n 9 Zimbabwe                 2022      61.7\n10 Guinea-Bissau            2022      61.9\n11 Guinea                   2022      62.2\n12 Botswana                 2022      62.6\n13 Burkina Faso             2022      62.9\n14 Mali                     2022      63  \n15 Sierra Leone             2022      63  \n16 Niger                    2022      63.6\n17 Zambia                   2022      64  \n18 Afghanistan              2022      64.3\n19 Cameroon                 2022      64.3\n\n\nWhich country has had their happiness index increase the most from 2012 to 2022? Which dropped the most?\n\nhappiness |&gt; \n  mutate(change_2022_2012 = `2022` - `2012`) |&gt; \n  select(country, change_2022_2012) |&gt; \n  arrange(desc(change_2022_2012))\n\n# A tibble: 163 √ó 2\n   country     change_2022_2012\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Guinea                  14.2\n 2 Honduras                14.2\n 3 Romania                 14.2\n 4 Hungary                 13.6\n 5 Congo, Rep.             13.5\n 6 Bulgaria                12.5\n 7 Benin                   11.8\n 8 Senegal                 11.8\n 9 Bahrain                 11.4\n10 Nepal                   11.3\n# ‚Ñπ 153 more rows\n\nhappiness |&gt; \n  mutate(change_2022_2012 = `2022` - `2012`) |&gt; \n  select(country, change_2022_2012) |&gt; \n  arrange(change_2022_2012)\n\n# A tibble: 163 √ó 2\n   country          change_2022_2012\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Lebanon                     -21.8\n 2 Afghanistan                 -19.2\n 3 Venezuela                   -18.6\n 4 Zimbabwe                    -17.5\n 5 Congo, Dem. Rep.            -14.3\n 6 Botswana                    -14  \n 7 Zambia                      -10.3\n 8 Jordan                      -10.1\n 9 Mexico                       -9.9\n10 Malawi                       -7.8\n# ‚Ñπ 153 more rows"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#joining-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#joining-data",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Joining data",
    "text": "Joining data\nTry joining the happiness and life_expectancy datasets together and use the different *_join() functions so you can see how they differ. Check their dimensions and look at them. Think about how you might want to do different joins in different situations.\n\nleft_joined &lt;- \n  left_join(x = life_expectancy, y = happiness, by = \"country\") \n\nright_joined &lt;- \n  right_join(x = life_expectancy, y = happiness, by = \"country\")\n\ninner_joined &lt;- \n  inner_join(x = life_expectancy, y = happiness, by = \"country\")\n\nfull_joined &lt;- \n  full_join(x = life_expectancy, y = happiness, by = \"country\")\n\n\ndim(left_joined)\n\n[1] 195 320\n\ndim(right_joined)\n\n[1] 163 320\n\ndim(inner_joined)\n\n[1] 163 320\n\ndim(full_joined)\n\n[1] 195 320\n\n\nIf you wanted to create a plot that allowed you to see the correlation between happiness score and life expectancy in 2022, which joined dataset would you use and why?\n\n# with wide data\nfor_correlation_wide &lt;-\n  inner_join(x = life_expectancy |&gt; select(country, `2022`), \n             y = happiness |&gt; select(country, `2022`), \n             by = \"country\") |&gt; \n  rename(life_expectancy_2022 = `2022.x`) |&gt; \n  rename(happy_value_2022 = `2022.y`)\n\n# with long data\nlife_expectancy_2005_2022 &lt;- life_expectancy |&gt; \n  select(country, `2005`:`2022`) |&gt; \n  pivot_longer(cols = `2005`:`2022`,\n               names_to = \"year\",\n               values_to = \"life_expectancy\")\n\nfor_correlation_long &lt;- \n  inner_join(x = life_expectancy_2005_2022, y = happiness_long,\n             by = c(\"country\", \"year\"))\n\nI am not expecting you to be able to make a plot but I wanted to just give you a sense of the kinds of things you‚Äôll be learning in class.\n\nextremes &lt;- for_correlation_wide |&gt; \n  filter(life_expectancy_2022 &gt; 85 | happy_value_2022 &lt; 38)\n\nfor_correlation_wide |&gt; \n  ggplot(aes(x = life_expectancy_2022, y = happy_value_2022)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggrepel::geom_label_repel(data = extremes,\n                            aes(x = life_expectancy_2022, y = happy_value_2022, \n                                label = country),\n                            size = 3) +\n  theme_minimal() +\n  labs(x = \"Life expectancy, 2022\",\n       y = \"Happiness index, 2022\",\n       title = \"Relationship between life expectancy and happiness index\",\n       caption = \"Data from Gapminder\")"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "",
    "text": "Nacho (Jess‚Äôs dog, left) along with his friends Petunia (middle) and Inu (right) waiting for dinner"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Introduction",
    "text": "Introduction\nWe will practice what we learned this week in ggplot102 on:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data. Today we will be looking again at some different data from the Tidy Tuesday project (here is the Github repo) about dog breeds.\n\ninstall.packages(\"tidytuesdayR\")\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nWe will be using the data that is from February 1, 2022, so let‚Äôs download it. The readme for this data is here.\n\ntuesdata &lt;- ???\n\nLet‚Äôs look at it. How can you do that?"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigating",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigating",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Investigating",
    "text": "Investigating\nWrite code to determine what the 5 most popular dog breeds in 2020 were. There are many ways to do this.\n\n\n\n\n\n\nNeed a hint about how to do this? (Click to expand)\n\n\n\n\n\nCreate a new variable that is a sum of all the ranks from 2013, allowing a composite score of the popularity of each dog breed across this time period.\n\n\n\nWhat are the 5 most popular and the 5 least popular dogs across this time frame? There are many ways to do this."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-1",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-1",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Visualization 1",
    "text": "Visualization 1\nCreate a plot where you take the 12 most popular dogs from 2020, and plot their popularity rank from 2013 to 2020.\n\n\n\n\n\n\nNeed a hint about how to do this? (Click to expand)\n\n\n\n\n\nTo facet, you need to have the variable you want to facet in one column."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-2",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-2",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Visualization 2",
    "text": "Visualization 2\nAlter the aesthetics of this plot until you think it looks good."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigate-more",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigate-more",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Investigate more",
    "text": "Investigate more\nWhat dog has jumped in the rankings most from 2013 to 2020? What has dropped the most?"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "",
    "text": "Nacho (Jess‚Äôs dog, left) along with his friends Petunia (middle) and Inu (right) waiting for dinner"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Introduction",
    "text": "Introduction\nWe will practice what we learned this week in ggplot102 on:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data. Today we will be looking again at some different data from the Tidy Tuesday project (here is the Github repo) about dog breeds.\n\ninstall.packages(\"tidytuesdayR\")\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nWe will be using the data that is from February 1, 2022, so let‚Äôs download it. The readme for this data is here.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-02-01')\n\n--- Compiling #TidyTuesday Information for 2022-02-01 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `breed_traits.csv`\n    Downloading file 2 of 3: `trait_description.csv`\n    Downloading file 3 of 3: `breed_rank.csv`\n\n\n--- Download complete ---\n\n\n\n\nLet‚Äôs look at it\ntuesdata is a list of 3 dataframes breed_traits, trait_description and breed_rank.\n\nglimpse(tuesdata)\n\nList of 3\n $ breed_traits     : spc_tbl_ [195 √ó 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Breed                     : chr [1:195] \"Retrievers¬†(Labrador)\" \"French¬†Bulldogs\" \"German¬†Shepherd¬†Dogs\" \"Retrievers¬†(Golden)\" ...\n  ..$ Affectionate With Family  : num [1:195] 5 5 5 5 4 5 3 5 5 5 ...\n  ..$ Good With Young Children  : num [1:195] 5 5 5 5 3 5 5 3 5 3 ...\n  ..$ Good With Other Dogs      : num [1:195] 5 4 3 5 3 3 5 3 4 4 ...\n  ..$ Shedding Level            : num [1:195] 4 3 4 4 3 1 3 3 3 2 ...\n  ..$ Coat Grooming Frequency   : num [1:195] 2 1 2 2 3 4 2 1 2 2 ...\n  ..$ Drooling Level            : num [1:195] 2 3 2 2 3 1 1 3 2 2 ...\n  ..$ Coat Type                 : chr [1:195] \"Double\" \"Smooth\" \"Double\" \"Double\" ...\n  ..$ Coat Length               : chr [1:195] \"Short\" \"Short\" \"Medium\" \"Medium\" ...\n  ..$ Openness To Strangers     : num [1:195] 5 5 3 5 4 5 3 3 4 4 ...\n  ..$ Playfulness Level         : num [1:195] 5 5 4 4 4 5 4 4 4 4 ...\n  ..$ Watchdog/Protective Nature: num [1:195] 3 3 5 3 3 5 2 5 4 4 ...\n  ..$ Adaptability Level        : num [1:195] 5 5 5 5 3 4 4 4 4 4 ...\n  ..$ Trainability Level        : num [1:195] 5 4 5 5 4 5 3 5 5 4 ...\n  ..$ Energy Level              : num [1:195] 5 3 5 3 3 4 4 3 5 3 ...\n  ..$ Barking Level             : num [1:195] 3 1 3 1 2 4 4 1 3 5 ...\n  ..$ Mental Stimulation Needs  : num [1:195] 4 3 5 4 3 5 4 5 5 3 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Breed = col_character(),\n  .. ..   `Affectionate With Family` = col_double(),\n  .. ..   `Good With Young Children` = col_double(),\n  .. ..   `Good With Other Dogs` = col_double(),\n  .. ..   `Shedding Level` = col_double(),\n  .. ..   `Coat Grooming Frequency` = col_double(),\n  .. ..   `Drooling Level` = col_double(),\n  .. ..   `Coat Type` = col_character(),\n  .. ..   `Coat Length` = col_character(),\n  .. ..   `Openness To Strangers` = col_double(),\n  .. ..   `Playfulness Level` = col_double(),\n  .. ..   `Watchdog/Protective Nature` = col_double(),\n  .. ..   `Adaptability Level` = col_double(),\n  .. ..   `Trainability Level` = col_double(),\n  .. ..   `Energy Level` = col_double(),\n  .. ..   `Barking Level` = col_double(),\n  .. ..   `Mental Stimulation Needs` = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ trait_description: spc_tbl_ [16 √ó 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Trait      : chr [1:16] \"Affectionate With Family\" \"Good With Young Children\" \"Good With Other Dogs\" \"Shedding Level\" ...\n  ..$ Trait_1    : chr [1:16] \"Independent\" \"Not Recommended\" \"Not Recommended\" \"No Shedding\" ...\n  ..$ Trait_5    : chr [1:16] \"Lovey-Dovey\" \"Good With Children\" \"Good With Other Dogs\" \"Hair Everywhere\" ...\n  ..$ Description: chr [1:16] \"How affectionate a breed is likely to be with family members, or other people he knows well. Some breeds can be\"| __truncated__ \"A breed's level of tolerance and patience with childrens' behavior, and overall family-friendly nature. Dogs sh\"| __truncated__ \"How generally friendly a breed is towards other dogs. Dogs should always be supervised for interactions and int\"| __truncated__ \"How much fur and hair you can expect the breed to leave behind. Breeds with high shedding will need to be brush\"| __truncated__ ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Trait = col_character(),\n  .. ..   Trait_1 = col_character(),\n  .. ..   Trait_5 = col_character(),\n  .. ..   Description = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ breed_rank       : spc_tbl_ [195 √ó 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Breed    : chr [1:195] \"Retrievers (Labrador)\" \"French Bulldogs\" \"German Shepherd Dogs\" \"Retrievers (Golden)\" ...\n  ..$ 2013 Rank: num [1:195] 1 11 2 3 5 8 4 9 13 10 ...\n  ..$ 2014 Rank: num [1:195] 1 9 2 3 4 7 5 10 12 11 ...\n  ..$ 2015 Rank: num [1:195] 1 6 2 3 4 8 5 9 11 13 ...\n  ..$ 2016 Rank: num [1:195] 1 6 2 3 4 7 5 8 11 13 ...\n  ..$ 2017 Rank: num [1:195] 1 4 2 3 5 7 6 8 10 13 ...\n  ..$ 2018 Rank: num [1:195] 1 4 2 3 5 7 6 8 9 12 ...\n  ..$ 2019 Rank: num [1:195] 1 4 2 3 5 6 7 8 9 11 ...\n  ..$ 2020 Rank: num [1:195] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ links    : chr [1:195] \"https://www.akc.org/dog-breeds/labrador-retriever/\" \"https://www.akc.org/dog-breeds/french-bulldog/\" \"https://www.akc.org/dog-breeds/german-shepherd-dog/\" \"https://www.akc.org/dog-breeds/golden-retriever/\" ...\n  ..$ Image    : chr [1:195] \"https://www.akc.org/wp-content/uploads/2017/11/Labrador-Retriever-illustration.jpg\" \"https://www.akc.org/wp-content/uploads/2017/11/French-Bulldog-Illo-2.jpg\" \"https://www.akc.org/wp-content/uploads/2017/11/German-Shepherd-Dog-Illo-2.jpg\" \"https://www.akc.org/wp-content/uploads/2017/11/Golden-Retriever-Illo-2.jpg\" ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Breed = col_character(),\n  .. ..   `2013 Rank` = col_double(),\n  .. ..   `2014 Rank` = col_double(),\n  .. ..   `2015 Rank` = col_double(),\n  .. ..   `2016 Rank` = col_double(),\n  .. ..   `2017 Rank` = col_double(),\n  .. ..   `2018 Rank` = col_double(),\n  .. ..   `2019 Rank` = col_double(),\n  .. ..   `2020 Rank` = col_double(),\n  .. ..   links = col_character(),\n  .. ..   Image = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n - attr(*, \".tt\")= 'tt' chr [1:3] \"breed_traits.csv\" \"trait_description.csv\" \"breed_rank.csv\"\n  ..- attr(*, \".files\")='data.frame':   3 obs. of  3 variables:\n  .. ..$ data_files: chr [1:3] \"breed_traits.csv\" \"trait_description.csv\" \"breed_rank.csv\"\n  .. ..$ data_type : chr [1:3] \"csv\" \"csv\" \"csv\"\n  .. ..$ delim     : chr [1:3] \",\" \",\" \",\"\n  ..- attr(*, \".readme\")=List of 2\n  .. ..$ node:&lt;externalptr&gt; \n  .. ..$ doc :&lt;externalptr&gt; \n  .. ..- attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n  ..- attr(*, \".date\")= Date[1:1], format: \"2022-02-01\"\n - attr(*, \"class\")= chr \"tt_data\"\n\n\nNote tuesdata is a list of 3 dataframes.\nWe will use the $ to see the different dataframes individually.\n\nglimpse(tuesdata$breed_traits)\n\nRows: 195\nColumns: 17\n$ Breed                        &lt;chr&gt; \"Retrievers¬†(Labrador)\", \"French¬†Bulldogs‚Ä¶\n$ `Affectionate With Family`   &lt;dbl&gt; 5, 5, 5, 5, 4, 5, 3, 5, 5, 5, 5, 3, 5, 4,‚Ä¶\n$ `Good With Young Children`   &lt;dbl&gt; 5, 5, 5, 5, 3, 5, 5, 3, 5, 3, 3, 5, 5, 5,‚Ä¶\n$ `Good With Other Dogs`       &lt;dbl&gt; 5, 4, 3, 5, 3, 3, 5, 3, 4, 4, 4, 3, 3, 3,‚Ä¶\n$ `Shedding Level`             &lt;dbl&gt; 4, 3, 4, 4, 3, 1, 3, 3, 3, 2, 4, 3, 1, 2,‚Ä¶\n$ `Coat Grooming Frequency`    &lt;dbl&gt; 2, 1, 2, 2, 3, 4, 2, 1, 2, 2, 2, 2, 5, 2,‚Ä¶\n$ `Drooling Level`             &lt;dbl&gt; 2, 3, 2, 2, 3, 1, 1, 3, 2, 2, 1, 1, 1, 3,‚Ä¶\n$ `Coat Type`                  &lt;chr&gt; \"Double\", \"Smooth\", \"Double\", \"Double\", \"‚Ä¶\n$ `Coat Length`                &lt;chr&gt; \"Short\", \"Short\", \"Medium\", \"Medium\", \"Sh‚Ä¶\n$ `Openness To Strangers`      &lt;dbl&gt; 5, 5, 3, 5, 4, 5, 3, 3, 4, 4, 4, 3, 5, 4,‚Ä¶\n$ `Playfulness Level`          &lt;dbl&gt; 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4,‚Ä¶\n$ `Watchdog/Protective Nature` &lt;dbl&gt; 3, 3, 5, 3, 3, 5, 2, 5, 4, 4, 5, 3, 5, 4,‚Ä¶\n$ `Adaptability Level`         &lt;dbl&gt; 5, 5, 5, 5, 3, 4, 4, 4, 4, 4, 4, 3, 5, 3,‚Ä¶\n$ `Trainability Level`         &lt;dbl&gt; 5, 4, 5, 5, 4, 5, 3, 5, 5, 4, 4, 5, 4, 4,‚Ä¶\n$ `Energy Level`               &lt;dbl&gt; 5, 3, 5, 3, 3, 4, 4, 3, 5, 3, 4, 5, 4, 4,‚Ä¶\n$ `Barking Level`              &lt;dbl&gt; 3, 1, 3, 1, 2, 4, 4, 1, 3, 5, 4, 3, 4, 3,‚Ä¶\n$ `Mental Stimulation Needs`   &lt;dbl&gt; 4, 3, 5, 4, 3, 5, 4, 5, 5, 3, 4, 5, 4, 4,‚Ä¶\n\n\n\nglimpse(tuesdata$trait_description)\n\nRows: 16\nColumns: 4\n$ Trait       &lt;chr&gt; \"Affectionate With Family\", \"Good With Young Children\", \"G‚Ä¶\n$ Trait_1     &lt;chr&gt; \"Independent\", \"Not Recommended\", \"Not Recommended\", \"No S‚Ä¶\n$ Trait_5     &lt;chr&gt; \"Lovey-Dovey\", \"Good With Children\", \"Good With Other Dogs‚Ä¶\n$ Description &lt;chr&gt; \"How affectionate a breed is likely to be with family memb‚Ä¶\n\n\n\nglimpse(tuesdata$breed_rank)\n\nRows: 195\nColumns: 11\n$ Breed       &lt;chr&gt; \"Retrievers (Labrador)\", \"French Bulldogs\", \"German Shephe‚Ä¶\n$ `2013 Rank` &lt;dbl&gt; 1, 11, 2, 3, 5, 8, 4, 9, 13, 10, 24, 20, 6, 7, 16, 14, 18,‚Ä¶\n$ `2014 Rank` &lt;dbl&gt; 1, 9, 2, 3, 4, 7, 5, 10, 12, 11, 22, 18, 6, 8, 15, 13, 19,‚Ä¶\n$ `2015 Rank` &lt;dbl&gt; 1, 6, 2, 3, 4, 8, 5, 9, 11, 13, 20, 17, 7, 10, 15, 12, 18,‚Ä¶\n$ `2016 Rank` &lt;dbl&gt; 1, 6, 2, 3, 4, 7, 5, 8, 11, 13, 18, 16, 9, 10, 14, 12, 19,‚Ä¶\n$ `2017 Rank` &lt;dbl&gt; 1, 4, 2, 3, 5, 7, 6, 8, 10, 13, 15, 17, 9, 11, 14, 12, 19,‚Ä¶\n$ `2018 Rank` &lt;dbl&gt; 1, 4, 2, 3, 5, 7, 6, 8, 9, 12, 13, 15, 10, 11, 16, 14, 18,‚Ä¶\n$ `2019 Rank` &lt;dbl&gt; 1, 4, 2, 3, 5, 6, 7, 8, 9, 11, 10, 13, 12, 14, 17, 15, 16,‚Ä¶\n$ `2020 Rank` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,‚Ä¶\n$ links       &lt;chr&gt; \"https://www.akc.org/dog-breeds/labrador-retriever/\", \"htt‚Ä¶\n$ Image       &lt;chr&gt; \"https://www.akc.org/wp-content/uploads/2017/11/Labrador-R‚Ä¶"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-2020",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-2020",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Top 5 breeds 2020",
    "text": "Top 5 breeds 2020\nWrite code to determine what the 5 most popular dog breeds in 2020 were. There are many ways to do this.\n\nclean_names()\nBecause each of these datasets has some non-conventional column names, I am going to run janitor::clean_names() on each df, and save them in my environment so I won‚Äôt have to keep using the dollar sign $ accessor.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nbreed_traits &lt;- clean_names(tuesdata$breed_traits)\ntrait_description &lt;- clean_names(tuesdata$trait_description)\nbreed_rank &lt;- clean_names(tuesdata$breed_rank)\n\nWhat are the most popular breeds in 2020, three ways.\n\nbreed_rank %&gt;%\n  filter(x2020_rank &lt;= 5)\n\n# A tibble: 5 √ó 11\n  breed        x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank x2018_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers ‚Ä¶          1          1          1          1          1          1\n2 French Bull‚Ä¶         11          9          6          6          4          4\n3 German Shep‚Ä¶          2          2          2          2          2          2\n4 Retrievers ‚Ä¶          3          3          3          3          3          3\n5 Bulldogs              5          4          4          4          5          5\n# ‚Ñπ 4 more variables: x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;, links &lt;chr&gt;,\n#   image &lt;chr&gt;\n\nbreed_rank %&gt;%\n  arrange(x2020_rank) %&gt;%\n  slice(1:5)\n\n# A tibble: 5 √ó 11\n  breed        x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank x2018_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers ‚Ä¶          1          1          1          1          1          1\n2 French Bull‚Ä¶         11          9          6          6          4          4\n3 German Shep‚Ä¶          2          2          2          2          2          2\n4 Retrievers ‚Ä¶          3          3          3          3          3          3\n5 Bulldogs              5          4          4          4          5          5\n# ‚Ñπ 4 more variables: x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;, links &lt;chr&gt;,\n#   image &lt;chr&gt;\n\nbreed_rank %&gt;%\n  slice_min(x2020_rank, n = 5) # min because a low rank is \"high\"\n\n# A tibble: 5 √ó 11\n  breed        x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank x2018_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers ‚Ä¶          1          1          1          1          1          1\n2 French Bull‚Ä¶         11          9          6          6          4          4\n3 German Shep‚Ä¶          2          2          2          2          2          2\n4 Retrievers ‚Ä¶          3          3          3          3          3          3\n5 Bulldogs              5          4          4          4          5          5\n# ‚Ñπ 4 more variables: x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;, links &lt;chr&gt;,\n#   image &lt;chr&gt;"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-across-2013-2020",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-across-2013-2020",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Top 5 breeds across 2013-2020",
    "text": "Top 5 breeds across 2013-2020\nWhat are the 5 most popular and the 5 least popular dogs across this time frame? There are many ways to do this. Hint: create a new variable that is a sum of all the ranks from 2013, allowing a composite score of the popularity of each dog breed across this time period.\n\n# most popular\nbreed_rank %&gt;%\n  rowwise() %&gt;%\n  mutate(rank_sum = sum(across(x2013_rank:x2020_rank))) %&gt;%\n  ungroup() %&gt;%\n  select(breed, rank_sum, everything()) %&gt;%\n  slice_min(n = 5, order_by = rank_sum) %&gt;% # min because here lower sum is more popular\n  knitr::kable() # makes a nice formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreed\nrank_sum\nx2013_rank\nx2014_rank\nx2015_rank\nx2016_rank\nx2017_rank\nx2018_rank\nx2019_rank\nx2020_rank\nlinks\nimage\n\n\n\n\nRetrievers (Labrador)\n8\n1\n1\n1\n1\n1\n1\n1\n1\nhttps://www.akc.org/dog-breeds/labrador-retriever/\nhttps://www.akc.org/wp-content/uploads/2017/11/Labrador-Retriever-illustration.jpg\n\n\nGerman Shepherd Dogs\n17\n2\n2\n2\n2\n2\n2\n2\n3\nhttps://www.akc.org/dog-breeds/german-shepherd-dog/\nhttps://www.akc.org/wp-content/uploads/2017/11/German-Shepherd-Dog-Illo-2.jpg\n\n\nRetrievers (Golden)\n25\n3\n3\n3\n3\n3\n3\n3\n4\nhttps://www.akc.org/dog-breeds/golden-retriever/\nhttps://www.akc.org/wp-content/uploads/2017/11/Golden-Retriever-Illo-2.jpg\n\n\nBulldogs\n37\n5\n4\n4\n4\n5\n5\n5\n5\nhttps://www.akc.org/dog-breeds/bulldog/\nhttps://www.akc.org/wp-content/uploads/2017/11/Bulldog-Illo-2.jpg\n\n\nBeagles\n45\n4\n5\n5\n5\n6\n6\n7\n7\nhttps://www.akc.org/dog-breeds/beagle/\nhttps://www.akc.org/wp-content/uploads/2017/11/Beagle-Illo-2.jpg\n\n\n\n\n# least popular            \nbreed_rank %&gt;%\n  rowwise() %&gt;%\n  mutate(rank_sum = sum(c_across(x2013_rank:x2020_rank))) %&gt;%\n  ungroup() %&gt;%\n  select(breed, rank_sum, everything()) %&gt;%\n  arrange(rank_sum) %&gt;%\n  slice_max(n = 5, order_by = rank_sum) %&gt;% # max bc here higher sum is less popular\n  knitr::kable() # makes a nice formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreed\nrank_sum\nx2013_rank\nx2014_rank\nx2015_rank\nx2016_rank\nx2017_rank\nx2018_rank\nx2019_rank\nx2020_rank\nlinks\nimage\n\n\n\n\nNorwegian Lundehunds\n1499\n175\n184\n182\n188\n190\n191\n194\n195\nhttps://www.akc.org/dog-breeds/norwegian-lundehund/\nhttps://www.akc.org/wp-content/uploads/2017/11/Norwegian-Lundehund-Illo-2.jpg\n\n\nEnglish Foxhounds\n1497\n177\n183\n184\n187\n189\n188\n195\n194\nhttps://www.akc.org/dog-breeds/english-foxhound/\nhttps://www.akc.org/wp-content/uploads/2017/11/English-Foxhound-Illo-2.jpg\n\n\nAmerican Foxhounds\n1482\n176\n180\n181\n189\n187\n186\n191\n192\nhttps://www.akc.org/dog-breeds/american-foxhound/\nhttps://www.akc.org/wp-content/uploads/2017/11/American-Foxhound-Illo-2.jpg\n\n\nHarriers\n1473\n173\n181\n183\n186\n183\n189\n188\n190\nhttps://www.akc.org/dog-breeds/harrier/\nhttps://www.akc.org/wp-content/uploads/2017/11/Harrier-Illo-2.jpg\n\n\nCesky Terriers\n1468\n174\n182\n179\n182\n185\n185\n190\n191\nhttps://www.akc.org/dog-breeds/cesky-terrier/\nhttps://www.akc.org/wp-content/uploads/2017/11/Cesky-Terrier-Illo-2.jpg\n\n\n\n\n\nAnother way to do it still with wide data.\n\n# create a vector with the rowSums for the ranks from 2013 to 2020\nbreed_rank_2013to2020 &lt;- breed_rank %&gt;%\n  select(x2013_rank:x2020_rank) %&gt;%\n  rowSums() %&gt;% # calculate the sum across the rows\n  as.data.frame() %&gt;% # convert from vector to dataframe\n  rename(rank_sum = 1) # rename the first and only column to rank_sum \n\n# bind back to the rest of the data\nbreed_rank_sum &lt;- bind_cols(breed_rank, breed_rank_2013to2020) %&gt;%\n  select(breed, rank_sum, everything()) # move breed and rank_sum to front\n\n# most popular\nbreed_rank_sum %&gt;%\n  arrange(rank_sum) %&gt;%\n  slice_min(n = 5, order_by = rank_sum)\n\n# A tibble: 5 √ó 12\n  breed          rank_sum x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers (L‚Ä¶        8          1          1          1          1          1\n2 German Shephe‚Ä¶       17          2          2          2          2          2\n3 Retrievers (G‚Ä¶       25          3          3          3          3          3\n4 Bulldogs             37          5          4          4          4          5\n5 Beagles              45          4          5          5          5          6\n# ‚Ñπ 5 more variables: x2018_rank &lt;dbl&gt;, x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;,\n#   links &lt;chr&gt;, image &lt;chr&gt;\n\n# least popular\nbreed_rank_sum %&gt;%\n  arrange(rank_sum) %&gt;%\n  slice_max(n = 5, order_by = rank_sum)\n\n# A tibble: 5 √ó 12\n  breed          rank_sum x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Norwegian Lun‚Ä¶     1499        175        184        182        188        190\n2 English Foxho‚Ä¶     1497        177        183        184        187        189\n3 American Foxh‚Ä¶     1482        176        180        181        189        187\n4 Harriers           1473        173        181        183        186        183\n5 Cesky Terriers     1468        174        182        179        182        185\n# ‚Ñπ 5 more variables: x2018_rank &lt;dbl&gt;, x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;,\n#   links &lt;chr&gt;, image &lt;chr&gt;\n\n\nA third way with long data.\n\n# create long data cleaned\nbreed_rank_sum_long &lt;- breed_rank %&gt;%\n  pivot_longer(cols = x2013_rank:x2020_rank, names_to = \"year\", values_to = \"rank\") %&gt;%\n  separate(col = year, sep = \"_\", into = c(\"year\", \"extra\")) %&gt;% # separate year column parts\n  mutate(year = str_remove(string = year, pattern = \"x\")) %&gt;% # remove the \"x\"\n  select(breed, year, rank)\n\nhead(breed_rank_sum_long)\n\n# A tibble: 6 √ó 3\n  breed                 year   rank\n  &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt;\n1 Retrievers (Labrador) 2013      1\n2 Retrievers (Labrador) 2014      1\n3 Retrievers (Labrador) 2015      1\n4 Retrievers (Labrador) 2016      1\n5 Retrievers (Labrador) 2017      1\n6 Retrievers (Labrador) 2018      1\n\n# most popular\nbreed_rank_sum_long %&gt;%\n  group_by(breed) %&gt;% # do operation by breed\n  summarize(rank_sum = sum(rank)) %&gt;% # add all ranks\n  arrange(rank_sum) %&gt;% # actually not necessary\n  slice_min(order_by = rank_sum, n = 5)\n\n# A tibble: 5 √ó 2\n  breed                 rank_sum\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Retrievers (Labrador)        8\n2 German Shepherd Dogs        17\n3 Retrievers (Golden)         25\n4 Bulldogs                    37\n5 Beagles                     45\n\n# least popular\nbreed_rank_sum_long %&gt;%\n  group_by(breed) %&gt;% # do operation by breed\n  summarize(rank_sum = sum(rank)) %&gt;% # add all ranks\n  arrange(desc(rank_sum)) %&gt;% # actually not necessary\n  slice_max(order_by = rank_sum, n = 5)\n\n# A tibble: 5 √ó 2\n  breed                rank_sum\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Norwegian Lundehunds     1499\n2 English Foxhounds        1497\n3 American Foxhounds       1482\n4 Harriers                 1473\n5 Cesky Terriers           1468"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#popularity-ranking-2013-2020",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#popularity-ranking-2013-2020",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Popularity ranking 2013-2020",
    "text": "Popularity ranking 2013-2020\nCreate a plot where you take the 12 most popular dogs from 2020, and plot their popularity rank from 2013 to 2020. Hint, to facet, you need to have the variable you want to facet in one column.\nI‚Äôm showing you a slightly different way to clean up your long dataframe here.\n\n# go from wide to long data or \"tidy\" data\nbreed_rank_tidy &lt;- breed_rank %&gt;%\n  pivot_longer(cols = x2013_rank:x2020_rank,\n              names_to = \"year\",\n              values_to = \"rank\")\n\n# remove extra character in year\nbreed_rank_tidy$year &lt;- breed_rank_tidy$year %&gt;%\n  str_remove(pattern = \"x\") %&gt;%\n  str_remove(pattern = \"_rank\")\n\n\n# what dogs should I include?\ndogs_to_include &lt;- breed_rank_tidy %&gt;%\n  filter(year == 2020 & rank &lt;= 12) \n\n# plot\nbreed_rank_tidy %&gt;%\n  filter(breed %in% dogs_to_include$breed) %&gt;%\n  ggplot(aes(x = year, y = rank)) +\n    geom_point() +\n    facet_wrap(vars(breed), \n               labeller = labeller(breed = label_wrap_gen(20))) +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\nAlter the aesthetics of this plot until you think it looks good.\n\nbreed_rank_tidy %&gt;%\n  filter(breed %in% dogs_to_include$breed) %&gt;%\n  ggplot(aes(x = year, y = rank, group = breed)) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(vars(fct_reorder(breed, rank))) + # orders facets by mean rank so more popular first\n    scale_y_reverse(breaks = seq(1, 25, 4)) + # reverse so that lower number rank is at the top and set the labels to start at 1, increment by 4 until 25 since a zero rank doesn't exist\n    theme_minimal() + \n    theme(axis.text.x = element_text(angle = 90), # make x-axis labels on 90degree angle\n          strip.text = element_text(size = 8), # change strip text size\n          panel.grid.major.x = element_blank()) + # remove some grid lines\n    labs(x = \"Year\",\n         y = \"Popularity rank among all AKC dogs\",\n         title = \"Popularity of 12 Most Popular AKC Dog Breeds in 2020 from 2013-2020\",\n         subtitle = \"Labrador retrievers are so popular!\")\n\n\n\n\nWhat dog has jumped in the rankings most from 2013 to 2020? What has dropped the most?\n\nbreed_rank %&gt;%\n  mutate(rank_inc = (x2020_rank - x2013_rank)) %&gt;%\n  select(breed, rank_inc, everything()) %&gt;%\n  slice_min(n = 1, order_by = rank_inc) %&gt;%\n  select(breed, rank_inc, x2013_rank, x2020_rank)\n\n# A tibble: 1 √ó 4\n  breed             rank_inc x2013_rank x2020_rank\n  &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Spaniels (Boykin)      -34        121         87\n\nbreed_rank %&gt;%\n  mutate(rank_inc = (x2020_rank - x2013_rank)) %&gt;%\n  select(breed, rank_inc, everything()) %&gt;%\n  slice_max(n = 1, order_by = rank_inc) %&gt;%\n  select(breed, rank_inc, x2013_rank, x2020_rank)\n\n# A tibble: 1 √ó 4\n  breed                     rank_inc x2013_rank x2020_rank\n  &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Treeing Walker Coonhounds       52        101        153"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "We are going to practice using ggplot today, focusing on the data, aesthetic, and geom layers. We are going to use data from the TidyTuesday project. For this recitation, we are going to use the Giant Pumpkins data which is collected from the Great Pumpkin Commonwealth.\nAt the end of of this module you will create of of this descriptive plots\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n\n\n\npumpkins_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-19/pumpkins.csv')\n\nRows: 28065 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (14): id, place, weight_lbs, grower_name, city, state_prov, country, gpc...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#libraries",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#libraries",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#read-in-data",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#read-in-data",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "pumpkins_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-19/pumpkins.csv')\n\nRows: 28065 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (14): id, place, weight_lbs, grower_name, city, state_prov, country, gpc...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html",
    "title": "R Markdown for Reproducible Research",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#setting-future-you-up-for-success",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#setting-future-you-up-for-success",
    "title": "R Markdown for Reproducible Research",
    "section": "Setting future you up for success",
    "text": "Setting future you up for success\nHow often do you conduct some kind of data analysis, get some results, ignore the project for 6 months, then return back to your data and realize you can‚Äôt figure out exactly what you did?\nThis does not need to happen to you. Be kind to your future self and take steps to avoid this avoidable problem.\nJust like a lab notebook helps you document all of the steps you take in your wet lab work, R Markdown can function like a lab notebook for all your data analyses."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#what-is-r-markdown",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#what-is-r-markdown",
    "title": "R Markdown for Reproducible Research",
    "section": "What is R Markdown?",
    "text": "What is R Markdown?\nRMarkdown provides a framework for saving and executing code, and sharing your results. R Markdown files have the file format .Rmd.\n\n\n\nWhat is R Markdown? from RStudio, Inc. on Vimeo.\n\n\nA minute long introduction to R Markdown\n\nYou can do so many things in R Markdown, from making reports that include text, code, code annotations, figures, tables etc., to creating this course website!\nIf you‚Äôve never used R Markdown before, you can download it using the chunk below. Unlike other R packages, you don‚Äôt need to use library(rmarkdown) to load the package each time you want to use R Markdown.\n\ninstall.packages(\"rmarkdown\")"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#why-i-love-r-markdown",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#why-i-love-r-markdown",
    "title": "R Markdown for Reproducible Research",
    "section": "Why I love R Markdown",
    "text": "Why I love R Markdown\nBasically everything I do in R uses R Markdown. I really value to ability to easily add text and annotate code so that future me, my team, or collaborators can understand what I‚Äôve done and why. I try to write my code in such a way that it could be read by anyone, and is ready to be pushed to our lab‚Äôs Github repositories to act as supplementary materials for our publications. It helps others to be able to truly see what we‚Äôve done, and I think makes science more reproducible and open.\n\n\n\n\n\nFigure from Allison Horst"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#open-an-r-markdown-document",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#open-an-r-markdown-document",
    "title": "R Markdown for Reproducible Research",
    "section": "Open an R Markdown document",
    "text": "Open an R Markdown document\nOpen up RStudio, go to File &gt; New File &gt; R Markdown. Change the name of the title to something meaningful to you, mine will be called ‚ÄúTrying R Markdown‚Äù.\n\n\n\n\n\n\n\n\n\n\nSaving our file\nWe gave our file a title, but if you look at the top left corner of our new document, you‚Äôll see it‚Äôs called ‚ÄúUntitled1‚Äù. Let‚Äôs change the name to something easier for our future selves to recognize.\n\n\n\n\n\n\n\n\n\nYou can go to File &gt; Save as and place this new R Markdown with your other course materials, and save it with a meaningful name.\n\nAlways having issues with setting your working directory? R Markdown solves this problem! The default working directory is the location of the saved R Markdown file. Voila!\n\n\n\nAn example\nYou‚Äôll note when you create your template document, it is not blank. So you get a sense of what these documents will look like when they are ‚Äúrun,‚Äù let‚Äôs do that with the template doc.\nIn the taskbar of your R Markdown document you will see a button called Knit in your task bar (there is a little ball of yarn with knitting needles next to it). If you click it, R will run all of the code in your R Markdown file, and default compile it to a .html file (though you can select to compile to other file formats).\n\n\n\n\n\n\n\n\n\nLet‚Äôs compare what our document looks like when viewing it in RStudio (left), and after it is knitted (right).\n\n\n\n\n\n\n\n\n\nIf you have a little bit of R experience, you can begin to see how (some of) the content on the left related to the knitted document on the right. We see text, code chunks (but not all of them), and the output of code.\nNow that we have seen a template R Markdown and have 10,000 foot view as to what it is, we can start going through what the different pieces of the document are."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#yaml-header",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#yaml-header",
    "title": "R Markdown for Reproducible Research",
    "section": "YAML Header",
    "text": "YAML Header\nThe YAML (Yet Another Markdown Language, or YAML ain‚Äôt markup language, if you want to learn more about this name and its origins, you can read about it at this stack overflow post) is at the top of your document and is surrounded by ---.\nThe YAML is where you can set the content that will show up on the top of your knitted document.\n\ntitle: ‚ÄúYour title but put it in quotes‚Äù\nauthor: ‚ÄúThe author and still in quotes‚Äù\ndate: the date you want at the top of your doc in quotes. If you want this to be today‚Äôs date (whatever that is) you can use ‚ÄúSys.Date()‚Äù\noutput: will indicate the format of your compiled document. I would recommend for this class you use html_document as it is the richest format. Your output will be a .html file, which you can save or share.\n\nHere‚Äôs a simple example.\n\n---\ntitle: \"This is my descriptive title\"\nauthor: \"Jess\"\ndate: \"May 10, 2022\"\noutput: html_document\n---\n\nIn the YAML, you can also set options that govern how your document will be compiled within output. For example, you can add a table of contents, make that toc float, add a theme, number your sections, and add a button that allows someone to click and access your .Rmd from your knitted .html file. This last one is especially nice because it allows you to send one viewable document, and if someone wants to edit it, they can download and do so easily.\nHere‚Äôs an example of what a more customized YAML could look like.\n\n---\ntitle: \"This is my descriptive title\"\nauthor: \"Jess\"\ndate: \"May 10, 2022\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    number_sections: true\n    theme: flatly\n    code_download: true\n---\n\nBe sure you pay attention to the indents (which are 2 or 4 spaces, and not tabs), as the YAML is picky here. If your indents are not correct, you will get an error when you knit. (Pro tip: you can set in RStudio to insert spaces when you click tab by going to Preferences &gt; Code &gt; Use spaces for tab (and indicate 2).)\nAbove are just some of the options that I like to put in my YAML, but there are tons more. Additional output options that are explained on the second page of the RStudio R Markdown cheatsheet."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#text",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#text",
    "title": "R Markdown for Reproducible Research",
    "section": "Text",
    "text": "Text\nUnlike an R script (.R), where R by default interprets anything as code (and material that isn‚Äôt code needed to be commented out by using #), in an R Markdown, the default is text (and code exists only within code chunks or backticks).\nThe text portion of the document is written in a language called Markdown (which is why this format is called R Markdown). The philosophy of Markdown is that it is easy to both write and read. If you want to learn more about markup languages I‚Äôd recommend the this brief explanation by Michael Broe from a past Code Club Session and the Markup language wikipedia page.\nIf we look back to our template R Markdown, we can see there is text written in the same way that we would write in Word document, or an email, and we recognize immediately as text (i.e., the sentence at line 24).\n\n\n\n\n\n\n\n\n\nBut we can also see markup that is perhaps not immediately, recognizable, for example, the **Knit** on line 16. In this case, two asterisks around a word will make it compile to be bolded (second paragraph in the right photo).\nBelow I‚Äôm compiling some commonly used markdown syntax.\n\n\n\n\n\nFigure from R Markdown Reference Guide\n\n\n\n\nNote, the headers are useful and will indicate the levels in your table of contents. You want to use them, and make them meaningful for your document.\nYou can use Markdown to insert tables, images, mathematical formulas, block quotes, and almost anything else you‚Äôd like. You can even write your papers and dissertation in R Markdown. The old version of this course website is made with distill and R Markdown. My lab website is made with R Markdown and the hugo Aper√≥ theme. Quarto and .qmd documents are a slightly updated version of RMarkdown with some new functionality (here you can find some discussion about the different between the two). Here are some links where you can find lots of other Markdown syntactical information:\n\nMarkdown Guide\nR Markdown reference guide\nR Studio R Markdown Cheatsheet\njust google for what you want\n\nSo how is this useful for this course and making your own data analyses more reproducible? You can embed text along with your code, where you provide introductory information, your rationale for data analysis decision making, links and more information about interpreting your code and its output, provide context as to your results, and anything else that would aid your data‚Äôs interpretation."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#code",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#code",
    "title": "R Markdown for Reproducible Research",
    "section": "Code",
    "text": "Code\nCode chunks are the parts of your R Markdown document where code lives. You can insert a new code chunk by:\n\nusing the keyboard shortcut Cmd + Option + I (Mac) or Ctrl + Alt + I (Windows)\ntyping ```{r} and ``` (and your code goes in between)\nusing the Add Chunk command in the editor toolbar and select R\n\nCode chunks look like this:\n\n\n\n\n\n\n\n\n\nThe code goes in the empty line, and there can be more than 1 bit of code per chunk.\n\nthe gear allows you to modify the chunk options (we are going to talk more about this)\nthe triangle with the line below it runs all code chunks that come previous to this chunk\nthe play button runs the current chunk\n\nYou can still add comments within a code chunk, but you need to comment them out using #.\n# here is my in chunk annotation\nsome_function()\nWhen you knit your R Markdown, this process will run all of the code in your document. This means if you have code that throws errors or doesn‚Äôt work, your document will not knit. This is some of why I am asking you to knit for your final assignments - all your code needs to work!\nYou can also embed code inline (i.e., within your text).\n\n\n\n\n\n\n\nRaw\nRendered\n\n\n\n\nThere are `r 365*24` hours in a year\nThere are 8760 hours in a year\n\n\nThere are `r nrow(cars)` observations (i.e.¬†rows) in the cars dataset\nThere are 50 observations (i.e.¬†rows) in the cars dataset\n\n\n\nThink about how you could use this ‚Äì embed information from your data analysis (e.g, p-values) within your narrative text without having to hard-code/type it in manually.\n\nAdding options to your code chunks\nYou add options to your code chunks between the {}. This gives R additional instructions regarding running your code and compiling your document. Here are some common examples:\n\necho = FALSE runs your code chunk, displays output, but does not display code in your final doc (this is useful if you want to show a figure but not the code used to create it)\neval = FALSE does not run your code, but does display it in your final doc\ninclude = FALSE runs your code but does not display the code or its output in your final doc\nmessage = FALSE prevents messages from showing up in your final doc\nwarning = FALSE prevents earnings from showing up in your final doc\nfig.height = X and fig.width = Y will allow you to specify the dimensions of your figures (in inches)\nfig.align = can be set to ‚Äúleft‚Äù, ‚Äúright‚Äù, or ‚Äúcenter‚Äù\nfig.cap = \"Your figure caption\" will allow you to set a figure caption\nfig.alt = \"Your alt text\" will allow you to set alt text for screen readers\ncache = TRUE will cache results, meaning if you have a chunk that takes a long time to run, if you haven‚Äôt changed anything and you knit again, the code won‚Äôt run again but access the cache."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#introductions",
    "href": "modules/module1/01_principles/01_principles.html#introductions",
    "title": "1 - Principles of Data Visualization",
    "section": "Introductions üëã",
    "text": "Introductions üëã\n\n\nName\nProgram\nWhy you decided to take this class\nOne thing you hope to learn"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#teaching-team",
    "href": "modules/module1/01_principles/01_principles.html#teaching-team",
    "title": "1 - Principles of Data Visualization",
    "section": "Teaching Team",
    "text": "Teaching Team\nInstructor: Jessica Cooperstone\n‚úâÔ∏è cooperstone.1@osu.edu\n\n\nTA: Daniel Quiroz Moreno\n‚úâÔ∏è quirozmoreno.1@osu.edu\n\n\nOffice hours: go.osu.edu/dataviz-times"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#website",
    "href": "modules/module1/01_principles/01_principles.html#website",
    "title": "1 - Principles of Data Visualization",
    "section": "Website",
    "text": "Website\nIf you have found these slides, you‚Äôve made it to the website! (Good job.)\n\n\nAll course materials will be posted to, or linked to from datavisualizing.netlify.app"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#syllabus",
    "href": "modules/module1/01_principles/01_principles.html#syllabus",
    "title": "1 - Principles of Data Visualization",
    "section": "Syllabus",
    "text": "Syllabus\n\nA full version of the syllabus can be found on Carmen\nA trimmed version of the syllabus can be found on our course site"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#attendance",
    "href": "modules/module1/01_principles/01_principles.html#attendance",
    "title": "1 - Principles of Data Visualization",
    "section": "Attendance",
    "text": "Attendance\n\n\n\nClass will taught in a hybrid, synchronous manner, meaning I expect you to attend class during class time. This attendance can happen in person, or virtually via Zoom I have found that students who attend in person are more engaged, and tend to master material more quickly. But, it is up to you how you want to attend.\n\nI will record class time for those who want to 1) revisit material or 2) can‚Äôt attend (this should be uncommon). These recordings are not to replace coming to class."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#how-class-will-be",
    "href": "modules/module1/01_principles/01_principles.html#how-class-will-be",
    "title": "1 - Principles of Data Visualization",
    "section": "How class will be?",
    "text": "How class will be?\n\nA combination of lecture, code run-throughs, live coding, and hands-on exercises.\nBring a laptop (not tablet) to class with R and RStudio downloaded\nCome with your questions!\nEngage as much as you can!"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#previous-programming-experience",
    "href": "modules/module1/01_principles/01_principles.html#previous-programming-experience",
    "title": "1 - Principles of Data Visualization",
    "section": "Previous programming experience",
    "text": "Previous programming experience"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#assigments",
    "href": "modules/module1/01_principles/01_principles.html#assigments",
    "title": "1 - Principles of Data Visualization",
    "section": "Assigments",
    "text": "Assigments\n\nModule assignments: After each module, there will be an assignment to provide practice for the techniques learned in class.\nClass reflections: After 10 of the 15 weeks, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nCapstone assignment: At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#late-assignments",
    "href": "modules/module1/01_principles/01_principles.html#late-assignments",
    "title": "1 - Principles of Data Visualization",
    "section": "Late assignments",
    "text": "Late assignments\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule",
    "href": "modules/module1/01_principles/01_principles.html#schedule",
    "title": "1 - Principles of Data Visualization",
    "section": "üóì Schedule",
    "text": "üóì Schedule\nThis is our tentative class schedule - but subject to change depending on our pacing, and your interests!"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-1",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-1",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 1)",
    "text": "üóìÔ∏è Schedule (part 1)\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2023-08-22\n1: Principles\nPrinciples of data visualization\n\n\n2023-08-29\n1: Principles\nGood and bad visualizations"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-2",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-2",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 2)",
    "text": "üóìÔ∏è Schedule (part 2)\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2023-09-05\n2: Coding fundamentals\nR Markdown for reproducible research\n\n\n2023-09-12\n2: Coding fundamentals\nWrangling, the basics\n\n\n2023-09-19\n2: Coding fundamentals\nggplot 101\n\n\n2023-09-26\n2: Coding fundamentals\nThemes, labels, facets (ggplot 102)"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-3",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-3",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 3)",
    "text": "üóìÔ∏è Schedule (part 3)\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2023-10-03\n3: Data exploration\nData distributions\n\n\n2023-10-10\n3: Data exploration\nCorrelations\n\n\n2023-10-17\nOpen session, capstone prep\nOpen session, capstone prep\n\n\n2023-10-24\n3: Data exploration\nAnnotating statistics"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-4",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-4",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 4)",
    "text": "üóìÔ∏è Schedule (part 4)\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2023-10-31\n4: Putting it together\nPrincipal components analysis\n\n\n2023-11-07\n4: Putting it together\nManhattan plots and making lots of plots at once\n\n\n2023-11-14\n4: Putting it together\nInteractive plots\n\n\n2023-11-21\nNo class, Thanksgiving\nRelaxing and eating\n\n\n2023-11-28\n4: Putting it together\nggplot extension packages and complexheatmap\n\n\n2023-12-05\n4: Putting it together\nCapstone assignment open session"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#there-may-be-a-data-dinosaur",
    "href": "modules/module1/01_principles/01_principles.html#there-may-be-a-data-dinosaur",
    "title": "1 - Principles of Data Visualization",
    "section": "There may be a data dinosaur ü¶ñ",
    "text": "There may be a data dinosaur ü¶ñ\n\nFigure by Alberto Cairo"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-understand-distribution",
    "href": "modules/module1/01_principles/01_principles.html#to-understand-distribution",
    "title": "1 - Principles of Data Visualization",
    "section": "To understand distribution",
    "text": "To understand distribution\nAnscombe‚Äôs quartet üéª"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-discover-data-secrets",
    "href": "modules/module1/01_principles/01_principles.html#to-discover-data-secrets",
    "title": "1 - Principles of Data Visualization",
    "section": "To discover data secrets",
    "text": "To discover data secrets\n\nFigures from Justin Matejka and George Fitzmaurice"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-convey-our-message",
    "href": "modules/module1/01_principles/01_principles.html#to-convey-our-message",
    "title": "1 - Principles of Data Visualization",
    "section": "To convey our message",
    "text": "To convey our message\n\nBilbrey et al., New Phytologist, 2021"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#the-data-visualization-process",
    "href": "modules/module1/01_principles/01_principles.html#the-data-visualization-process",
    "title": "1 - Principles of Data Visualization",
    "section": "The data visualization process",
    "text": "The data visualization process\n\n\nFigure adapted from one by Rick Scavetta"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability",
    "href": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability",
    "title": "1 - Principles of Data Visualization",
    "section": "Simple changes improve interpretability",
    "text": "Simple changes improve interpretability"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability-1",
    "href": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Simple changes improve interpretability",
    "text": "Simple changes improve interpretability"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues",
    "href": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues",
    "title": "1 - Principles of Data Visualization",
    "section": "Encoding data with easy-to-process visual clues",
    "text": "Encoding data with easy-to-process visual clues\nLength is easier to see than angles or areas."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues-1",
    "href": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Encoding data with easy-to-process visual clues",
    "text": "Encoding data with easy-to-process visual clues\nLength is easier to see than angles or areas."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#color-scales-should-be-intuitive-and-accessible",
    "href": "modules/module1/01_principles/01_principles.html#color-scales-should-be-intuitive-and-accessible",
    "title": "1 - Principles of Data Visualization",
    "section": "Color scales should be intuitive and accessible",
    "text": "Color scales should be intuitive and accessible\n\n\nThese are not."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-1",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-2",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-2",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-1",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-2",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-2",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti",
    "href": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti",
    "title": "1 - Principles of Data Visualization",
    "section": "Avoid figure spaghetti üçù",
    "text": "Avoid figure spaghetti üçù"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti-1",
    "href": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Avoid figure spaghetti üçù",
    "text": "Avoid figure spaghetti üçù"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#be-consistent-among-figures",
    "href": "modules/module1/01_principles/01_principles.html#be-consistent-among-figures",
    "title": "1 - Principles of Data Visualization",
    "section": "Be consistent among figures",
    "text": "Be consistent among figures\n\nUse the same color schemes/shapes across figures\nIf you‚Äôre ordering/grouping, do so in the same manner"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#make-sure-your-plot-has-a-clear-message",
    "href": "modules/module1/01_principles/01_principles.html#make-sure-your-plot-has-a-clear-message",
    "title": "1 - Principles of Data Visualization",
    "section": "Make sure your plot has a clear message üçï",
    "text": "Make sure your plot has a clear message üçï"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#marie-kondo-your-plots",
    "href": "modules/module1/01_principles/01_principles.html#marie-kondo-your-plots",
    "title": "1 - Principles of Data Visualization",
    "section": "Marie Kondo your plots",
    "text": "Marie Kondo your plots\nDeclutter, and keep only parts that are informative (and spark joy) üòª\n\nFrom https://socviz.co/lookatdata.html"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#oral-presentation-and-publication-figures-might-not-be-the-same",
    "href": "modules/module1/01_principles/01_principles.html#oral-presentation-and-publication-figures-might-not-be-the-same",
    "title": "1 - Principles of Data Visualization",
    "section": "Oral presentation and publication figures might not be the same",
    "text": "Oral presentation and publication figures might not be the same"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#what-should-you-think-about-when-making-visualizations",
    "href": "modules/module1/01_principles/01_principles.html#what-should-you-think-about-when-making-visualizations",
    "title": "1 - Principles of Data Visualization",
    "section": "What should you think about when making visualizations?",
    "text": "What should you think about when making visualizations?\n\nWho are you talking to? üì¢\nWhat are you trying to convey? üìù\nHow can you fairly represent your data? üöØ\n\n\n\n01 Principles, ¬© Jessica Cooperstone, 2023"
  }
]