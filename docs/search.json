[
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#what-was-easier-to-find",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#what-was-easier-to-find",
    "title": "2 - Good and Bad Data Visualizations",
    "section": "What was easier to find?",
    "text": "What was easier to find?\n\nGood üëè visualizations\nBad üò° visualizations"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd_recitation.html",
    "href": "modules/module2/03_rmarkdown/03_Rmd_recitation.html",
    "title": "R Markdown for Reproducible Research Recitation",
    "section": "",
    "text": "Today you will be playing around with modifying the following within R Markdown:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd_recitation.html#r-markdown-recitation",
    "href": "modules/module2/03_rmarkdown/03_Rmd_recitation.html#r-markdown-recitation",
    "title": "R Markdown for Reproducible Research Recitation",
    "section": "",
    "text": "Today you will be playing around with modifying the following within R Markdown:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "We are going to practice using ggplot2 today, focusing wrangling data, mapping variables to aesthetics, and adding geoms.\nWe are going to use data from the TidyTuesday project. For this recitation, we are going to use the Giant Pumpkins data which is collected from the Great Pumpkin Commonwealth. You can learn more about how the data is structured here.\nToday, you are going to make this plot:\n\n\n\nOur plot for today\n\n\n\nQuestion: How can we replicate this plot?\n\n\n\nWork with real messy data\n\nImport data from github\nModify variables types\nSelect observations with certain values\nWrangle some more\nPractice plotting\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\n\n\nWhen you open the github page you will see a file called pumpkins.csv. You also are introduced about the details of the data (i.e., variables, variable types, descriptions), as well as how to import the it.\nFirst thing first, we are going to import the data by reading the csv file with the Github link provided. You can also read the data in by downloading it manually, saving it, and then loading it.\n# load libraries\nlibrary(tidyverse)\n\n# Import giant pumpkins data\npumpkins_raw &lt;- readr::read_csv('WHAT-GOES-HERE??')\nOnce we have imported our data, how can you check it out?\n\nglimpse(pumpkins_raw)\n\nRows: 28,065\nColumns: 14\n$ id                &lt;chr&gt; \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2‚Ä¶\n$ place             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"‚Ä¶\n$ weight_lbs        &lt;chr&gt; \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1‚Ä¶\n$ grower_name       &lt;chr&gt; \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen‚Ä¶\n$ city              &lt;chr&gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo‚Ä¶\n$ state_prov        &lt;chr&gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc‚Ä¶\n$ country           &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"‚Ä¶\n$ gpc_site          &lt;chr&gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum‚Ä¶\n$ seed_mother       &lt;chr&gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar‚Ä¶\n$ pollinator_father &lt;chr&gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open‚Ä¶\n$ ott               &lt;chr&gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", ‚Ä¶\n$ est_weight        &lt;chr&gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141‚Ä¶\n$ pct_chart         &lt;chr&gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0‚Ä¶\n$ variety           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nDo some of these variables contain more than one piece of information?\n\nWhat is embedded within the variable id?\nWhat type of info does id contain?\nWhat types of variables are place and weight_lbs? Are there any limitations to plotting these variable types?"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#goals-of-this-recitation",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#goals-of-this-recitation",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "Work with real messy data\n\nImport data from github\nModify variables types\nSelect observations with certain values\nWrangle some more\nPractice plotting\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\n\n\nWhen you open the github page you will see a file called pumpkins.csv. You also are introduced about the details of the data (i.e., variables, variable types, descriptions), as well as how to import the it.\nFirst thing first, we are going to import the data by reading the csv file with the Github link provided. You can also read the data in by downloading it manually, saving it, and then loading it.\n# load libraries\nlibrary(tidyverse)\n\n# Import giant pumpkins data\npumpkins_raw &lt;- readr::read_csv('WHAT-GOES-HERE??')\nOnce we have imported our data, how can you check it out?\n\nglimpse(pumpkins_raw)\n\nRows: 28,065\nColumns: 14\n$ id                &lt;chr&gt; \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2‚Ä¶\n$ place             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"‚Ä¶\n$ weight_lbs        &lt;chr&gt; \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1‚Ä¶\n$ grower_name       &lt;chr&gt; \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen‚Ä¶\n$ city              &lt;chr&gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo‚Ä¶\n$ state_prov        &lt;chr&gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc‚Ä¶\n$ country           &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"‚Ä¶\n$ gpc_site          &lt;chr&gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum‚Ä¶\n$ seed_mother       &lt;chr&gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar‚Ä¶\n$ pollinator_father &lt;chr&gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open‚Ä¶\n$ ott               &lt;chr&gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", ‚Ä¶\n$ est_weight        &lt;chr&gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141‚Ä¶\n$ pct_chart         &lt;chr&gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0‚Ä¶\n$ variety           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nDo some of these variables contain more than one piece of information?\n\nWhat is embedded within the variable id?\nWhat type of info does id contain?\nWhat types of variables are place and weight_lbs? Are there any limitations to plotting these variable types?"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#turn-one-character-column-into-two",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#turn-one-character-column-into-two",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Turn one character column into two ‚úÇÔ∏è",
    "text": "Turn one character column into two ‚úÇÔ∏è\nFrom both looking at the data, and reading about the variable id on the documentation page, you can see that it contains two pieces of information. To be able to interact with them separately, we need to separate this column into two columns (i.e.¬†year and type).\nTry doing this with the function separate() from the tidyr package. Or you can use separate_wider_delim().\npumpkins_raw %&gt;%\n   separate(WHAT-GOES-HERE)"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#select-observations-by-their-values",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#select-observations-by-their-values",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Select observations by their values üéÉ",
    "text": "Select observations by their values üéÉ\nNow that you separated the year and crop type, keep only the data for Giant Pumpkins.\n\n\nNeed a hint?\n\nHint, you can use the filter() function from the dplyr package.\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\npumpkins_raw %&gt;%\n   filter(...predicate/condition...) \nNow that you hae kept only the Giant Pumpkins, retain only the observations that were the winners (i.e.¬†those in first place).\npumpkins_raw %&gt;%\n   filter(...predicate/condition...) %&gt;%\n   filter(...predicate/condition...)"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#remove-pesky-strings",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#remove-pesky-strings",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Remove pesky strings üòë",
    "text": "Remove pesky strings üòë\nIf we were to try and plot our data as it is now we would not get our desired outcome. But try it anyway.\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  code-to-plot\nWhat is weird about this y-axis?\n\n\n\n\n\nIf you take a look at the variables of the weight_lbs column, it contains a commas as the thousand separator. R does not recognize this as a number (and instead views it as a character) so and it has to be removed prior changing the column type. There are a few ways to do this.\nLet‚Äôs practice handling strings by removing the comma. You can use str_remove() function from the tidyverse package stringr. Here is an example of how str_remove() works.\n\nwrong_number &lt;- \"700,057.58\"\nwrong_number\n\n[1] \"700,057.58\"\n\n\nUsing str_remove()\n\nstringr::str_remove(string = wrong_number, pattern = \",\")\n\n[1] \"700057.58\"\n\n\nRemember, we don‚Äôt want to just remove the thousands place comma in one number, we want to edit the dataset to remove the comma.\nIn this case, you can embed str_remove() within the mutate() function, which can create new variables or modify existing ones. In our case, we want to modify the weight_lbs variable to remove the comma. Give it a try.\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) \nCommas, gone! üëèüëèüëè"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#convert-character-to-numeric",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#convert-character-to-numeric",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Convert character to numeric üî¢",
    "text": "Convert character to numeric üî¢\nNow the comma is gone, you can simply change the variable weight_lbs from a character to numeric, so it can be plotted like a number., to change the column type, we are going to use the as.numeric() function. Here‚Äôs some example about how to use as.numeric().\n\nright_number_chr &lt;- stringr::str_remove(string = wrong_number, pattern = \",\")\n\nright_number_number &lt;- as.numeric(right_number_chr)\nclass(right_number_number)\n\n[1] \"numeric\"\n\n\nLet‚Äôs add this to our growing pipe.\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) %&gt;%\n  mutate(variable = as.numeric(arguments-here))"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#where-are-the-lines",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#where-are-the-lines",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Where are the lines?",
    "text": "Where are the lines?\nWhy do you think the lines aren‚Äôt showing up?\n\n\nNeed a hint?\n\nHint - look at what variable type year is.\n\nHow can you fix this? Hint, you can change year to either numeric or a date. Try using some functions from the package lubridate or the function as.Date().\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) %&gt;%\n  mutate(variable = as.numeric(arguments-here)) %&gt;%\n  mutate(do-something-with-your-date) %&gt;%\n  code-to-plot"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#playing-around",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#playing-around",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Playing around",
    "text": "Playing around\nTry using different geoms besides geom_point() and geom_line(). Which might make sense in this situation?\nCan you color all the lines blue?\nCan you color the data based on year?\nCan you change color and change shape based on country?\nCan you make a plot showing the distribution of weights of all giant pumpkins entered in 2021?\nCan you make a boxplot showing the distribution of weights of all giant pumpkins across all years? Also can you add all the datapoints on top of the boxplot? Is this a good idea? Might there be a better geom to use than a boxplot?"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Introduction",
    "text": "Introduction\nWe will will build upon our last lesson on ggplot101 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, and geoms. Today we will focus on some of the other more commonly adjusted layers:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data.\n\nlibrary(tidyverse)\nlibrary(gardenR)\n\nAnd let‚Äôs remember whats in garden_harvest.\n\nglimpse(garden_harvest)\n\nRows: 781\nColumns: 5\n$ vegetable &lt;chr&gt; \"lettuce\", \"radish\", \"lettuce\", \"lettuce\", \"radish\", \"lettuc‚Ä¶\n$ variety   &lt;chr&gt; \"reseed\", \"Garden Party Mix\", \"reseed\", \"reseed\", \"Garden Pa‚Ä¶\n$ date      &lt;date&gt; 2020-06-06, 2020-06-06, 2020-06-08, 2020-06-09, 2020-06-11,‚Ä¶\n$ weight    &lt;dbl&gt; 20, 36, 15, 10, 67, 12, 9, 8, 53, 19, 14, 10, 48, 58, 8, 121‚Ä¶\n$ units     &lt;chr&gt; \"grams\", \"grams\", \"grams\", \"grams\", \"grams\", \"grams\", \"grams‚Ä¶"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#facets",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#facets",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Facets",
    "text": "Facets\nFaceting allows to create small multiples of plots, enabling the easy comparison across the entirety of your data. A benefit of plots like this is they are all structured the same way, so once you understand one, you can begin to look at trends across groups/treatments/conditions simply and easily.\nHere is a more infographic example of using small multiples.\n\n\n\n\n\nFigure from Five Thirty Eight\n\n\n\n\nSo we can easily see that states with more of a maroon color have a lower than average life expectancy, while those that are higher than average are orange. We also can see easily where each state is on the map, so we can begin to understand how geography is related to life expectancy. We can also see which states have gotten better (i.e.¬†their people live longer) with time, and those that haven‚Äôt. And this is all with a quick glance!\nIf we look back to the plot we were using as our example last week, can see how we have a plot faceted by tomato variety.\nFirst lets select only the data for tomatoes.\n\n# filter data to include only tomatoes \n# filter() is a useful function from dplyr (part of tidyverse)\n# it allows us to select observations based on their values\ngarden_harvest_tomato &lt;- garden_harvest %&gt;%\n  filter(vegetable == \"tomatoes\")\n\nLet‚Äôs remember what our base plot is currently looking like.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) \n\n\n\n\nSee how crowded this is? I think faceting might help us better see our data by variety.\nThere are two functions that allow you to facet:\n\nfacet_wrap: allows to lay out your facets in a wrapped type. You can use facet_wrap if you have 1 variable you‚Äôd like to facet on.\nfacet_grid: allows you to lay out your facets in a grid. You can use facet_grid if you have 1 or 2 variables you‚Äôd like to facet on.\n\nThere are a few different sets of syntax that work for faceting, but I think this is the most intuitive.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety))\n\n\n\n\nWe will get a very reasonably different looking plot with facet_grid with the default settings.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_grid(vars(variety))\n\n\n\n\nNote because you have provided only one variable, ggplot has put that facet in one row.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_grid(cols = vars(variety))\n\n\n\n\nWe can make the faceting go by column, but this is also looks bad in this case\nHowever, you might be thinking now that if you have two variables, and you want to facet by the combination of them, you could do that with facet_grid. Here is an example with the mpg dataset from the tidyverse (since there isn‚Äôt really good data to demonstrate this from garden_harevst).\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy)) + # city and highway gas mileage\n  geom_point() +\n  facet_grid(cols = vars(class), # category of car\n             rows = vars(drv)) # type of drive train, 4 wheel, front, rear\n\n\n\n\nThe default in both facet_wrap and facet_grid are for the x and y-axis to be fixed and constant among all the plots. This is often what you want to take advance of the comparisons between small multiples, but this is something you can change if you want. You can adjust the scales within facets to:\n\nscales = \"fixed\": both the x- and y-axes are fixed for all plots to be the same (this is the default)\nscales = \"free\": both the x- and y-axes are set per plot\nscales = \"free_x\": the x-axis scales are free between plots\nscales = \"free_y\": the y-axis scales are free between plots\n\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety), scales = \"free\")\n\n\n\n\nDo note how this affects how easy it is to compare among the facets now. Also note that in this case, since we have all the same x-axis labels between the plots, when we set scales = \"free\" it really only changes the y, making it functionally equivalent to scales = \"free_y\". This will not hold true in other situations."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#scales",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#scales",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Scales",
    "text": "Scales\nUsing scales allows you to control how the data are linked to the visual properties of your plot. Some books will include labels as a part of scales but I‚Äôm going to cover them separately.\nScales allow you to pick colors, shapes, alphas, lines, transformations (e.g.¬†scaling your axes to a log scale), and others. You can also use scales to set the limits of your plots.\nScales functions start with scale_.\nHere are some common things you might do with the scale_ functions.\n\nPosition scales\nYou can set position scales for dates/times (like we have here), x and y data, binned data, continuous data, and for discrete data.\nHere is one example for date/time data, which is what we have here. The date-time POSIX standards are listed here.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_x_date(date_labels = \"%m/%y\")\n\n\n\n\nHere is another example (which isn‚Äôt very good) about how you can also use scales to log transform your axes. Remember you are not actually transforming your data, you are just transforming the axis labels.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_y_log10()\n\n\n\n\n\n\nColor scales\nYou can set color scales for continuous and binned colour data, sequential, diverging and qualitative data using ColorBrewer, and perceptually uniform scales using viridis from viridisLite\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_color_brewer(palette = \"Set3\")\n\n\n\n\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_color_viridis_d()\n\n\n\n\nYou can play around with scales to see all you can do with it."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#labels",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#labels",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Labels",
    "text": "Labels\nHaving good labels helps your reader (and you, when you come back to the plot in the future) understand what its all about.\nIn the labs() function, you can indicate:\n\nx for the x-axis label\ny for the y-axis label\ntitle for a title\nsubtitle for a subtitle underneath your title\ncaption for a caption\n\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) + \n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\",\n       alt = \"A plot showing 12 varieties of tomatoes and how much of each of them Dr. Lisa Lendway harvested in her home garden in 2022. The biggest producers were amish paste and better boy, which had earlier season peaks, and mortgage lifter, old german, and volunteer plants were more productive towards the end of the season.\")\n\n\n\n\nYou can also use get_alt_text() to pull the alt-text for an image. This will come back with an empty string if there is no alt-text provided.\nIn theme() you can change characteristics of these labels like their size, fonts, justification, etc."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#themes",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#themes",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Themes",
    "text": "Themes\nThemes will control all the non-data parts of your plot. There are some pre-set ‚Äúcomplete‚Äù themes that you can recognize as they‚Äôll be called theme_*(), and you can adjust any theme parameters by setting parameters within theme(). There are probably 50 parameters you can set within theme() and they include text size, axis label orientation, the presence of a legend, and many others.\n\nComplete themes from ggplot\nThere are some pre-set complete themes that control the look of the non-data displays. Below are some examples. theme_grey() is the default ggplot2 theme.\n\ntheme_minimal()\nThis is the one I use the most.\n\nbase_plot &lt;- garden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) + \n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\nbase_plot + theme_minimal()\n\n\n\n\n\n\ntheme_classic()\nThis is another nice lightweight theme.\n\nbase_plot + theme_classic()\n\n\n\n\n\n\ntheme_bw()\nIn black and white.\n\nbase_plot + theme_bw()\n\n\n\n\n\n\ntheme_dark()\nFor dark-mode aficionados.\n\nbase_plot + theme_dark()\n\n\n\n\n\n\ntheme_void()\nHere is a theme with very little if you really want only the bare bones.\n\nbase_plot + theme_void()\n\n\n\n\n\n\n\nComplete themes from other packages\nThe packages ggthemes and hrbrthemes have some nice themes you might be interested in.\n\nlibrary(ggthemes)\nlibrary(hrbrthemes)\n\n\ntheme_tufte()\nAnother lightweight theme\n\nbase_plot + theme_tufte()\n\n\n\n\n\n\ntheme_excel()\nIn case you find yourself wishing your plots looked more Excel 2005.\n\nbase_plot + theme_excel()\n\n\n\n\n\n\ntheme_ipsum()\nYou need to have Roboto Condensed for this.\n\nbase_plot + theme_ipsum()\n\n\n\n\n\n\n\nModify components of a theme\nIf there is a part of the non-data components of your plot you want to change, chances are you do this using theme(). You can also start with a complete theme and then modify from there. This is what I do most of the time.\nThere are more than 40 unique theme elements that can be modified to control the appearance of a plot.\nYou can find the complete list of theme elements in the ggplot2 documentation. Let‚Äôs play around a little bit.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\n\n\n\nThe legend here is duplicative, let‚Äôs remove it.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\n\n\n\nWhat if we wanted to make the strip text background black, and the strip text white?\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        strip.text = element_text(color = \"white\"),\n        strip.background = element_rect(fill = \"black\"))\n\n\n\n\nRemember that ggplot works on layers and these layers are added in the order you indicate. That means if you write something code that negates or edits something that comes above, the lower code will prevail.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(color = \"white\"),\n        strip.background = element_rect(fill = \"black\")) +\n  theme_classic() \n\n\n\n\n\n\nSetting an active theme\nIf you know you want to use one theme for all your plots, you can set all the parameters for that theme using theme_set() and theme_update() and then your theme will carry for all the plots you make going forward.\n\nmy_theme &lt;- theme_set(theme_classic())\n\nbase_plot"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#saving-your-plots",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#saving-your-plots",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Saving your plots",
    "text": "Saving your plots\nYou probably won‚Äôt want to save all your plots, but you definitely will want to save some of them. The function ggsave() makes this each. I like to save images as .svg as these are vectorized and have unlimited resolution. You could also adjust the file extension to save it in the format you like.\n\nggsave(plot = base_plot,\n       filename = \"img/my_plot.svg\",\n       width = 9,\n       height = 6)\n\nI like to set the code chunk options for my chunks where I am saving plots to eval = FALSE this way I don‚Äôt accidentally save over figures I don‚Äôt intend to. If I want to save the plot, I can do so manually."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#useful-resources",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#useful-resources",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Useful resources",
    "text": "Useful resources\n\nggplot2 cheatsheet\nggplot2 documentation\nggplot2: elegant graphics for data analysis by Hadley Wickham\nA really compehensive list of resources compiled by Erik Gahner Larsen\nPast ggplot Code Clubs:\n\nVisualizing Data by Michael Broe\nggplot round 2 by me\nFaceting, multi-plots, and animating\nVisualizing Data by Michael Broe a second one\nggplot round 2 a second one by me"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html",
    "href": "modules/module2/04_wrangling/04_wrangling.html",
    "title": "Wrangling your data ü§†, the basics",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling.html#introduction",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Introduction",
    "text": "Introduction\nThis is a new lecture from the previous delivery of this course. In the last offering, I found that the process of wrangling data was by far the thing that people had the most trouble with. In recitations, and for module assignments, I would provide data in a way that would need some adjustment before visualization can be made - and if I‚Äôm being honest, I heard a lot of rumblings about this.\nStill, I am going to leave in the course activities that required data to the wrangled before visualization. I am doing this because real data is mostly not structured precisely how it needs to be to make the visualizations you want. I want to provide you all some practice to get comfortable with using your data lassos. This is something you need to get comfortable with on your coding journey.\nBut, I have added in this extra lecture to explicitly go over what I think are the most useful wrangling functions and tools you can use in R. I hope this introduces you to some of what is possible with R, so it will trigger your memory later when you need to use it. You can also always come back to this page during the course."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#what-is-the-tidyverse",
    "href": "modules/module2/04_wrangling/04_wrangling.html#what-is-the-tidyverse",
    "title": "Wrangling your data ü§†, the basics",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\n‚ÄúThe tidyverse‚Äù is a collection of packages called that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier. We will be using mostly tidyverse functions in this class, with some base R syntax scattered throughout.\nThe ‚Äúcore tidyverse‚Äù contains the 8 packages below:\n\ndplyr: for data manipulation\nggplot2: a ‚Äúgrammar of graphics‚Äù for creating beautiful plots\nreadr: for reading in rectangular data (i.e., Excel-style formatting)\ntibble: using tibbles as modern/better dataframes\nstringr: handling strings (i.e., text or stuff in quotes)\nforcats: for handling categorical variables (i.e., factors) (meow!)\ntidyr: to make ‚Äútidy data‚Äù\npurrr: for enhancing functional programming (also meow!)\n\nWe will be using many of these other packages in this course, but will talk about them as we go. There are more tidyverse packages outside of these core eight, and we will talk about some of them another time.\n\ntl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are required, but I think you‚Äôll find many tidyverse approaches easier and more intuitive than using base R.\n\nYou can find here some examples of comparing tidyverse and base R syntax.\nToday we will be mostly talking through functions that live within the dplyr package."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#installing-ggplot-tidyverse",
    "href": "modules/module2/04_wrangling/04_wrangling.html#installing-ggplot-tidyverse",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Installing ggplot & tidyverse",
    "text": "Installing ggplot & tidyverse\nTo install packages in R that are on the Comprehensive R Archive Network (CRAN), you can use the function install.packages().\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\nWe only need to install packages once. But, every time we want to use them, we need to ‚Äúload‚Äù them, and can do this using the function library().\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIt‚Äôs a good habit to not ignore warnings/messages that R gives you.\n\ntl:dr install.packages() once, library() every time."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#loading-data",
    "href": "modules/module2/04_wrangling/04_wrangling.html#loading-data",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Loading data",
    "text": "Loading data\nIn class, we will use a combination of data embedded within R (or packages in R), from the internet, or data you import yourself. I am going to quickly go over ways to import common data types.\n\n.csv\nFiles saved as comma separated values are the most common data type I tend to import. The function read_csv() which is a part of the tidyverse package readr allows you to do this easily as it has a special function for this file type, as it is so common.\nMake sure that your file is within your working directory (or you have its relative or complete path), and you can install it (and save it) like this:\n\nsample_csv_data &lt;- read_csv(file = \"my-file-name.csv\")\n\n\n\n.xlsx\nThe second most common file type I import are those made in Excel. These files can either be converted to a .csv and then read in like we just went over, or you can load the package readxl and read files in directly. If you don‚Äôt already have readxl you can download it using install.packages().\n\nlibrary(readxl)\nsample_excel_data &lt;- read_excel(file = \"my-file-name.xlsx\",\n                                sheet = \"Sheet1\")\n\nHere you can find the readr cheatsheet."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#the-pipe",
    "href": "modules/module2/04_wrangling/04_wrangling.html#the-pipe",
    "title": "Wrangling your data ü§†, the basics",
    "section": "The pipe |>",
    "text": "The pipe |&gt;\nThe pipe |&gt; (which used to be written %&gt;%, and you will see this widely when googling/troubleshooting and sometimes see me default to this older syntax) is a tool that allows you to take the output of one function, and send it to the next function.\nYou can read the pipe as ‚Äúand then‚Äù - here is a theoretical example.\n\ntake_this_data |&gt;\n  then_this_function() |&gt;\n  then_another_function() |&gt; \n  finally_a_last_function()\n\nThe easiest way to see how the pipe works is with an example. We are going to use the dataset diamonds which comes pre-loaded when you load the tidyverse.\nWhat is in the dataset diamonds? We can get a ‚Äúglimpse‚Äù of it with the function glimpse, which is sort of like the tidyverse version of str().\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\n\nWhat if we want to see what is the average price of a diamond where cut = \"Premium\". There are a few ways we can do this.\n\n# one way\n# filter for only the premium diamonds\ndiamonds_premium &lt;- filter(diamonds, cut == \"Premium\")\n\n# calculate the mean using summarize\nsummarize(diamonds_premium, mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      4584.\n\n# or calculate mean using mean\n# the function mean() requires a vector\nmean(diamonds_premium$price)\n\n[1] 4584.258\n\n\nOr, we can use the pipe |&gt;. We are going to talk about summarize() in a minute.\n\ndiamonds |&gt;\n  filter(cut == \"Premium\") |&gt;\n  summarize(mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      4584.\n\n# if we want to use the function mean() we need to supply a vector\ndiamonds |&gt; \n  filter(cut == \"Premium\") |&gt;\n  pull(price) |&gt; # pulls out price as a vector\n  mean()\n\n[1] 4584.258\n\n\nSome reasons I like the pipe:\n\nits easier to read (and doesn‚Äôt have a lot of nested parentheses)\nit doesn‚Äôt require you to create lots of interim objects which you won‚Äôt use again\nits easy to troubleshoot\n\n\nThe keyboard shortcut for |&gt; is Ctrl/Cmd + Shift + M\n\nOf course you can assign the output of a pipe to something using the assignment operator &lt;- and then use it for other things.\nSsome functions are not ‚Äúpipe friendly‚Äù meaning they will not work using pipes. This is often because the data is not the first argument passed to the function. All tidyverse functions work with piping."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#selecting-columns-with-select",
    "href": "modules/module2/04_wrangling/04_wrangling.html#selecting-columns-with-select",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nOften you will want to pick only certain columns in your dataframe, and you can do this with the function select(). You can pick columns by:\n\ntheir names\ntheir position (i.e., index)\ncharacteristics of that column\n\nLet‚Äôs select first by name.\n\ndiamonds |&gt; \n  select(carat, cut, price)\n\n# A tibble: 53,940 √ó 3\n   carat cut       price\n   &lt;dbl&gt; &lt;ord&gt;     &lt;int&gt;\n 1  0.23 Ideal       326\n 2  0.21 Premium     326\n 3  0.23 Good        327\n 4  0.29 Premium     334\n 5  0.31 Good        335\n 6  0.24 Very Good   336\n 7  0.24 Very Good   336\n 8  0.26 Very Good   337\n 9  0.22 Fair        337\n10  0.23 Very Good   338\n# ‚Ñπ 53,930 more rows\n\n\nNote that when you use the pipe, the potential column names will autofill for you after you type 3 letters. You can also hit tab to scroll through all the potential objects to select.\nWe can also select by index. In general I would recommend against this because its really hard to remember which column indices are which variables today, nevermind returning back to old code 1 year from now.\n\ndiamonds |&gt; \n  select(c(1, 2, 7)) # you could also use the colon syntax if your columns are sequential\n\n# A tibble: 53,940 √ó 3\n   carat cut       price\n   &lt;dbl&gt; &lt;ord&gt;     &lt;int&gt;\n 1  0.23 Ideal       326\n 2  0.21 Premium     326\n 3  0.23 Good        327\n 4  0.29 Premium     334\n 5  0.31 Good        335\n 6  0.24 Very Good   336\n 7  0.24 Very Good   336\n 8  0.26 Very Good   337\n 9  0.22 Fair        337\n10  0.23 Very Good   338\n# ‚Ñπ 53,930 more rows\n\n\nYou can also select using selection helpers like:\n\neverything(): picks all variables\nstarts_with(): starts with some prefix\ncontains(): contains a specific string\nwhere(): selects columns where the statement given in the argument is TRUE\n\nHere is an example of using where() to select only the columns that are numeric.\n\ndiamonds |&gt; \n  select(where(is.numeric))\n\n# A tibble: 53,940 √ó 7\n   carat depth table price     x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23  61.5    55   326  3.95  3.98  2.43\n 2  0.21  59.8    61   326  3.89  3.84  2.31\n 3  0.23  56.9    65   327  4.05  4.07  2.31\n 4  0.29  62.4    58   334  4.2   4.23  2.63\n 5  0.31  63.3    58   335  4.34  4.35  2.75\n 6  0.24  62.8    57   336  3.94  3.96  2.48\n 7  0.24  62.3    57   336  3.95  3.98  2.47\n 8  0.26  61.9    55   337  4.07  4.11  2.53\n 9  0.22  65.1    61   337  3.87  3.78  2.49\n10  0.23  59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nYou can find more helpers here.\nUsing select() will also set the order of your columns. More about this later."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#choosing-observations-with-filter",
    "href": "modules/module2/04_wrangling/04_wrangling.html#choosing-observations-with-filter",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Choosing observations with filter()",
    "text": "Choosing observations with filter()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nSometimes you want to select observations (rows) based on values. To do this you use filter(). Try not to confuse this with select().\n\nselect() picks columns, while filter() picks rows.\n\nThe function filter() will keep only observations that meet your filtering criteria.\nLet‚Äôs say we want to only keep the diamonds that are bigger than 3 carats.\n\ndiamonds |&gt; \n  filter(carat &gt; 3)\n\n# A tibble: 32 √ó 10\n   carat cut     color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.01 Premium I     I1       62.7    58  8040  9.1   8.97  5.67\n 2  3.11 Fair    J     I1       65.9    57  9823  9.15  9.02  5.98\n 3  3.01 Premium F     I1       62.2    56  9925  9.24  9.13  5.73\n 4  3.05 Premium E     I1       60.9    58 10453  9.26  9.25  5.66\n 5  3.02 Fair    I     I1       65.2    56 10577  9.11  9.02  5.91\n 6  3.01 Fair    H     I1       56.1    62 10761  9.54  9.38  5.31\n 7  3.65 Fair    H     I1       67.1    53 11668  9.53  9.48  6.38\n 8  3.24 Premium H     I1       62.1    58 12300  9.44  9.4   5.85\n 9  3.22 Ideal   I     I1       62.6    55 12545  9.49  9.42  5.92\n10  3.5  Ideal   H     I1       62.8    57 12587  9.65  9.59  6.03\n# ‚Ñπ 22 more rows\n\n\nHere I made use of the greater than &gt; sign, and there are other operators you could also use to help you filter.\n\n==: equal to (I usually read this as exactly equal to, and is different than using an equal sign in an equation)\n&lt;, &gt;: less than or greater than\n&lt;=, &gt;=: less than or equal to, great than or equal to\n&: and\n|: or\n!: not equal\nis.na: is NA\n\nYou can also layer your filtering.\n\ndiamonds |&gt; \n  filter(carat &gt; 3 & cut == \"Premium\")\n\n# A tibble: 13 √ó 10\n   carat cut     color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.01 Premium I     I1       62.7    58  8040  9.1   8.97  5.67\n 2  3.01 Premium F     I1       62.2    56  9925  9.24  9.13  5.73\n 3  3.05 Premium E     I1       60.9    58 10453  9.26  9.25  5.66\n 4  3.24 Premium H     I1       62.1    58 12300  9.44  9.4   5.85\n 5  3.01 Premium G     SI2      59.8    58 14220  9.44  9.37  5.62\n 6  4.01 Premium I     I1       61      61 15223 10.1  10.1   6.17\n 7  4.01 Premium J     I1       62.5    62 15223 10.0   9.94  6.24\n 8  3.67 Premium I     I1       62.4    56 16193  9.86  9.81  6.13\n 9  3.01 Premium I     SI2      60.2    59 18242  9.36  9.31  5.62\n10  3.04 Premium I     SI2      59.3    60 18559  9.51  9.46  5.62\n11  3.51 Premium J     VS2      62.5    59 18701  9.66  9.63  6.03\n12  3.01 Premium J     SI2      60.7    59 18710  9.35  9.22  5.64\n13  3.01 Premium J     SI2      59.7    58 18710  9.41  9.32  5.59"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#make-new-columns-with-mutate",
    "href": "modules/module2/04_wrangling/04_wrangling.html#make-new-columns-with-mutate",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Make new columns with mutate()",
    "text": "Make new columns with mutate()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nSometimes you want to make new columns based on existing variables and you can do this with mutate().\nFor example, we might want to create a new column called ‚Äúprice_per_carat‚Äù which we calculate by taking price and divide it by carat. Keep in mind this would be an easy way to log transform data.\n\ndiamonds |&gt; \n  mutate(price_per_carat = price/carat)\n\n# A tibble: 53,940 √ó 11\n   carat cut   color clarity depth table price     x     y     z price_per_carat\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43           1417.\n 2  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31           1552.\n 3  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31           1422.\n 4  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63           1152.\n 5  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75           1081.\n 6  0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48           1400 \n 7  0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47           1400 \n 8  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53           1296.\n 9  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49           1532.\n10  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39           1470.\n# ‚Ñπ 53,930 more rows\n\n\nMutated columns are by default put at the end of the dataframe. We can reorder simply using select().\n\ndiamonds |&gt; \n  mutate(price_per_carat = price/carat) |&gt; \n  select(price_per_carat, everything()) # put new column first, then everything\n\n# A tibble: 53,940 √ó 11\n   price_per_carat carat cut   color clarity depth table price     x     y     z\n             &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1           1417.  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2           1552.  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31\n 3           1422.  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 4           1152.  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63\n 5           1081.  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n 6           1400   0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7           1400   0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8           1296.  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53\n 9           1532.  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\n10           1470.  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nYou can also make new columns using conditional statements. For example, what if we want to create a new column that tells us if a diamond is more than $1000 called ‚Äúat_least_1000‚Äù. We will do this using if_else().\n\ndiamonds |&gt; \n  mutate(at_least_1000 = if_else(condition = price &gt;= 1000,\n                                  true = \"$1000 or more\",\n                                  false = \"less than $1000\")) |&gt; \n  select(at_least_1000, everything()) # move to front so we can see it\n\n# A tibble: 53,940 √ó 11\n   at_least_1000   carat cut   color clarity depth table price     x     y     z\n   &lt;chr&gt;           &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 less than $1000  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2 less than $1000  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31\n 3 less than $1000  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 4 less than $1000  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63\n 5 less than $1000  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n 6 less than $1000  0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7 less than $1000  0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8 less than $1000  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53\n 9 less than $1000  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\n10 less than $1000  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nIf you have more than two conditions, you can use case_when().\nIf you use mutate() to create a new column that has the same name as an existing column, it will override that current column.\nYou can find other mutate() helpers here."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#sorting-with-arrange",
    "href": "modules/module2/04_wrangling/04_wrangling.html#sorting-with-arrange",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Sorting with arrange()",
    "text": "Sorting with arrange()\nSometimes you just want to see a dataframe ordered by a particular column. We can do that easily with arrange().\n\ndiamonds |&gt; \n  arrange(price)\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nBy default, arrange() sorts from smallest to largest. We can change that if that‚Äôs what we want.\n\n# these are the same\ndiamonds |&gt; \n  arrange(-price)\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ‚Ñπ 53,930 more rows\n\ndiamonds |&gt; \n  arrange(desc(price))\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ‚Ñπ 53,930 more rows"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#computing-summaries-with-summarize",
    "href": "modules/module2/04_wrangling/04_wrangling.html#computing-summaries-with-summarize",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Computing summaries with summarize()",
    "text": "Computing summaries with summarize()\nThe function summarize() calculates summary information based on the functions you provide as arguments. This function creates a wholly new dataframe, providing one row for each grouping variable. If there is no grouping, the resulting dataframe will have one row.\nLet‚Äôs look at an example. We can use summarize() The syntax is new_column_name = function().\n\ndiamonds |&gt; \n  summarize(mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      3933.\n\n\nWe can also provide multiple items for summary.\n\ndiamonds |&gt; \n  summarize(mean_price = mean(price),\n            sd_price = sd(price),\n            count = n())\n\n# A tibble: 1 √ó 3\n  mean_price sd_price count\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1      3933.    3989. 53940\n\n\nHere are some examples of functions you can use within summarize():\n\nmean() and median(): calculate mean and median\nsd() and IQR(): calculate standard deviation and interquartile range\nmin() and max(): calculate min and max\nn() and n_distinct(): calculate how many observations there are, and how many distinct observations there are\n\nYou can also use the function across() combined with where() to calculate summary data ‚Äúacross‚Äù different columns.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nFor example, like we see in the illustration above, we might want to calculate the mean ‚Äúacross‚Äù all columns ‚Äúwhere‚Äù if we asked if that column contains numeric data, we would get TRUE.\n\ndiamonds |&gt; \n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 1 √ó 7\n  carat depth table price     x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.798  61.7  57.5 3933.  5.73  5.73  3.54\n\n\nI hope you can start to see now how combining lots of these different functions together will help you achieve what you want with your coding."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#operations-by-group-with-group_by",
    "href": "modules/module2/04_wrangling/04_wrangling.html#operations-by-group-with-group_by",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Operations by group with group_by()",
    "text": "Operations by group with group_by()\nSometimes you might want to group your data together to perform operations group-wise. You can do this with group_by(). The way to ungroup is to use ungroup().\nFor example, say we want to calculate the average price of a diamond for each cut type.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(mean_price = mean(price))\n\n# A tibble: 5 √ó 2\n  cut       mean_price\n  &lt;ord&gt;          &lt;dbl&gt;\n1 Fair           4359.\n2 Good           3929.\n3 Very Good      3982.\n4 Premium        4584.\n5 Ideal          3458.\n\n\nNow instead of getting one row for the mean price, we are getting a mean price for each cut.\nNote that when you use group_by(), the groupings are now embedded within your data. Let me show you what I mean.\n\ndiamonds_cut &lt;- diamonds |&gt; \n  group_by(cut)\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\nglimpse(diamonds_cut)\n\nRows: 53,940\nColumns: 10\nGroups: cut [5]\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\n\nAgain we can combine these different functions together to summarize for the mean value across all columns that are numeric, but this time grouped by cut.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 5 √ó 8\n  cut       carat depth table price     x     y     z\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40\n\n\nWe can also use summarize() to add how many observations there are for each category.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(across(where(is.numeric), mean), n = n())\n\n# A tibble: 5 √ó 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\n\nHere is a helpful blogpost by Hadley Wickham for working across columns."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#pivoting-with-pivot_longer-and-pivot_wider",
    "href": "modules/module2/04_wrangling/04_wrangling.html#pivoting-with-pivot_longer-and-pivot_wider",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Pivoting with pivot_longer() and pivot_wider()",
    "text": "Pivoting with pivot_longer() and pivot_wider()\nThe function pivot_longer() will often let you make your data in ‚Äútidy‚Äù format, and pivot_wider() allow you to make it untidy (but often still useful) again. Let me explain more what I mean.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nThis is easier to ‚Äúsee‚Äù üëÄ than to explain. Here is an example of non-tidy data, where there is data embedded in column names, and one variable (the rank of a song) is spread across many columns:\n\nbillboard\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nHere is an example of the same exact data, in a tidy format, where those data that used to be column names, are now values coded for a particular variable.\n\nbillboard_long &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\nbillboard_long\n\n# A tibble: 24,092 √ó 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ‚Ñπ 24,082 more rows\n\n\nWe can go back from our new longer dataframe with pivot_wider().\n\nbillboard_long |&gt; \n  pivot_wider(names_from = week,\n              values_from = rank)\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nIn recap:\npivot_longer() pulls data that is embedded in column names, and reshapes your dataframe such this information is now embedded within the values. Or put differently, it collects variables that are spread across multiple columns into a single column. This makes your dataframes longer, i.e., increases the number of rows. Typically, we use pivot_longer() to make an untidy dataset tidy.\npivot_wider() takes data that is embedded in the values of your dataframe, and puts this information in variable names. Or put differently, it spreads a variable across multiple columns. This makes your dataframe ‚Äúwider‚Äù, i.e., increases the number of columns. Typically, pivot_wider() will make a dataset untidy. This can be useful for certain calculations, or if you want to use a for loop to do something iteratively across columns."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#joining-data-together",
    "href": "modules/module2/04_wrangling/04_wrangling.html#joining-data-together",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Joining data together",
    "text": "Joining data together\nOften you will have two separate dataframes that you want to join together. You can do this in two main ways:\n\nby matching something between them (i.e., using _join())\nby smushing them together in their existing order by columns bind_cols() or rows bind_rows().\n\n\n*_join()\nWe can join two dataframes, let‚Äôs call them x and y, together based on a key that we provide. This is one of the first things I did using R that I felt like wow this is really a lot easier than be doing this manually.\nThere are four types of joins:\n\ninner_join(): keeps observations in x that are also present in y\nleft_join(): keeps observations in x\nright_join()`: keeps observations in y\nfull_join(): keeps observations in both x and y\n\nWe will use the datasets band_members and band_instruments which are pre-loaded with the tidyverse to show how this works. You can also see these examples on the mutating joins documentation page.\n\nglimpse(band_members)\n\nRows: 3\nColumns: 2\n$ name &lt;chr&gt; \"Mick\", \"John\", \"Paul\"\n$ band &lt;chr&gt; \"Stones\", \"Beatles\", \"Beatles\"\n\nglimpse(band_instruments)\n\nRows: 3\nColumns: 2\n$ name  &lt;chr&gt; \"John\", \"Paul\", \"Keith\"\n$ plays &lt;chr&gt; \"guitar\", \"bass\", \"guitar\"\n\n\nR will make its best guess as to what you want to ‚Äújoin‚Äù based on, and that works a lot of the time, but I always like to be exclicit and indicate the column key for the join with by =.\nAn inner join: we will only get the observations that are present in both dataframes.\n\ninner_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 2 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nAn left join: we will only get the observations that are present in band_members. Note the appearance of NA for Mick.\n\nleft_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 3 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nAn right join: we will only get the observations that are present in band_instruments. Note the appearance of NA for band for Keith (Richards, who is in the Rolling Stones). You could also switch the order of the dataframes in your argument instead of using left vs right.\n\nright_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 3 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n3 Keith &lt;NA&gt;    guitar\n\n\nAn full join: we get all observations of what is present in band_members and band_instruments.\n\nfull_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 4 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nThere is nuances to what happens in different joining situations, so do this cautiously and always check that it went the way you expected it to.\n\n\nbind_rows() and bind_cols()\nGenerally it would be preferrable to use a _join() over bind_cols() or bind_rows() since in the latter, the binding happens in the order that observations appear. This might make your data not meaningful without you knowing.\nLet‚Äôs get to the examples.\n\ndata1 &lt;- tibble(x = 1:5)\ndata2 &lt;- tibble(y = 6:10)\n\nbind_cols(data1, data2)\n\n# A tibble: 5 √ó 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     6\n2     2     7\n3     3     8\n4     4     9\n5     5    10\n\n\n\nbind_rows(data1, data2)\n\n# A tibble: 10 √ó 2\n       x     y\n   &lt;int&gt; &lt;int&gt;\n 1     1    NA\n 2     2    NA\n 3     3    NA\n 4     4    NA\n 5     5    NA\n 6    NA     6\n 7    NA     7\n 8    NA     8\n 9    NA     9\n10    NA    10"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#handling-strings",
    "href": "modules/module2/04_wrangling/04_wrangling.html#handling-strings",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Handling strings",
    "text": "Handling strings\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nHandling strings (i.e., ‚Äústrings‚Äù of characters) could be multiple whole lessons, so my goal is to introduce you here to how to handle them. The tidyverse package to manage strings is called stringr. Sometimes you might want to automate extraction of only part of a value present in a column to use, remove some values, or split strings apart. This is valuable especially when the way that data is coded/recorded is different than the way you want it to be when you analyze it. Instead of manually recoding in excel, you can reproducibly and tracibly recode in R. You can read about all the functions within stringr here.\nYou can use regular expressions within stringr functions, but I‚Äôm not going to explicitly go over that (check out these code clubs regex1, regex2 if you want to learn more).\nI‚Äôm going to create some sample data to play with.\n\nstrings &lt;- tibble(\n  sample = c(rep.int(\"Treatment_Level1\", 3), \n             rep.int(\"Treatment_Level2\", 3),\n             rep.int(\"Treatment_Level3\", 3),\n             rep.int(\"Control_Level1\", 3),\n             rep.int(\"Control_Level2\", 3),\n             rep.int(\"Control_Level3\", 3)))\n\nLet‚Äôs first ask how many times do we have the string ‚Äú3‚Äù in our dataframe? Note that these functions accept a vector, so you need to provide data in that form. The function str_detect() gives a logical vector as the output, the same length as the vector provided, and indicates FALSE when the pattern is not met, and TRUE when it is.\n\nstr_detect(strings$sample, pattern = \"3\")\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n# using sum will count how many times the logical is evaluated to be TRUE\nsum(str_detect(strings$sample, pattern = \"3\"))\n\n[1] 6\n\n\nYou might want to re-code your data so that Level1 becomes that actual level used. Let‚Äôs say that Level1 is 100mg, Level2 is 300 mg, and Level3 is 500mg. We can do this with str_replace() to replace the first match only or str_replace_all() to replace all matches (which is what we want here).\n\n(strings$sample &lt;- strings |&gt; \n  select(sample) |&gt; \n  pull(sample) |&gt; # make a vector so can pass to next fxn\n  str_replace_all(pattern = \"Level1\", replacement = \"100mg\") |&gt; \n  str_replace_all(pattern = \"Level2\", replacement = \"300mg\") |&gt; \n  str_replace_all(pattern = \"Level3\", replacement = \"500mg\"))   \n\n [1] \"Treatment_100mg\" \"Treatment_100mg\" \"Treatment_100mg\" \"Treatment_300mg\"\n [5] \"Treatment_300mg\" \"Treatment_300mg\" \"Treatment_500mg\" \"Treatment_500mg\"\n [9] \"Treatment_500mg\" \"Control_100mg\"   \"Control_100mg\"   \"Control_100mg\"  \n[13] \"Control_300mg\"   \"Control_300mg\"   \"Control_300mg\"   \"Control_500mg\"  \n[17] \"Control_500mg\"   \"Control_500mg\"  \n\n\nWe might not want to have both Treatment/Control and Level nested in the same cell, we can split them apart using separate_*() functions. Here we are using separate_wider_delim() to seprate the column ‚Äúsample‚Äù into two new columns called ‚Äútreatment‚Äù and ‚Äúdose.\n\n(strings_separated &lt;- strings |&gt; \n  separate_wider_delim(cols = sample,\n                       delim = \"_\", # what is the delimiter\n                       names = c(\"treatment\", \"dose\")))\n\n# A tibble: 18 √ó 2\n   treatment dose \n   &lt;chr&gt;     &lt;chr&gt;\n 1 Treatment 100mg\n 2 Treatment 100mg\n 3 Treatment 100mg\n 4 Treatment 300mg\n 5 Treatment 300mg\n 6 Treatment 300mg\n 7 Treatment 500mg\n 8 Treatment 500mg\n 9 Treatment 500mg\n10 Control   100mg\n11 Control   100mg\n12 Control   100mg\n13 Control   300mg\n14 Control   300mg\n15 Control   300mg\n16 Control   500mg\n17 Control   500mg\n18 Control   500mg\n\n\nThe opposite function for separate_() is unite().\nIf we wanted to extract just the number part out of ‚Äúdose‚Äù we could use readr::parse_number() to do that. Note I‚Äôve embedded parse_number() within a mutate() function to change the values in the dataset.\n\nstrings_separated |&gt; \n  mutate(dose = parse_number(dose))\n\n# A tibble: 18 √ó 2\n   treatment  dose\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Treatment   100\n 2 Treatment   100\n 3 Treatment   100\n 4 Treatment   300\n 5 Treatment   300\n 6 Treatment   300\n 7 Treatment   500\n 8 Treatment   500\n 9 Treatment   500\n10 Control     100\n11 Control     100\n12 Control     100\n13 Control     300\n14 Control     300\n15 Control     300\n16 Control     500\n17 Control     500\n18 Control     500"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#cleaning-up-column-names-with-clean_names",
    "href": "modules/module2/04_wrangling/04_wrangling.html#cleaning-up-column-names-with-clean_names",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Cleaning up column names with clean_names()",
    "text": "Cleaning up column names with clean_names()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nI really like the package janitor which has some nice functions for cleaning up üßπ ‚Äúmessy‚Äù data. I use clean_names() a lot which converts untidy column names into only characters (default all in lower case) and connects words or terms with underscores.\nI am making up some messy names so you can see how this works.\n\n# make messy data\nmessy_data &lt;- tibble(\n  \"Sample Name\" = 1:5,\n  \"THE NEXT VARIABLE\" = 6:10,\n  \"ThisIsChaos\" = 11:15\n)\n\n# print column names\ncolnames(messy_data)\n\n[1] \"Sample Name\"       \"THE NEXT VARIABLE\" \"ThisIsChaos\"      \n\n\n\n# install and load janitor\n# install.packages(\"janitor)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# clean up column names\nclean_names(messy_data)\n\n# A tibble: 5 √ó 3\n  sample_name the_next_variable this_is_chaos\n        &lt;int&gt;             &lt;int&gt;         &lt;int&gt;\n1           1                 6            11\n2           2                 7            12\n3           3                 8            13\n4           4                 9            14\n5           5                10            15"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html",
    "href": "modules/module3/08_correlations/08_correlations.html",
    "title": "Visualizing Correlations",
    "section": "",
    "text": "Figure from XKCD"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations.html#introduction",
    "title": "Visualizing Correlations",
    "section": "Introduction",
    "text": "Introduction\nWe will will building on our lessons on ggplot101 and ggplot102 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, geoms, facets, scales, labels, and themes. Today we are going to apply what we learned towards trying to better understanding and visualize correlations within our data. To do this we will also use some ggplot extension packages.\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\n\nlibrary(tidyverse)\n\nToday we are going to continue to use the same real research data from my group from last week. We will be reading in the supplementary data from a paper written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 1. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloids &lt;- readxl::read_excel(\"tpg220192-sup-0002-supmat.xlsx\",\n                                sheet = \"S1 Raw Data Diversity Panel\")\n\n\nknitr::kable(head(alkaloids))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nYear\nEnvironment\nBlock\nGenotype\nPlot_Source\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nLatitude\nLongitude\nDehydrotomatidine\nTomatidine\nDehydrotomatine1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\n\n\n\n\n7805\n2018\nFreEarly18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.000000\n5.726010\n0.350331\n6.076341\n172.66244\n1.079190\n86.72742\n17.831892\n9.142607\n114.78111\n18.902399\n56.307182\n1.890053\n77.099634\n5.125904\n10.277325\n336.8893\n347.1666\n3.787979\n0.924195\n3.943230\n8.655404\n731.5675\n\n\n7898\n2017\nFre17\n2\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.169068\n0.000000\n0.000000\n0.000000\n55.47329\n0.000000\n53.32292\n13.630697\n4.841762\n71.79538\n3.557348\n4.107289\n0.000000\n7.664637\n2.905500\n5.548102\n199.6694\n205.2175\n8.978931\n1.897850\n6.794690\n17.671471\n360.8969\n\n\n7523\n2018\nFreLate18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.135675\n0.680554\n5.073552\n0.000000\n5.073552\n123.85835\n0.000000\n50.90989\n6.503939\n1.368847\n58.78268\n3.931461\n4.123222\n0.623340\n8.678023\n2.185082\n5.104115\n259.0177\n264.1218\n4.049145\n0.000000\n6.749386\n10.798531\n474.3143\n\n\n7724\n2017\nFre17\n1\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.054300\n0.497261\n19.419087\n0.000000\n19.419087\n239.01264\n0.000000\n36.02318\n8.557673\n7.483933\n52.06478\n3.341048\n16.415426\n1.057100\n20.813574\n0.000000\n0.000000\n203.0061\n203.0061\n1.678210\n0.000000\n2.349633\n4.027843\n538.8955\n\n\n7427\n2018\nFreLate18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.139454\n0.553801\n0.000000\n0.000000\n0.000000\n64.31783\n0.879435\n39.91027\n7.228388\n3.015298\n51.03339\n0.000000\n3.131685\n0.000000\n3.131685\n0.000000\n4.054211\n299.5687\n303.6229\n10.146857\n0.000000\n4.882339\n15.029197\n437.8283\n\n\n7854\n2018\nFreEarly18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.049700\n0.262174\n3.737579\n0.000000\n3.737579\n68.44913\n0.000000\n23.86864\n13.506299\n1.456982\n38.83192\n4.657902\n4.259007\n0.605729\n9.522638\n9.832149\n11.595595\n459.5205\n471.1161\n6.839930\n0.486236\n5.595751\n12.921917\n614.7233\n\n\n\n\n\nThis dataset has 605 observations, with data about different steroidal alkaloids in the fruits of different tomato germplasm grown in 3 locations across 2 years. There is also some other metadata too.\nFor those who are chemistry minded, here is a little pathway context for the compounds we are investigating today.\n\n\n\nFigure from Syzma≈Ñsky et al., Nature Genetics 2020"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#scatterplots",
    "href": "modules/module3/08_correlations/08_correlations.html#scatterplots",
    "title": "Visualizing Correlations",
    "section": "Scatterplots",
    "text": "Scatterplots\nA very simple first pass way to understand if you have relationships within your data is to make scatterplots of the variables you think might be correlated. Let‚Äôs start by investigating how the different alkaloid concentrations are correlated to each other. First we will see how alpha-tomatine content (Tomatine) is related to total steroidal alkaloid content (Total).\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total, y = Tomatine)) +\n  geom_point() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g)\",\n       y = \"Alpha-Tomatine (¬µg/100 g)\")\n\n\n\n\nIt seems like there are two separate groups here - the points with a steeper slope, and the points with a less steep slope. We can color our points based on what Class of tomato the data comes from, maybe that will reveal something. In the meanwhile let‚Äôs make this plot look a bit nicer. The package scales has some nice functions that help you control the scaling of your plots, in this case, making each of the axes have numbers in comma_format(). I also am using the hex codes for a color-blind friendly qualitative color scheme developed by Paul Tol.\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point(alpha = 0.8) +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")\n\n\n\n\nAll of the tomatoes in the two extremes of this plot are from the Class Wild Cherry. What would this look like if we removed these fruits? Note, I adjusted the color scale to remove the hex code associated with Wild Cherry but keeping the other colors the same.\n\nalkaloids %&gt;%\n  filter(Class != \"Wild Cherry\") %&gt;%\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point() +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#adding-geom_smooth",
    "href": "modules/module3/08_correlations/08_correlations.html#adding-geom_smooth",
    "title": "Visualizing Correlations",
    "section": "Adding geom_smooth()",
    "text": "Adding geom_smooth()\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point(alpha = 0.8) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nFaceted scatterplots\nWe may be able to see trends by tomato class more easily if we facet our scatterplots. I also am demonstrating here how within the ggplot function you can make alter the aesthetics you plot - here I am turning data that is present as ¬µg/100 g to mg/100 g by dividing by 1000 and changing the axis labels accordingly.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Total/1000, y = Tomatine/1000, color = Class)) +\n  geom_point(alpha = 0.8) +\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  facet_wrap(vars(Class), scales = \"free\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total Steroidal Alkaloids (mg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (mg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#correlation-matrix-with-cor",
    "href": "modules/module3/08_correlations/08_correlations.html#correlation-matrix-with-cor",
    "title": "Visualizing Correlations",
    "section": "Correlation matrix with cor()",
    "text": "Correlation matrix with cor()\ncor() is a function from base R that will allow you to create a correlation matrix.\nBefore we use cor() we will clean up our dataset to include only the variables we want to correlate.\n\ncolnames(alkaloids)\n\n [1] \"ID\"                                      \n [2] \"Year\"                                    \n [3] \"Environment\"                             \n [4] \"Block\"                                   \n [5] \"Genotype\"                                \n [6] \"Plot_Source\"                             \n [7] \"Class\"                                   \n [8] \"Origin\"                                  \n [9] \"Provence\"                                \n[10] \"Blanca_Cluster1\"                         \n[11] \"Blanca_Cluster2\"                         \n[12] \"Passport_Species\"                        \n[13] \"Passport_Classification\"                 \n[14] \"Sim_Grouping\"                            \n[15] \"Latitude\"                                \n[16] \"Longitude\"                               \n[17] \"Dehydrotomatidine\"                       \n[18] \"Tomatidine\"                              \n[19] \"Dehydrotomatine1\"                        \n[20] \"Dehydrotomatine2\"                        \n[21] \"TotalDehydrotomatine\"                    \n[22] \"Tomatine\"                                \n[23] \"Hydroxytomatine1\"                        \n[24] \"Hydroxytomatine2\"                        \n[25] \"Hydroxytomatine3\"                        \n[26] \"Hydroxytomatine4\"                        \n[27] \"TotalHydroxytomatine\"                    \n[28] \"Acetoxytomatine1\"                        \n[29] \"Acetoxytomatine2\"                        \n[30] \"Acetoxytomatine3\"                        \n[31] \"TotalAcetoxytomatine\"                    \n[32] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[33] \"LycoperosideFGEsculeosideA1\"             \n[34] \"LycoperosideFGEsculeosideA2\"             \n[35] \"TotalLycoperosideFGEsculeosideA\"         \n[36] \"EsculeosideB1\"                           \n[37] \"EsculeosideB2\"                           \n[38] \"EsculeosideB3\"                           \n[39] \"TotalEsculeosideB\"                       \n[40] \"Total\"                                   \n\n\nFrom looking at the colnames and reading the supplemental information, we can see that some columns are composites of others. For example, the column TotalAcetoxytomatine = Acetoxytomatine1 + Acetoxytomatine2 + Acetoxytomatine3. So we want to pull only the columns that represent the total for any given alkaloids. There should be 10 columns.\n\n# create a vector of the names we want to keep\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\n# make a new df including some metadata and the alkaloid_total_names\nalkaloids_totals &lt;- alkaloids %&gt;%\n  select(ID, Year, Environment, Block, Genotype, Class, all_of(alkaloid_total_names))\n\n# did it work? look at colnames()\ncolnames(alkaloids_totals)\n\n [1] \"ID\"                                      \n [2] \"Year\"                                    \n [3] \"Environment\"                             \n [4] \"Block\"                                   \n [5] \"Genotype\"                                \n [6] \"Class\"                                   \n [7] \"Dehydrotomatidine\"                       \n [8] \"Tomatidine\"                              \n [9] \"TotalDehydrotomatine\"                    \n[10] \"Tomatine\"                                \n[11] \"TotalHydroxytomatine\"                    \n[12] \"TotalAcetoxytomatine\"                    \n[13] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[14] \"TotalLycoperosideFGEsculeosideA\"         \n[15] \"TotalEsculeosideB\"                       \n[16] \"Total\"                                   \n\n\nNow we can create a correlation matrix to see how each of our 10 alkaloids is correlated to the concentration of each other alkaloid (including the compile metric of Total which sums all the alkaloids). The default for cor() is to use Pearson‚Äôs correlation coefficient, but you can set to use Spearman method = \"spearman\" or Kendall method = \"kendall\" if you prefer. Check the documentation for cor() for more information.\n\nalkaloids_cor &lt;- alkaloids_totals %&gt;%\n  select(all_of(alkaloid_total_names)) %&gt;%\n  cor()\n\n# look at our correlation matrix\nknitr::kable(alkaloids_cor) # kable makes a nicely formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDehydrotomatidine\nTomatidine\nTotalDehydrotomatine\nTomatine\nTotalHydroxytomatine\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nTotalLycoperosideFGEsculeosideA\nTotalEsculeosideB\nTotal\n\n\n\n\nDehydrotomatidine\n1.0000000\n0.2974462\n0.0324918\n0.0238230\n0.0099126\n0.0322029\n0.0305049\n0.0761907\n0.0282219\n0.0708252\n\n\nTomatidine\n0.2974462\n1.0000000\n0.3744672\n0.3736949\n0.1003558\n0.0382981\n-0.0059964\n0.0373649\n0.0126724\n0.2044979\n\n\nTotalDehydrotomatine\n0.0324918\n0.3744672\n1.0000000\n0.9214859\n0.2290192\n0.4011257\n-0.0820469\n-0.1149682\n-0.1217560\n0.5636969\n\n\nTomatine\n0.0238230\n0.3736949\n0.9214859\n1.0000000\n0.0995212\n0.1220596\n-0.1140360\n-0.1357819\n-0.1260377\n0.3756155\n\n\nTotalHydroxytomatine\n0.0099126\n0.1003558\n0.2290192\n0.0995212\n1.0000000\n0.3563506\n0.0330078\n0.0284887\n0.0134806\n0.4774036\n\n\nTotalAcetoxytomatine\n0.0322029\n0.0382981\n0.4011257\n0.1220596\n0.3563506\n1.0000000\n-0.0865506\n-0.1106212\n-0.0947254\n0.6782337\n\n\nDehydrolycoperosideFGdehydroesculeosideA\n0.0305049\n-0.0059964\n-0.0820469\n-0.1140360\n0.0330078\n-0.0865506\n1.0000000\n0.8862982\n0.7401116\n0.4792108\n\n\nTotalLycoperosideFGEsculeosideA\n0.0761907\n0.0373649\n-0.1149682\n-0.1357819\n0.0284887\n-0.1106212\n0.8862982\n1.0000000\n0.7779405\n0.5222751\n\n\nTotalEsculeosideB\n0.0282219\n0.0126724\n-0.1217560\n-0.1260377\n0.0134806\n-0.0947254\n0.7401116\n0.7779405\n1.0000000\n0.4059824\n\n\nTotal\n0.0708252\n0.2044979\n0.5636969\n0.3756155\n0.4774036\n0.6782337\n0.4792108\n0.5222751\n0.4059824\n1.0000000\n\n\n\n\n\nNote the diagonal is all composed of 1s. This makes sense because the correlation of each alkaloid with itself is 1."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#using-ggcorrplot-from-ggcorrplot",
    "href": "modules/module3/08_correlations/08_correlations.html#using-ggcorrplot-from-ggcorrplot",
    "title": "Visualizing Correlations",
    "section": "Using ggcorrplot() from ggcorrplot",
    "text": "Using ggcorrplot() from ggcorrplot\nUse the function ggcorrplot() without any additional arguments besides the correlation matrix alkaloids_cor. In general, I think if you want to make a bunch of correlation plots quickly, and don‚Äôt intend to publish them, `ggcorrplot() works well, but the visuals of the plot are quite difficult to customize.\n\nlibrary(ggcorrplot)\n\nggcorrplot(alkaloids_cor)\n\n\n\n\nThis is not a perfect plot but its a good starting point. Correlation matrices are inherently symmetric, meaning if we display only the top or bottom triangle, we do not lose any information. We will work on editing this plot in different ways to show more information and make it more beautiful.\nWe could also make the plot circles instead of squares at the same time.\n\nggcorrplot(alkaloids_cor, \n           method = \"circle\",\n           type = \"lower\")\n\n\n\n\nIn general, I think if you want to make a bunch of correlation plots quickly, and don‚Äôt intend to publish them, `ggcorrplot() works well, but the visuals of the plot are quite difficult to customize."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#using-corrplot-from-corrplot",
    "href": "modules/module3/08_correlations/08_correlations.html#using-corrplot-from-corrplot",
    "title": "Visualizing Correlations",
    "section": "Using corrplot() from corrplot",
    "text": "Using corrplot() from corrplot\nSimilarly, you can use a base R plotting based package corrplot() to make correlation plots. The customization syntax here is quite different from what we‚Äôve been working with in ggplot, but I wanted you to feel familiar with some base R tools.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrplot(alkaloids_cor, type = \"lower\")\n\n\n\n\nI have used corrplot() in publications before and felt like I couldn‚Äôt customize the plots as much as I wanted. In the process of putting together this content, I learned some news ways to customize these plots that are actually very nice. Here are some parameters you can modify in R. You can also order your variables by hierarchical clustering.\nFirst we will start (as we always do) by wrangling.\n\n# create matrix for correlation\nalkaloids_to_cor &lt;- alkaloids_totals %&gt;%\n  select(all_of(alkaloid_total_names)) %&gt;%\n  as.matrix() # rcorr() needs a matrix\n\nlibrary(Hmisc) # does cor() but also computes significance levels\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n# create a matrix of pvalues for the correlations\nalkaloids_rcorr = rcorr(alkaloids_to_cor, type = \"pearson\")\n\n# create a vector of the alkaloid names for labeling\nalkaloid_labels &lt;- c(\"Dehydrotomatidine\",\n                     \"Tomatidine\",\n                     \"Dehydrotomatine\",\n                     \"Alpha-Tomatine\",\n                     \"Hydroxytomatine\",\n                     \"Acetoxytomatine\",\n                     \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\",\n                     \"Lycoperoside F, G, \\nor Escueloside A\",\n                     \"Escueloside B\",\n                     \"Total Steroidal Alkaloids\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(alkaloids_rcorr$r) &lt;- alkaloid_labels\nrownames(alkaloids_rcorr$r) &lt;- alkaloid_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(alkaloids_rcorr$P) &lt;- alkaloid_labels\nrownames(alkaloids_rcorr$P) &lt;- alkaloid_labels\n\nNow we are ready to plot\n\ncorrplot(alkaloids_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = alkaloids_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"black\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 0.6) # size of correlation font"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#ggally",
    "href": "modules/module3/08_correlations/08_correlations.html#ggally",
    "title": "Visualizing Correlations",
    "section": "GGally",
    "text": "GGally\n\nggcorr()\nAnother ggplot extension package ggally has the function ggcorr() which also allows the creation of correlation plots, but ones that are more easily customizable. ggcorr() objects are moderately customizable. They make work for some of you so I‚Äôm sharing how to make them.\nNote, GGally::ggcorr() does not take a correlation matrix, but instead takes the data you want to make a correlation matrix for. You can specific the method of correlation in the arguments. The default is Pearson‚Äôs correlation.\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nto_corr &lt;- alkaloids_totals %&gt;%\n  select(all_of(alkaloid_total_names)) \n\nggcorr(to_corr)\n\n\n\n\nThere is only one labeled axis - this is because there is no diagonal in these plots, like we saw with ggcorrplot() and corrplot().\nWe can now spend some time improving the aesthetics of our plot.\n\nggcorr(to_corr, # data for correlation\n       low = \"#f1a340\", # -1 correlation color\n       mid = \"#f7f7f7\", # 0 correlation color\n       high = \"#998ec3\") # 1 correlation color\n\n\n\nggcorr(to_corr,\n       low = \"#f1a340\", mid = \"#f7f7f7\", high = \"#998ec3\",\n       geom = \"circle\",\n       label = TRUE, \n       label_size = 2, \n       label_round = 2,\n       layout.exp = 3)\n\n\n\n\nFor this example, we have very long label names which are really difficult to wrap, but if your labels are more reasonable this may work well for you.\n\n\nggpairs()\nWe can also use the function GGally::ggpairs() to make a matrix of correlation related plots.\n\nalkaloids_totals %&gt;%\n  ggpairs(columns = c(\"Tomatine\", \"TotalLycoperosideFGEsculeosideA\", \"Total\"), # pick variables\n          aes(color = Class))\n\n\n\n\nLet‚Äôs customized a bit.\n\n# remove zeroes since they don't log transform\n# make log transformed columns\nalkaloids_totals_log &lt;- alkaloids_totals %&gt;%\n  filter(Tomatine != 0, \n         TotalLycoperosideFGEsculeosideA != 0,\n         Total != 0) %&gt;%\n  mutate(log10_tomatine = log10(Tomatine),\n         log10_FGA = log10(TotalLycoperosideFGEsculeosideA),\n         log10_total = log10(Total))\n\nalkaloids_totals_log %&gt;%\n    ggpairs(columns = c(\"log10_tomatine\", \"log10_FGA\", \"log10_total\"),\n          aes(color = Class, alpha = 0.5), # note alpha inside aes which is weird idk why\n          columnLabels = c(\"Alpha-Tomatine\", \"Lycoperoside F/G\\n Escueloside A\", \"Total Alkaloids\"))"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#manually-making-correlation-plots-with-reshapemelt-and-ggplot",
    "href": "modules/module3/08_correlations/08_correlations.html#manually-making-correlation-plots-with-reshapemelt-and-ggplot",
    "title": "Visualizing Correlations",
    "section": "Manually making correlation plots with reshape::melt() and ggplot",
    "text": "Manually making correlation plots with reshape::melt() and ggplot\nBecause some of the correlation specific packages are hard to customize, I am going to show you how to make your own plots by reshaping your data with reshape2::melt() and some base R functions, and plotting using the standard ggplot syntax.\n\nlibrary(reshape2) # contains melt()\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# take cor matrix and convert to df with 3 columns: Var1, Var2, and value\nmelted_alkaloids_cor &lt;- melt(alkaloids_cor)\n\n# what does it look like?\nhead(melted_alkaloids_cor)\n\n                  Var1              Var2       value\n1    Dehydrotomatidine Dehydrotomatidine 1.000000000\n2           Tomatidine Dehydrotomatidine 0.297446153\n3 TotalDehydrotomatine Dehydrotomatidine 0.032491778\n4             Tomatine Dehydrotomatidine 0.023823011\n5 TotalHydroxytomatine Dehydrotomatidine 0.009912624\n6 TotalAcetoxytomatine Dehydrotomatidine 0.032202892\n\n\nFirst pass minimalist plotting\n\nmelted_alkaloids_cor %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile()\n\n\n\n\nLots to fix! What if we want only the upper or lower triangle, again since this plot is symmetric.\n\nUpper triangle\nKeep only the upper triangle.\n\n# \"save as\"\nalkaloids_upper &lt;- alkaloids_cor\n\n# use function lower.tri() and set the lower triangle all to NA\n# then we can keep only the upper triangle\nalkaloids_upper[lower.tri(alkaloids_upper)] &lt;- NA\n\n# melt to go back to long format\nmelted_alkaloids_upper &lt;- melt(alkaloids_upper, na.rm = TRUE)\n\n# did it work?\nhead(melted_alkaloids_upper) # yup\n\n                   Var1                 Var2      value\n1     Dehydrotomatidine    Dehydrotomatidine 1.00000000\n11    Dehydrotomatidine           Tomatidine 0.29744615\n12           Tomatidine           Tomatidine 1.00000000\n21    Dehydrotomatidine TotalDehydrotomatine 0.03249178\n22           Tomatidine TotalDehydrotomatine 0.37446722\n23 TotalDehydrotomatine TotalDehydrotomatine 1.00000000\n\n\n\n\nLower triangle\nCreate a lower triangle object to plot.\n\n# \"save as\"\nalkaloids_lower &lt;- alkaloids_cor\n\n# use function upper.tri() and set the upper triangle all to NA\n# then we can keep only the lower triangle\nalkaloids_lower[upper.tri(alkaloids_lower)] &lt;- NA\n\n# melt to go back to long format\nmelted_alkaloids_lower &lt;- melt(alkaloids_lower, na.rm = TRUE)\n\n# did it work?\nhead(melted_alkaloids_lower) # yup\n\n                  Var1              Var2       value\n1    Dehydrotomatidine Dehydrotomatidine 1.000000000\n2           Tomatidine Dehydrotomatidine 0.297446153\n3 TotalDehydrotomatine Dehydrotomatidine 0.032491778\n4             Tomatine Dehydrotomatidine 0.023823011\n5 TotalHydroxytomatine Dehydrotomatidine 0.009912624\n6 TotalAcetoxytomatine Dehydrotomatidine 0.032202892\n\n\nPlot\n\n# remember we made alkaloid_labels\nprint(alkaloid_labels)\n\n [1] \"Dehydrotomatidine\"                                 \n [2] \"Tomatidine\"                                        \n [3] \"Dehydrotomatine\"                                   \n [4] \"Alpha-Tomatine\"                                    \n [5] \"Hydroxytomatine\"                                   \n [6] \"Acetoxytomatine\"                                   \n [7] \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\"\n [8] \"Lycoperoside F, G, \\nor Escueloside A\"             \n [9] \"Escueloside B\"                                     \n[10] \"Total Steroidal Alkaloids\"                         \n\nmelted_alkaloids_lower %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#f1a340\",\n                       mid = \"#f7f7f7\",\n                       high = \"#998ec3\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = alkaloid_labels) +\n  scale_y_discrete(labels = alkaloid_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y =\"\",\n       title = \"Correlation between steroidal alkaloids using \\nPearson's correlation coefficient\")\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\n‚Ñπ Please use the `legend.position.inside` argument of `theme()` instead."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#useful-resources",
    "href": "modules/module3/08_correlations/08_correlations.html#useful-resources",
    "title": "Visualizing Correlations",
    "section": "Useful resources",
    "text": "Useful resources\n\ncor()\nHmisc::rcorr()\nggcorrplot::ggcorrplot()\ncorrplot::corrplot()\nGGally\nGGally:ggcorr()\nGGally:ggpairs()"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(???)"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#introduction",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(???)"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#total-cupping-score-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#total-cupping-score-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "Total cupping score in Arabica and Robusta",
    "text": "Total cupping score in Arabica and Robusta\nMake 3 different visualizations that shows the distribution of total cupping score (i.e.¬†total_cup_points) across Arabica and Robusta beans. Make the plots so you think they look good."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "Individual characteristic cupping scores in Arabica and Robusta",
    "text": "Individual characteristic cupping scores in Arabica and Robusta\nMake 3 different visualizations that show the distribution of all the individual contributors (i.e., aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points) to total cupping score across Arabica and Robusta in one plot."
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html",
    "href": "modules/module3/09_add-stats/09_add-stats.html",
    "title": "Annotating Statistics onto Plots",
    "section": "",
    "text": "Figure from XKCD"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats.html#introduction",
    "title": "Annotating Statistics onto Plots",
    "section": "Introduction",
    "text": "Introduction\nNow that we‚Äôve spent some time going through how to make plots, today we will focus on how to annotate statistics that you‚Äôve calculated to show statistical differences, embedded within your plot. I will go over a few different ways to do this.\nThe purpose of today‚Äôs session is more to give you practical experience with running and retrieving statistical analysis output, than teaching about the assumptions and background of the test itself. If you are looking for a good statistics class, I would recommend Dr.¬†Kristin Mercer‚Äôs HCS 8887 Experimental Design.\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\nWe are going to use data that was collection about body characteristics of penguins on Palmer Station in Antarctica. This data is in a dataframe called penguins in the package palmerpenguins which you can download from CRAN.\n\n\n\n\n\nFrom Palmer Penguins\n\n\n\n\n\n# install.packages(palmerpenguins)\nlibrary(tidyverse)\nlibrary(palmerpenguins) # for penguins data\nlibrary(rstatix) # for pipeable stats testing\nlibrary(agricolae) # for posthoc tests \nlibrary(ggpubr) # extension for adding stats to plots\nlibrary(glue) # for easy pasting\n\n\nknitr::kable(head(penguins)) # kable to make a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-t-tests-or-similar",
    "href": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-t-tests-or-similar",
    "title": "Annotating Statistics onto Plots",
    "section": "2 group comparisons (t-tests or similar)",
    "text": "2 group comparisons (t-tests or similar)\n\nOur question: Is there a significant difference in the body_weight_g of male and female penguins?\n\nBefore we run the statistics, let‚Äôs make a plot to see what this data looks like.\n\n# what are the values for sex?\nunique(penguins$sex)\n\n[1] male   female &lt;NA&gt;  \nLevels: female male\n\n# plot\n(penguins_by_sex &lt;- penguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs for body_mass_g and sex\n  ggplot(aes(x = sex, y = body_mass_g, color = sex)) + \n  geom_boxplot(outlier.shape = NA) + # remove outliers\n  geom_jitter(height = 0, width = 0.3) + # jitter only in the x direction\n  scale_x_discrete(labels = c(\"Female\", \"Male\")) + # change x-axis labels\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sex\",\n       y = \"Body Mass (g)\",\n       title = \"Body mass of penguins by sex\",\n       subtitle = \"Collected from Palmer Station, Antarctica\",\n       caption = \"Data accessed from the R package palmerpenguins\"))\n\n\n\n\nIt looks like there is a difference here. Before adding the statistics to our plot, let‚Äôs:\n\ntest that our data is suitable for running the text we want\nrun the statistical test separately from the plot\n\n\nTesting assumptions\nBriefly, in order to use parametric procedures (like a t-test), we need to be sure our data meets the assumptions for 1) normality and 2) constant variance. This is just one way to do these tests, there are others that I am not going to go over.\n\n\n\n\n\nIllustration by Allison Horst\n\n\n\n\n\nNormality\nWe will test normality by the Shapiro-Wilk test using the function rstatix::shapiro_test(). This function is a pipe-friendly wrapper for the function shapiro.test(), which just means you can use it with pipes.\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  group_by(sex) %&gt;% # test by sex\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 2 √ó 4\n  sex    variable    statistic            p\n  &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 female body_mass_g     0.919 0.0000000616\n2 male   body_mass_g     0.925 0.000000123 \n\n\nThis data is not normal, which means we need to use non-parametric tests. Since we are not meeting the assumption for nornality, really you don‚Äôt need to test for constant variance, but I‚Äôll show you how to do it anyway.\n\n\nConstant variance\nWe can test for equal variance using Levene‚Äôs test, levene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  levene_test(body_mass_g ~ sex) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic      p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1   331      6.06 0.0143\n\n\nNo constant variance. Double Non-parametric.\nCan we visualize normality another way?\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  ggplot(aes(x = body_mass_g, y = sex, fill = sex)) +\n  ggridges::geom_density_ridges(alpha = 0.7) + # density ridgeline plot\n  scale_y_discrete(labels = c(\"Female\", \"Male\")) +\n  theme_classic() +  \n  theme(legend.position = \"none\") +\n  labs(x = \"Body Mass (g)\",\n       y = \"Sex\",\n       title = \"Distribution of body weights for male and female penguins\")\n\nPicking joint bandwidth of 235\n\n\n\n\n\nSome of these distribution are bimodal (i.e., not normal). This is likely because we have 3 different species of penguins here. You can see below that actually each species looks reasonably normal.\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  ggplot(aes(x = body_mass_g, fill = sex)) +\n  geom_histogram() +\n  facet_grid(cols = vars(species), rows = vars(sex)) + # 2 way facet\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Body Mass (g)\",\n       y = \"Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNon-parametric t-test\nThis means if we want to test for different means, we can use the Wilcoxon rank sun test, or Mann Whitney test. If your data was normal, you could just change wilcox_test() to t_test() and the rest would be the same.\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  wilcox_test(body_mass_g ~ sex,\n              paired = FALSE)\n\n# A tibble: 1 √ó 7\n  .y.         group1 group2    n1    n2 statistic        p\n* &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 body_mass_g female male     165   168     6874. 1.81e-15\n\n\nThis is not surprising, that there is a significant difference in body weight between male and female penguins. We can see this clearly in our plot.\nHow can we add the stats to our plot?\n\n\nPlot\n\nUsing stat_compare_means()\nThe function stat_compare_means() allows mean comparison p-values to be easily added to a ggplot.\nNote, the function should look at your data and test for normality and pick the statistical test accordingly. You can see that is working in the chunk below, but I would recommend that you always do your own statistical test and make sure you plot accordingly.\n\npenguins_by_sex +\n  stat_compare_means()\n\n\n\npenguins_by_sex +\n  stat_compare_means(method = \"wilcox.test\") \n\n\n\n\n\n\nManually with geom_text() or annotate()\nIn general, plotting using geom_text() is easier, and follows classic geom_() syntax (e.g., includes aes()) but for some reason these don‚Äôt pass as vectorized objects so sometimes it yields low quality images. Using annotate() passes as vectors and thus tends to be higher quality. You can decide which you want to use depending on your purpose.\nIf I‚Äôm being honest, the most common way that I would add statistics to a plot if I was trying to do just a few simple plots at once, would be with annotate() . I like to use annotate() over geom_text() or geom_label() because it is vectorized and don‚Äôt become low quality down the road.\nWith geom_text()\n\npenguins_by_sex +\n  geom_text(aes(x = 2, y = 6500, label = \"*\"), # x, y, and label within aes()\n            color = \"black\", size = 6)\n\nWarning in geom_text(aes(x = 2, y = 6500, label = \"*\"), color = \"black\", : All aesthetics have length 1, but the data has 333 rows.\n‚Ñπ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\nWith annotate()\n\npenguins_by_sex +\n  annotate(geom = \"text\", # note no aes()\n           x = 2, y = 6500, \n           label = \"*\", \n           size = 6)\n\n\n\n\nYou can also add multiple annotation layers. I‚Äôm introducing a new function here, glue() which is amazing for easy syntax pasting of strings with data.\nThe syntax for glue() is like this:\n\nx &lt;- 2 + 3\n\nglue(\"2 + 3 = {x}\")\n\n2 + 3 = 5\n\n\n\n# we did this already, just assigning to object\nby_sex_pval &lt;- penguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;%\n  wilcox_test(body_mass_g ~ sex,\n              paired = FALSE)\n\n# plot\npenguins_by_sex +\n  ylim(2500, 7500) + # adjust the y-axis so there's space for the label\n  annotate(geom = \"text\", x = 2, y = 6500, label = \"*\", size = 6) +\n  annotate(geom = \"text\", x = 2, y = 7000,\n           label = glue(\"Wilcoxon signed rank test \\np-value = {by_sex_pval$p}\"))"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-anova-or-similar",
    "href": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-anova-or-similar",
    "title": "Annotating Statistics onto Plots",
    "section": ">2 group comparisons (ANOVA or similar)",
    "text": "&gt;2 group comparisons (ANOVA or similar)\nWhen we are comparing means between more than 2 samples, we will have to first run a statistical test to see if there are any significant differences among our groups, and then if there are, run a post-hoc test. Before we do that, let‚Äôs plot.\nAre there significant differences in body mass\n\n(penguins_f_massbyspecies &lt;- penguins %&gt;%\n  drop_na(body_mass_g, species, sex) %&gt;%\n  filter(sex == \"female\") %&gt;%\n  ggplot(aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(outlier.shape = NA,\n              draw_quantiles = 0.5) + # add the median by drawing 50% quantile\n  ggdist::geom_dots(side = \"both\", color = \"black\", alpha = 0.8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Penguin Species\",\n       y = \"Body Mass (g)\",\n       title = \"Body mass of female penguins by species\",\n       subtitle = \"Collected from Palmer Station, Antarctica\",\n       caption = \"Data accessed from the R package palmerpenguins\"))\n\nWarning in geom_violin(outlier.shape = NA, draw_quantiles = 0.5): Ignoring\nunknown parameters: `outlier.shape`\n\n\n\n\n\n\nTesting assumptions\n\nNormality\n\n# testing normality by group\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  filter(sex == \"female\") %&gt;%\n  group_by(species) %&gt;% # test by species\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 3 √ó 4\n  species   variable    statistic     p\n  &lt;fct&gt;     &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    body_mass_g     0.977 0.199\n2 Chinstrap body_mass_g     0.963 0.306\n3 Gentoo    body_mass_g     0.981 0.511\n\n# testing normality across all data\npenguins %&gt;%\n  drop_na(body_mass_g, sex) %&gt;% # remove NAs\n  filter(sex == \"female\") %&gt;%\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 1 √ó 3\n  variable    statistic            p\n  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 body_mass_g     0.919 0.0000000616\n\n\nOk looks like we have normally distributed data among the different species of female penguins.\n\n\nConstant variance\nlevene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\n\npenguins %&gt;%\n  drop_na(body_mass_g, sex, species) %&gt;% # remove NAs\n  filter(sex == \"female\") %&gt;%\n  levene_test(body_mass_g ~ species) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2   162    0.0357 0.965\n\n\nWe have constant variance. Along with normally distributed data, this means that we can use parametric tests. In the case of &gt;2 samples, that would be ANOVA.\n\n\n\nANOVA\nThe most commonly used function to run ANOVA in R is called aov() which is a part of the stats package that is pre-loaded with base R. So no new packages need to be installed here.\nIf we want to learn more about the function aov() we can do so using the code below. The help documentation will show up in the bottom right quadrant of your RStudio.\n\n?aov()\n\nWe can run an ANOVA by indicating our model, and here I‚Äôm also selecting to drop the NAs for our variables of interest, and filtering within the data = argument.\n\naov_female_massbyspecies &lt;- \n  aov(data = penguins %&gt;% \n             filter(sex == \"female\") %&gt;%\n             drop_na(body_mass_g, species),\n      body_mass_g ~ species)\n\nNow lets look at the aov object.\n\nsummary(aov_female_massbyspecies)\n\n             Df   Sum Sq  Mean Sq F value Pr(&gt;F)    \nspecies       2 60350016 30175008   393.2 &lt;2e-16 ***\nResiduals   162 12430757    76733                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can take the output of our ANOVA and use the function tidy() within the broom package to turn our output into a tidy table. Here, the notation broom::tidy() means I want to use the function tidy() that is a part of the broom package. This works even though I haven‚Äôt called library(broom) at the beginning of my script.\n\ntidy_anova &lt;- broom::tidy(aov_female_massbyspecies)\n\nknitr::kable(tidy_anova)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nspecies\n2\n60350016\n30175008.01\n393.2465\n0\n\n\nResiduals\n162\n12430757\n76733.07\nNA\nNA\n\n\n\n\n\nSee how this is different from just saving the ANOVA summary? Open both anova_summary and tidy_anova and note the differences.\n\nanova_summary &lt;- summary(aov_female_massbyspecies)\n\n\n\nPosthoc group analysis\nNow that we see we have a significant difference somewhere in the body mass of the 3 species of female penguins, we can do a posthoc test to see which groups are significantly different. We will do our post-hoc analysis using Tukey‚Äôs Honestly Significant Difference test and the function HSD.test() which is a part of the useful package agricolae.\n\ntukey_massbyspecies &lt;- HSD.test(aov_female_massbyspecies, \n                      trt = \"species\", \n                      console = TRUE) # prints the results to console\n\n\nStudy: aov_female_massbyspecies ~ \"species\"\n\nHSD Test for body_mass_g \n\nMean Square Error:  76733.07 \n\nspecies,  means\n\n          body_mass_g      std  r       se  Min  Max    Q25  Q50     Q75\nAdelie       3368.836 269.3801 73 32.42126 2850 3900 3175.0 3400 3550.00\nChinstrap    3527.206 285.3339 34 47.50637 2700 4150 3362.5 3550 3693.75\nGentoo       4679.741 281.5783 58 36.37285 3950 5200 4462.5 4700 4875.00\n\nAlpha: 0.05 ; DF Error: 162 \nCritical Value of Studentized Range: 3.345258 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n          body_mass_g groups\nGentoo       4679.741      a\nChinstrap    3527.206      b\nAdelie       3368.836      c\n\n\nLike we did with the aov object, you can also look at the resulting HSD.test object (here, tukey_massbyspecies) in your environment pane.\nHere, instead of using the broom package, you can convert the part of the tukey_bill_length object that contains the post-hoc groupings into a dataframe using as.data.frame().\n\ntidy_tukey &lt;- as.data.frame(tukey_massbyspecies$groups)\n\ntidy_tukey\n\n          body_mass_g groups\nGentoo       4679.741      a\nChinstrap    3527.206      b\nAdelie       3368.836      c\n\n\n\n\nPlot\n\nUsing stat_compare_means()\n\npenguins_f_massbyspecies +\n  stat_compare_means()\n\n\n\npenguins_f_massbyspecies +\n  stat_compare_means(method = \"anova\")\n\n\n\n\n\n\nManually with geom_text() or annotate()\nIn general, plotting using geom_text() is easier, and follows classic geom_() syntax (e.g., includes aes()) but for some reason these don‚Äôt pass as vectorized objects so sometimes it yields low quality images. Using annotate() passes as vectors and thus tends to be higher quality. You can decide which you want to use depending on your purpose.\nWe want to add the letters to this plot, so we can tell which groups of penguin species are significantly different.\nBefore we can do this, we will need to do some of everyone‚Äôs favorite task, wrangling. We are going to figure out what the maximum body_mass_g for each species is, so it will help us determine where to put our letter labels. Then, we can add our labels to be higher than the largest data point. We will calculate this for each group, so that the letters are always right about our boxplot.\n\nbody_mass_max &lt;- penguins %&gt;%\n  filter(sex == \"female\") %&gt;%\n  drop_na(body_mass_g, species) %&gt;%\n  group_by(species) %&gt;%\n  summarize(max_body_mass = max(body_mass_g))\n\nbody_mass_max\n\n# A tibble: 3 √ó 2\n  species   max_body_mass\n  &lt;fct&gt;             &lt;int&gt;\n1 Adelie             3900\n2 Chinstrap          4150\n3 Gentoo             5200\n\n\nLet‚Äôs add our post-hoc group info to body_mass_max, since those two dataframes are not in the same order. Instead of binding the two dataframes together, we are going to join them using one of the dplyr _join() functions, which allows you to combine dataframes based on a specific common column. The join functions work like this:\n\ninner_join(): includes all rows in x and y.\nleft_join(): includes all rows in x.\nright_join(): includes all rows in y.\nfull_join(): includes all rows in x or y.\n\nIn this case, it doesn‚Äôt matter which _join() we use because our dfs all have the exact same rows.\n\ntidier_tukey &lt;- tidy_tukey %&gt;%\n  rownames_to_column() %&gt;% # converts rownames to columns\n  rename(species = rowname) # renames the column now called rowname to species\n  \n# join\nbody_mass_for_plotting &lt;- full_join(tidier_tukey, body_mass_max,\n                               by = \"species\")\n\nLet‚Äôs plot. First using geom_text()\n\npenguins_f_massbyspecies +\n  geom_text(data = body_mass_for_plotting,\n            aes(x = species,\n                y = 175 + max_body_mass, \n                label = groups))\n\n\n\n\nNext using annotate().\n\npenguins_f_massbyspecies +\n  annotate(geom = \"text\",\n           x = c(3,2,1),\n           y = 175 + body_mass_for_plotting$max_body_mass,\n           label = body_mass_for_plotting$groups)"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#useful-resources",
    "href": "modules/module3/09_add-stats/09_add-stats.html#useful-resources",
    "title": "Annotating Statistics onto Plots",
    "section": "Useful resources",
    "text": "Useful resources\nThere have been previous Code Club sessions about adding statistics to plots:\n\nggpubr to add stats to plots by Daniel Quiroz\nt-tests in R by Mike Sovic\nRunning ANOVA in R and accesing output\nTesting ANOVA assumptions"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html",
    "href": "modules/module4/10_pca/10_pca.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Today we are going to start Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\nPCA is a data reduction approach, and useful if you have many variables, for example, thousands of genes or metabolites. PCA creates summary variables (the principal components) which maximize the variation in the dataset. It can be categorized as an unsupervised approach, as PCA doesn‚Äôt know which samples belong to your different groups. When you look at a scores plot, points that are closer together are more similar based on your input data, and those further apart are more different. The location of the loadings helps you understand what is driving those differences in your scores plot.\nIf you are unfamiliar with PCA, I‚Äôd recommend these two Youtube videos by Josh Starmer of StatQuest which explain PCA in 5 mins, or with more detail in 20 min. Bam üí•!\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\n\n\n\nToday we are going to continue to use the same real research data from my group from the lessons on distributions and correlations. We will be reading in the supplementary data from a paper from my group written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 3. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloid_blups &lt;- read_excel(\"data/tpg220192-sup-0002-supmat.xlsx\",\n                             sheet = \"S3 BLUP Diversity Panel\")\n\nLet‚Äôs take a look at this new data sheet.\n\nknitr::kable(head(alkaloid_blups))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenotype\nPlot_Source\nSpecies\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nDehydrotomatidine\nTomatidine\nDehydrotomatineA1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\nLatitude\nLongitude\n\n\n\n\nCULBPT_05_11\n2K9-8584\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n0.0001504\n0.0043495\n0.0072241\n-0.0010738\n0.0061503\n0.1929106\n0.0102521\n0.1596566\n0.0352666\n0.0864042\n0.2917962\n-0.0680508\n-0.1242448\n0.0117481\n-0.1805475\n-0.0369856\n0.0056573\n-0.1693034\n-0.1636462\n-0.0340685\n-0.0061713\n-0.0371543\n-0.0773942\n0.0365648\n40.712800000000001\n-74.006\n\n\nCULBPT_05_15\n2K9-8622\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n-0.0001716\n0.0086547\n0.0000000\n0.0086547\n0.2824926\n0.0006910\n0.1118818\n0.0243950\n0.0189345\n0.1559021\n0.0038232\n0.0467380\n0.0024737\n0.0530349\n0.0066251\n0.0125334\n0.3417664\n0.3542998\n0.0138429\n0.0021833\n0.0139309\n0.0299571\n0.8907677\n40.712800000000001\n-74.006\n\n\nCULBPT_05_22\n2K17-7708-1\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000649\n-0.0002516\n0.0085205\n0.0000000\n0.0085205\n0.1755250\n-0.0001804\n0.0505547\n0.0040900\n0.0023229\n0.0576202\n0.0078240\n0.0069610\n-0.0001145\n0.0146705\n0.0031468\n0.0150674\n0.3840689\n0.3991363\n0.0007128\n0.0008227\n0.0028133\n0.0043489\n0.6618186\n40.712800000000001\n-74.006\n\n\nCULBPT04_1\n2K9-8566\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n0.0001259\n-0.0038737\n0.0000000\n-0.0038737\n-0.0061446\n0.0015930\n0.1059454\n0.0195829\n0.0083566\n0.1354779\n0.0098585\n0.0095344\n0.0007941\n0.0201870\n0.0049377\n0.0100416\n0.3466282\n0.3566699\n0.0100830\n0.0006432\n0.0089015\n0.0196277\n0.5269806\n40.712800000000001\n-74.006\n\n\nE6203\n2K9-8600\nProcessing\nCultivated Processing\nUSA\nCA\nSLL_processing_1\nSLL_processing_1_1\nSLL\nSLL_processing_CA\nArid\n-0.0000272\n0.0000159\n0.0099538\n0.0000000\n0.0099538\n0.2929606\n0.0000000\n0.0227075\n0.0039160\n0.0088279\n0.0354514\n0.0002365\n0.0395374\n0.0041794\n0.0439532\n0.0027439\n0.0133225\n0.3018732\n0.3151958\n0.0080894\n-0.0003574\n0.0100045\n0.0177365\n0.7179839\n36.778300000000002\n-119.4179\n\n\nF06-2041\n2K16-9843\nProcessing\nCultivated Processing\nUSA\nOH\nSLL_processing_1\nSLL_processing_1_3\nSLL\nSLL_processing_OH\nHumid\n-0.0000272\n-0.0001648\n-0.0011236\n0.0000000\n-0.0011236\n0.0169681\n0.0011237\n0.0195573\n-0.0016308\n-0.0016069\n0.0188454\n-0.0004237\n0.0271524\n-0.0005286\n0.0262002\n0.0053084\n0.0049033\n0.1647694\n0.1696728\n-0.0006298\n-0.0003074\n0.0019315\n0.0009943\n0.2352713\n40.417299999999997\n-82.9071\n\n\n\n\n\nWhat are the dimensions of this dataframe?\n\ndim(alkaloid_blups)\n\n[1] 107  37\n\n\n\n\n\nHere we have the best linear unbiased predictors (BLUPs) representing the alkaloid content of 107 genotypes of tomatoes. There is extra meta-data here we won‚Äôt use, so like we did in correlations, we are going to create a vector to indicate which column name reprents the alkaloids we want to include in our principal components analysis. Then we can create a new trimmed dataframe.\n\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\nalkaloid_blups_trim &lt;- alkaloid_blups |&gt;\n  select(Genotype, Species, Class, all_of(alkaloid_total_names))\n\n# did it work?\ncolnames(alkaloid_blups_trim) # yes\n\n [1] \"Genotype\"                                \n [2] \"Species\"                                 \n [3] \"Class\"                                   \n [4] \"Dehydrotomatidine\"                       \n [5] \"Tomatidine\"                              \n [6] \"TotalDehydrotomatine\"                    \n [7] \"Tomatine\"                                \n [8] \"TotalHydroxytomatine\"                    \n [9] \"TotalAcetoxytomatine\"                    \n[10] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[11] \"TotalLycoperosideFGEsculeosideA\"         \n[12] \"TotalEsculeosideB\"                       \n[13] \"Total\""
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#introduction",
    "href": "modules/module4/10_pca/10_pca.html#introduction",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Today we are going to start Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\nPCA is a data reduction approach, and useful if you have many variables, for example, thousands of genes or metabolites. PCA creates summary variables (the principal components) which maximize the variation in the dataset. It can be categorized as an unsupervised approach, as PCA doesn‚Äôt know which samples belong to your different groups. When you look at a scores plot, points that are closer together are more similar based on your input data, and those further apart are more different. The location of the loadings helps you understand what is driving those differences in your scores plot.\nIf you are unfamiliar with PCA, I‚Äôd recommend these two Youtube videos by Josh Starmer of StatQuest which explain PCA in 5 mins, or with more detail in 20 min. Bam üí•!\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\n\n\n\nToday we are going to continue to use the same real research data from my group from the lessons on distributions and correlations. We will be reading in the supplementary data from a paper from my group written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 3. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloid_blups &lt;- read_excel(\"data/tpg220192-sup-0002-supmat.xlsx\",\n                             sheet = \"S3 BLUP Diversity Panel\")\n\nLet‚Äôs take a look at this new data sheet.\n\nknitr::kable(head(alkaloid_blups))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenotype\nPlot_Source\nSpecies\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nDehydrotomatidine\nTomatidine\nDehydrotomatineA1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\nLatitude\nLongitude\n\n\n\n\nCULBPT_05_11\n2K9-8584\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n0.0001504\n0.0043495\n0.0072241\n-0.0010738\n0.0061503\n0.1929106\n0.0102521\n0.1596566\n0.0352666\n0.0864042\n0.2917962\n-0.0680508\n-0.1242448\n0.0117481\n-0.1805475\n-0.0369856\n0.0056573\n-0.1693034\n-0.1636462\n-0.0340685\n-0.0061713\n-0.0371543\n-0.0773942\n0.0365648\n40.712800000000001\n-74.006\n\n\nCULBPT_05_15\n2K9-8622\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n-0.0001716\n0.0086547\n0.0000000\n0.0086547\n0.2824926\n0.0006910\n0.1118818\n0.0243950\n0.0189345\n0.1559021\n0.0038232\n0.0467380\n0.0024737\n0.0530349\n0.0066251\n0.0125334\n0.3417664\n0.3542998\n0.0138429\n0.0021833\n0.0139309\n0.0299571\n0.8907677\n40.712800000000001\n-74.006\n\n\nCULBPT_05_22\n2K17-7708-1\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000649\n-0.0002516\n0.0085205\n0.0000000\n0.0085205\n0.1755250\n-0.0001804\n0.0505547\n0.0040900\n0.0023229\n0.0576202\n0.0078240\n0.0069610\n-0.0001145\n0.0146705\n0.0031468\n0.0150674\n0.3840689\n0.3991363\n0.0007128\n0.0008227\n0.0028133\n0.0043489\n0.6618186\n40.712800000000001\n-74.006\n\n\nCULBPT04_1\n2K9-8566\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n0.0001259\n-0.0038737\n0.0000000\n-0.0038737\n-0.0061446\n0.0015930\n0.1059454\n0.0195829\n0.0083566\n0.1354779\n0.0098585\n0.0095344\n0.0007941\n0.0201870\n0.0049377\n0.0100416\n0.3466282\n0.3566699\n0.0100830\n0.0006432\n0.0089015\n0.0196277\n0.5269806\n40.712800000000001\n-74.006\n\n\nE6203\n2K9-8600\nProcessing\nCultivated Processing\nUSA\nCA\nSLL_processing_1\nSLL_processing_1_1\nSLL\nSLL_processing_CA\nArid\n-0.0000272\n0.0000159\n0.0099538\n0.0000000\n0.0099538\n0.2929606\n0.0000000\n0.0227075\n0.0039160\n0.0088279\n0.0354514\n0.0002365\n0.0395374\n0.0041794\n0.0439532\n0.0027439\n0.0133225\n0.3018732\n0.3151958\n0.0080894\n-0.0003574\n0.0100045\n0.0177365\n0.7179839\n36.778300000000002\n-119.4179\n\n\nF06-2041\n2K16-9843\nProcessing\nCultivated Processing\nUSA\nOH\nSLL_processing_1\nSLL_processing_1_3\nSLL\nSLL_processing_OH\nHumid\n-0.0000272\n-0.0001648\n-0.0011236\n0.0000000\n-0.0011236\n0.0169681\n0.0011237\n0.0195573\n-0.0016308\n-0.0016069\n0.0188454\n-0.0004237\n0.0271524\n-0.0005286\n0.0262002\n0.0053084\n0.0049033\n0.1647694\n0.1696728\n-0.0006298\n-0.0003074\n0.0019315\n0.0009943\n0.2352713\n40.417299999999997\n-82.9071\n\n\n\n\n\nWhat are the dimensions of this dataframe?\n\ndim(alkaloid_blups)\n\n[1] 107  37\n\n\n\n\n\nHere we have the best linear unbiased predictors (BLUPs) representing the alkaloid content of 107 genotypes of tomatoes. There is extra meta-data here we won‚Äôt use, so like we did in correlations, we are going to create a vector to indicate which column name reprents the alkaloids we want to include in our principal components analysis. Then we can create a new trimmed dataframe.\n\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\nalkaloid_blups_trim &lt;- alkaloid_blups |&gt;\n  select(Genotype, Species, Class, all_of(alkaloid_total_names))\n\n# did it work?\ncolnames(alkaloid_blups_trim) # yes\n\n [1] \"Genotype\"                                \n [2] \"Species\"                                 \n [3] \"Class\"                                   \n [4] \"Dehydrotomatidine\"                       \n [5] \"Tomatidine\"                              \n [6] \"TotalDehydrotomatine\"                    \n [7] \"Tomatine\"                                \n [8] \"TotalHydroxytomatine\"                    \n [9] \"TotalAcetoxytomatine\"                    \n[10] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[11] \"TotalLycoperosideFGEsculeosideA\"         \n[12] \"TotalEsculeosideB\"                       \n[13] \"Total\""
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#run-pca",
    "href": "modules/module4/10_pca/10_pca.html#run-pca",
    "title": "Principal Components Analysis",
    "section": "Run PCA",
    "text": "Run PCA\nThere are many packages that have functions that run PCA (including ) but I think the most common function used is a part of base R, and is called prcomp().\n\n\n\n\n\n\nWarning\n\n\n\nNote, PCA will allow zeroes, but will throw an error if you feed it NAs.\n\n\n\nalkaloids_pca &lt;- prcomp(alkaloid_blups_trim |&gt; select(all_of(alkaloid_total_names)),\n                        scale = TRUE, # default is false\n                        center = TRUE) # default is true, just being explicit\n\nLet‚Äôs investigate alkaloids_pca.\n\nglimpse(alkaloids_pca)\n\nList of 5\n $ sdev    : num [1:10] 1.794 1.732 1.215 0.99 0.776 ...\n $ rotation: num [1:10, 1:10] 0.162 0.309 0.422 0.326 0.311 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n  .. ..$ : chr [1:10] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:10] 0.000169 0.00721 0.142798 1.865975 1.323755 ...\n  ..- attr(*, \"names\")= chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n $ scale   : Named num [1:10] 0.000505 0.024096 0.339379 5.889986 2.91824 ...\n  ..- attr(*, \"names\")= chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n $ x       : num [1:107, 1:10] -1.51 -1.5 -1.55 -1.55 -1.52 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:10] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\n\nprint(alkaloids_pca)\n\nStandard deviations (1, .., p=10):\n [1] 1.7944141686 1.7318715307 1.2151400892 0.9904494488 0.7763914595\n [6] 0.6695313752 0.4394142511 0.2041300753 0.1932182462 0.0001503975\n\nRotation (n x k) = (10 x 10):\n                                               PC1         PC2         PC3\nDehydrotomatidine                        0.1621938  0.05363402 -0.06746652\nTomatidine                               0.3088504 -0.13094437 -0.44243776\nTotalDehydrotomatine                     0.4222731 -0.30216776 -0.22116400\nTomatine                                 0.3263804 -0.28919346 -0.43424991\nTotalHydroxytomatine                     0.3111090 -0.07515005  0.42780515\nTotalAcetoxytomatine                     0.3190534 -0.17396967  0.54287286\nDehydrolycoperosideFGdehydroesculeosideA 0.2125680  0.49447329 -0.07346836\nTotalLycoperosideFGEsculeosideA          0.2130280  0.52056383 -0.08463631\nTotalEsculeosideB                        0.1864604  0.50165801 -0.10122926\nTotal                                    0.5191805  0.04443498  0.24834278\n                                                 PC4          PC5         PC6\nDehydrotomatidine                         0.92897283 -0.275272461 -0.09845544\nTomatidine                                0.13150651  0.482582095  0.65879800\nTotalDehydrotomatine                     -0.16555038 -0.170452131 -0.15777601\nTomatine                                 -0.13638091 -0.163532836 -0.41927007\nTotalHydroxytomatine                      0.15203796  0.706002906 -0.41591986\nTotalAcetoxytomatine                     -0.05462895 -0.326082234  0.41704857\nDehydrolycoperosideFGdehydroesculeosideA -0.16278360  0.013643330 -0.06919125\nTotalLycoperosideFGEsculeosideA          -0.09078436 -0.009476836 -0.02310538\nTotalEsculeosideB                         0.02495982 -0.037371129  0.01529154\nTotal                                    -0.11066018 -0.170588385  0.05600981\n                                                 PC7         PC8          PC9\nDehydrotomatidine                         0.13097525 -0.02201625 -0.012909093\nTomatidine                                0.03505366  0.05778857  0.054315300\nTotalDehydrotomatine                      0.03480557 -0.61023340 -0.475709677\nTomatine                                 -0.09303946  0.40353582  0.403874861\nTotalHydroxytomatine                     -0.04702814 -0.04023463  0.009165434\nTotalAcetoxytomatine                     -0.01377758 -0.02258828  0.182475550\nDehydrolycoperosideFGdehydroesculeosideA  0.61727735 -0.31823996  0.437114636\nTotalLycoperosideFGEsculeosideA           0.10012665  0.42335087 -0.583833541\nTotalEsculeosideB                        -0.76029219 -0.28461575  0.203734859\nTotal                                    -0.01570365  0.31194871 -0.025467058\n                                                  PC10\nDehydrotomatidine                        -0.0001604947\nTomatidine                                0.0010655927\nTotalDehydrotomatine                      0.0147128823\nTomatine                                  0.2559833151\nTotalHydroxytomatine                      0.1268345182\nTotalAcetoxytomatine                      0.5059522400\nDehydrolycoperosideFGdehydroesculeosideA  0.0080094872\nTotalLycoperosideFGEsculeosideA           0.3707973486\nTotalEsculeosideB                         0.0226436777\nTotal                                    -0.7239562737\n\n\n\nclass(alkaloids_pca)\n\n[1] \"prcomp\"\n\n\nWe can see that the resulting PCA object is a prcomp object, and is a list of 5 lists and vectors.\nThis includes:\n\nsdev: the standard deviations (square roots of the eigenvalues of the covariance matrix) of the principal components\nrotation: the PCs for the variables (i.e., the variable loadings)\nx: the PCs for samples (i.e., the scores)\ncenter: the centering used\nscale: the scaling used\n\nWe can also look at the output of our PCA in a different way using the function summary().\n\nsummary(alkaloids_pca) \n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.794 1.7319 1.2151 0.9904 0.77639 0.66953 0.43941\nProportion of Variance 0.322 0.2999 0.1477 0.0981 0.06028 0.04483 0.01931\nCumulative Proportion  0.322 0.6219 0.7696 0.8677 0.92796 0.97279 0.99210\n                           PC8     PC9      PC10\nStandard deviation     0.20413 0.19322 0.0001504\nProportion of Variance 0.00417 0.00373 0.0000000\nCumulative Proportion  0.99627 1.00000 1.0000000\n\n\nWe can convert this summary into something later usable by extraction the element importance from summary(alkaloids_pca) and converting it to a dataframe.\n\nimportance &lt;- summary(alkaloids_pca)$importance |&gt;\n  as.data.frame()\n\nknitr::kable(head(importance))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\n\n\n\n\nStandard deviation\n1.794414\n1.731872\n1.21514\n0.9904494\n0.7763915\n0.6695314\n0.4394143\n0.2041301\n0.1932182\n0.0001504\n\n\nProportion of Variance\n0.321990\n0.299940\n0.14766\n0.0981000\n0.0602800\n0.0448300\n0.0193100\n0.0041700\n0.0037300\n0.0000000\n\n\nCumulative Proportion\n0.321990\n0.621930\n0.76959\n0.8676900\n0.9279600\n0.9727900\n0.9921000\n0.9962700\n1.0000000\n1.0000000\n\n\n\n\n\nBy looking at the summary we can see, for example, that the first two PCs explain 62.19% of variance.\nWe are going to go over making scree, scores and loadings plots using helper functions (here, they start fviz_() and come from the package factoextra, and manually via ggplot. The helper functions allow you look at each plot type simply. This is an important step because when you make your plots with ggplot, you want to be sure they look how they should."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#scree-plot",
    "href": "modules/module4/10_pca/10_pca.html#scree-plot",
    "title": "Principal Components Analysis",
    "section": "Scree plot",
    "text": "Scree plot\nA scree plot shows what percentage of total variance is explained by each principal component.\n\nUsing fviz_eig()\nWe can do this quickly using fviz_eig().\n\nfviz_eig(alkaloids_pca)\n\n\n\n\nYou can actually do this very easily with base R plotting as well. If you weren‚Äôt planning to publish this type of plot, it might not be important it look beautiful, and then both of these options would be great and quick. Note though that the base R plot is plotting at a different scale.\n\nplot(alkaloids_pca)\n\n\n\n\n\n\nManually\nIf you wanted to make a scree plot manually, you could by plotting using a wrangled version of the importance dataframe we made earlier.\n\nimportance_tidy &lt;- importance |&gt;\n  rownames_to_column(var = \"measure\") |&gt;\n  pivot_longer(cols = PC1:PC10,\n               names_to = \"PC\",\n               values_to = \"value\")\n\nimportance_tidy |&gt;\n  filter(measure == \"Proportion of Variance\") |&gt;\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col()\n\n\n\n\nAlmost! PC10 is displaying right after PC1 because alphabetically, this is the order. Let‚Äôs fix it.\n\n# create a vector with the order we want\nmy_order &lt;- colnames(importance)\n\n# relevel according to my_order\nimportance_tidy$PC &lt;- factor(importance_tidy$PC, levels = my_order)\n\n# check to see if it worked\nlevels(importance_tidy$PC)\n\n [1] \"PC1\"  \"PC2\"  \"PC3\"  \"PC4\"  \"PC5\"  \"PC6\"  \"PC7\"  \"PC8\"  \"PC9\"  \"PC10\"\n\n\nLet‚Äôs plot again.\n\nimportance_tidy |&gt;\n  filter(measure == \"Proportion of Variance\") |&gt;\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col()\n\n\n\n\nSuccess!\nIf we want to tighten up this plot we can.\n\nimportance_tidy |&gt;\n  filter(measure == \"Proportion of Variance\") |&gt;\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col(alpha = 0.1, color = \"black\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  labs(x = \"Principal component\",\n       y = \"Percent variance explained\",\n       title = \"Scree plot of 10 alkaloids analyzed across 107 tomato accessions\")\n\n\n\n\nThis is a perfectly ready scree plot for the supplementary materials of a publication."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#scores-plot",
    "href": "modules/module4/10_pca/10_pca.html#scores-plot",
    "title": "Principal Components Analysis",
    "section": "Scores plot",
    "text": "Scores plot\nWhen people talk about PCA plots, what they most often mean is PCA scores plots. Here, each point represents a sample, and we are plotting their coordinates typically for the first 2 PCs. Sometimes people make 3D PCA plots with the first 3 PCs but I think these are not easy to look in 2D and I wouldn‚Äôt recommend you to put them in your papers.\n\nUsing fviz_pca_ind()\nWe can also look at a scores plot using fviz_pca_ind() where ind means individuals. Here, each point is a sample.\n\nfviz_pca_ind(alkaloids_pca)\n\n\n\n\nBecause our alkaloids_pca doesn‚Äôt have any meta-data, this is a hard to interpret plot, where each number indicates the rownumber of that sample. Making the scores plot this way is useful because it shows us the shape of the plot which we can use to confirm that we have made a ggplot that looks like its been created correctly.\n\n\nManually\nWe want to plot the scores, which are in provided in alkaloids_pca$x.\nWe can convert the list into a dataframe of scores values by using as.data.frame(). Then we can bind back our relevant metadata so they‚Äôre all together. Note, to use bind_cols() both datasets need to be in the same order. In this case they are so we are good.\n\n# create a df of alkaloids_pca$x\nscores_raw &lt;- as.data.frame(alkaloids_pca$x)\n\n# bind meta-data\nscores &lt;- bind_cols(alkaloid_blups |&gt; select(Genotype, Plot_Source, Species), # metadata\n                    scores_raw)\n\n# how does our new df look?\nscores[1:6, 1:6]\n\n# A tibble: 6 √ó 6\n  Genotype     Plot_Source Species      PC1    PC2     PC3\n  &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 CULBPT_05_11 2K9-8584    Processing -1.51 -1.17  0.00632\n2 CULBPT_05_15 2K9-8622    Processing -1.50 -0.915 0.0649 \n3 CULBPT_05_22 2K17-7708-1 Processing -1.55 -0.942 0.0658 \n4 CULBPT04_1   2K9-8566    Processing -1.55 -0.906 0.0817 \n5 E6203        2K9-8600    Processing -1.52 -0.940 0.0436 \n6 F06-2041     2K16-9843   Processing -1.58 -0.934 0.0677 \n\n\nNow we can plot.\n\nscores |&gt;\n  ggplot(aes(x = PC1, y = PC2, color = Species)) +\n  geom_point() \n\n\n\n\nOur shapes are looking the same, this is good. Let‚Äôs pretty up our plot.\nFirst let‚Äôs wrangle.\n\n# create objects indicating percent variance explained by PC1 and PC2\nPC1_percent &lt;- round((importance[2,1])*100, # index 2nd row, 1st column, times 100\n                     1) # round to 1 decimal\nPC2_percent &lt;- round((importance[2,2])*100, 1) \n\n# make Species a factor and set levels\n# from least wild to most wild\n# we did this back in data distributions.\nscores$Species &lt;- factor(scores$Species,\n                         levels = c(\"Processing\",\n                                    \"Cultivated Cherry\",\n                                    \"Hybrid\",\n                                    \"Wild Cherry\",\n                                    \"Wild\"))\n\nThen we can plot\n\n# plot\n(scores_plot &lt;- scores |&gt;\n  ggplot(aes(x = PC1, y = PC2, fill = Species)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\", size = 2.5, alpha = 0.7) +\n   # sequential color selecting hex codes from YlOrRd color by wildness\n  scale_fill_manual(values = c(\"#ffffcc\", \"#fed976\", \"#fd8d3c\", \"#e31a1c\", \"#800026\")) + \n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Scores Plot of 10 Alkaloids Present in 107 Tomato Accessions\"))\n\n\n\n\nThis looks nice."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#loadings-plot",
    "href": "modules/module4/10_pca/10_pca.html#loadings-plot",
    "title": "Principal Components Analysis",
    "section": "Loadings plot",
    "text": "Loadings plot\n\nUsing fviz_pca_var()\nWe can also look at a loadings plot using fviz_pca_var() where var means variables. Here, each point is a variable.\n\nfviz_pca_var(alkaloids_pca)\n\n\n\n\n\n\nManually\nWe can also make a more customized loadings plot manually using ggplot and using the dataframe alkaloids_pca$rotation.\n\n# grab raw loadings, without any metadata\nloadings_raw &lt;- as.data.frame(alkaloids_pca$rotation)\n\nloadings &lt;- loadings_raw |&gt;\n  rownames_to_column(var = \"alkaloid\")\n\nWe can then plot with ggplot like normal.\n\nloadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_text() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot\") \n\n\n\n\nWe have two problems with this plot.\n\nThe names are abbreviated and not how we want them to appear\nThe label names are on top of the points/each other\n\nWe can fix both of these problems.\nWe can create a vector of the labels as we want them to appear, as we have done previously.\n\nalkaloid_labels &lt;- c(\"Dehydrotomatidine\",\n                     \"Tomatidine\",\n                     \"Dehydrotomatine\",\n                     \"Alpha-Tomatine\",\n                     \"Hydroxytomatine\",\n                     \"Acetoxytomatine\",\n                     \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\",\n                     \"Lycoperoside F, G, \\nor Escueloside A\",\n                     \"Escueloside B\",\n                     \"Total Steroidal Alkaloids\")\n\nThen we can re-plot with these labels.\n\nloadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +  \n  geom_text() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot\") \n\n\n\n\nOk the label names are better but they‚Äôre still smushed. The package ggrepel has some good functions to help us. You can try using geom_text_repel() and geom_label_repel().\n\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\nWith geom_text_repel()\n\nlibrary(ggrepel)\n\n(loadings_plot &lt;- loadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +  \n  geom_point() +\n  geom_text_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot using geom_text_repel()\"))\n\n\n\n\nWith geom_label_repel()\n\nloadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_label_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot using geom_label_repel()\")"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#patchwork",
    "href": "modules/module4/10_pca/10_pca.html#patchwork",
    "title": "Principal Components Analysis",
    "section": "patchwork",
    "text": "patchwork\nYou can pop these two plots side by side easing using the package patchwork.\n\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\n\nlibrary(patchwork)\n\nscores_plot + loadings_plot"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#biplot",
    "href": "modules/module4/10_pca/10_pca.html#biplot",
    "title": "Principal Components Analysis",
    "section": "Biplot",
    "text": "Biplot\n\nUsing fviz_pca().\nYou can make a biplot quickly with fviz_pca(). Note, fviz_pca_biplot() and fviz_pca() are the same.\n\nfviz_pca(alkaloids_pca)\n\n\n\n\nInstead of making this plot manually, let‚Äôs go through how to alter the existing plot made with fviz_pca(). We can do this because factoextra creates ggplot objects. To start off, we need to be using a dataframe that includes our metadata.\n\nfviz_pca(alkaloids_pca, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = \"text\") +\n  geom_point(aes(fill = alkaloid_blups$Species), shape = 21) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot Plot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       fill = \"Species\")\n\n\n\n\nThis is almost what we want - except we have only the abbreviated names for the alkaloids. Since in a biplot, we are really plotting two different sets of data (the scores and the loadings)there isn‚Äôt the ability to use labeller or similar with fviz_pca for the loadings only. There is a workaround though, we can go into our PCA object, change the rownames of alkaloids_pca$rotation to be our longer labels, and that should inherit to our new plot.\n\n# save as a new df\nalkaloids_pca_labelled &lt;- alkaloids_pca\n\n# assign alkaloid_labels to rownames\nrownames(alkaloids_pca_labelled$rotation) &lt;- alkaloid_labels\n\n# plot\nfviz_pca(alkaloids_pca_labelled, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = c(\"text\", \"point\"),\n         col.var = \"black\") +\n  geom_point(aes(fill = alkaloid_blups$Species), shape = 21) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot Plot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       fill = \"Species\")\n\n\n\n\n\n\nManually\nWe can‚Äôt add the loadings right on top of the scores because they are on different scales.\n\n# plot\nscores |&gt;\n  ggplot() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(aes(x = PC1, y = PC2, fill = Species), # from piped data\n             shape = 21, color = \"black\", size = 2.5, alpha = 0.7) +\n  geom_point(data = loadings,\n             aes(x = PC1, y = PC2)) + # set locally\n   # sequential color selecting hex codes from YlOrRd color by wildness\n  scale_fill_manual(values = c(\"#ffffcc\", \"#fed976\", \"#fd8d3c\", \"#e31a1c\", \"#800026\")) + \n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Scores Plot of 10 Alkaloids Present in 107 Tomato Accessions\")\n\n\n\n\nThe issue here is that the scores and loadings are on different scales - let‚Äôs make them on the same scale.\nI can write a quick function to allow normalization.\n\nnormalize &lt;- function(x) return((x - min(x))/(max(x) - min(x)))\n\nThen I can nornalize the scores using the scale function, since the loadings are already normalized.\n\nscores_normalized &lt;- scores |&gt;\n  mutate(PC1_norm = scale(normalize(PC1), center = TRUE, scale = FALSE)) |&gt;\n  mutate(PC2_norm = scale(normalize(PC2), center = TRUE, scale = FALSE)) |&gt;\n  select(Genotype, Plot_Source, Species, \n         PC1_norm, PC2_norm, everything()) # pick metadata and reorder\n\nHow did it go? PC1_norm and PC2_norm should all now be between -1 and 1\n\n# look at first six rows\nhead(scores_normalized)\n\n# A tibble: 6 √ó 15\n  Genotype    Plot_Source Species PC1_norm[,1] PC2_norm[,1]   PC1    PC2     PC3\n  &lt;chr&gt;       &lt;chr&gt;       &lt;fct&gt;          &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 CULBPT_05_‚Ä¶ 2K9-8584    Proces‚Ä¶       -0.201      -0.114  -1.51 -1.17  0.00632\n2 CULBPT_05_‚Ä¶ 2K9-8622    Proces‚Ä¶       -0.200      -0.0892 -1.50 -0.915 0.0649 \n3 CULBPT_05_‚Ä¶ 2K17-7708-1 Proces‚Ä¶       -0.206      -0.0918 -1.55 -0.942 0.0658 \n4 CULBPT04_1  2K9-8566    Proces‚Ä¶       -0.206      -0.0883 -1.55 -0.906 0.0817 \n5 E6203       2K9-8600    Proces‚Ä¶       -0.203      -0.0916 -1.52 -0.940 0.0436 \n6 F06-2041    2K16-9843   Proces‚Ä¶       -0.210      -0.0910 -1.58 -0.934 0.0677 \n# ‚Ñπ 7 more variables: PC4 &lt;dbl&gt;, PC5 &lt;dbl&gt;, PC6 &lt;dbl&gt;, PC7 &lt;dbl&gt;, PC8 &lt;dbl&gt;,\n#   PC9 &lt;dbl&gt;, PC10 &lt;dbl&gt;\n\n# range of PC1\nrange(scores_normalized$PC1_norm)\n\n[1] -0.2128768  0.7871232\n\n# range of PC2\nrange(scores_normalized$PC2_norm)\n\n[1] -0.4608813  0.5391187\n\n\nWe can set the scores_normalized data for the overall plot and add the loadings within another geom_point().\n\nscores_normalized |&gt;\n  ggplot() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(aes(x = PC1_norm, y = PC2_norm, fill = Species), # scores\n             shape = 21, color = \"black\", size = 2.5, alpha = 0.7) + \n  geom_point(data = loadings, # loadings points\n             aes(x = PC1, y = PC2), color = \"gray\") +\n  geom_label_repel(data = loadings, # loadings labels\n                   aes(x = PC1, y = PC2, label = alkaloid_labels),\n                   size = 2.5, # make labels smaller \n                   fill = alpha(c(\"white\"),0.5)) + # make label background transparent\n  scale_fill_manual(values = c(\"#ffffcc\", \"#fed976\", \"#fd8d3c\", \"#e31a1c\", \"#800026\")) +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       subtitle = \"Samples are colored points and loadings are grey with labels\")\n\n\n\n\nVoila."
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#useful-resources",
    "href": "modules/module4/10_pca/10_pca.html#useful-resources",
    "title": "Principal Components Analysis",
    "section": "Useful resources",
    "text": "Useful resources\n\nCode club about PCA by Jelmer Poelstra\nfactoextra\nPCA from DataCamp"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "",
    "text": "Today we are going to work with whole genome sequencing of the pig gut microbiome.\n\nPig microbiome study: paper, data."
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-rows-and-columns-do-the-data-have",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-rows-and-columns-do-the-data-have",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "How many rows and columns do the data have?",
    "text": "How many rows and columns do the data have?"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-phyla-do-the-data-contains-and-how-many-columns-represents-metadata-of-the-experiment",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-phyla-do-the-data-contains-and-how-many-columns-represents-metadata-of-the-experiment",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "How many phyla do the data contains and how many columns represents metadata of the experiment?",
    "text": "How many phyla do the data contains and how many columns represents metadata of the experiment?\n\nCreate a new column with a new phyla assignation\nKeep the phyla when they are Firmicutes or Bacteroidetes, otherwise assign Phyla to ‚ÄúOther phyla‚Äù.\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint: You may need to pivot the data to evaluate the column names as observations\n\n\n\n\n\nCompute the cumulative abundance by the new Phyla levels that you created\n\n\nCreate the bar plot in ggplot\n\n\nMake your plot interactive\n\nggplotly(your_awesome_plot)"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html",
    "href": "modules/module4/13_leftovers/13_leftovers.html",
    "title": "Leftover tidbits",
    "section": "",
    "text": "Today we are going to go over a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#introduction",
    "href": "modules/module4/13_leftovers/13_leftovers.html#introduction",
    "title": "Leftover tidbits",
    "section": "",
    "text": "Today we are going to go over a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#really-start-using-an-rproject",
    "href": "modules/module4/13_leftovers/13_leftovers.html#really-start-using-an-rproject",
    "title": "Leftover tidbits",
    "section": "Really start using an Rproject üìΩÔ∏è",
    "text": "Really start using an Rproject üìΩÔ∏è\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nI have noticed that many of you are still not using RProjects. I would really recommend that for easy file management that you do. Here is an a chapter in R for Data Science on how to set one up. If you want to start using Git in the future, you will need to set up a project."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#gghighlight",
    "href": "modules/module4/13_leftovers/13_leftovers.html#gghighlight",
    "title": "Leftover tidbits",
    "section": "gghighlight üî¶",
    "text": "gghighlight üî¶\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nThe package gghighlight allows you to highlight certain geoms in ggplot. Doing this helps your reader focus on the thing you want them to, and helps prevent plot spaghetti. To practice with gghighlight we are going to use some data from the R package gapminder\n\nInstall\n\ninstalll.packages(\"gghighlight\")\ninstall.packages(\"gapminder\")\n\n\n\nLoad libraries\nFirst let‚Äôs load our libraries.\n\nlibrary(gghighlight) # for highlighting\nlibrary(gapminder) # where data is\n\n\n\nWrangle\nWe can create a dataframe that includes only the data for the countries in the continent Americas.\n\ngapminder_americas &lt;- gapminder %&gt;%\n  filter(continent == \"Americas\")\n\n\n\nPlot\nIf we look at all the countries at once, we get plot spaghetti üçù.\n\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country, color = country)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")\n\n\n\n\nCreate a lineplot showing the life expectacy over 1952 to 2007 for all countries, highlighting the United States.\n\n# highlight just the US\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country, color = country)) +\n  geom_line() +\n  gghighlight(country == \"United States\") +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")\n\n\n\n\nFacet our plot, and highlight the country for each facet.\n\n# facet and highlight each country\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_line(aes(color = country)) +\n  gghighlight() +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.text.x = element_text(size = 8),\n        axis.text.x = element_text(angle = 90)) +\n  facet_wrap(vars(country)) +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#patchwork-a-little-more",
    "href": "modules/module4/13_leftovers/13_leftovers.html#patchwork-a-little-more",
    "title": "Leftover tidbits",
    "section": "patchwork, a little more üìàüìäüìâ",
    "text": "patchwork, a little more üìàüìäüìâ\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nWe have talked a bit about patchwork in the lecture on PCA but its such a useful package I wanted to go over it a bit more. The goal of patchwork is to make it very simple to combine plots together.\n\nLoad libraries\n\nlibrary(patchwork)\nlibrary(palmerpenguins) # for making some plots to assemble\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\n\n\n\nMake some plots\n\nplot1 &lt;- penguins %&gt;%\n  ggplot(aes(x = species, y = body_mass_g, color = species)) +\n  geom_boxplot()\n\nplot2 &lt;- penguins %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()\n\nplot3 &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = island, y = flipper_length_mm, color = species)) +\n  geom_boxplot() +\n  facet_wrap(vars(sex))\n\n\n\nCombine plots\nThe simplest ways to combine plots is with the plus sign operator +. The forward slash / stacks plots. The pipe | puts plots next to each other. You can learn more about using patchwork here.\n\n(plot1 + plot2) / plot3 \n\n\n\n\nYou can also add annotation and style to your plots. Learn more here.\n\n(plot1 + plot2) / plot3 + plot_annotation(tag_levels = c(\"1\"),\n                  title = \"Here is some information about penguins\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#gganimate",
    "href": "modules/module4/13_leftovers/13_leftovers.html#gganimate",
    "title": "Leftover tidbits",
    "section": "gganimate üíÉ",
    "text": "gganimate üíÉ\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nhttps://gganimate.com/reference/transition_states.html\n\nInstall\n\ninstall.packages(\"gganimate\") # gganimate\ninstall.packages(\"gapminder\") # gapminder data for example\ninstall.packages(\"magick\") # for gif rendering\n\n\n\nLoad libraries\n\nlibrary(gganimate)\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\n\n\nPlot\nFirst let‚Äôs make a base plot. Note that this measure of population isn‚Äôt actually correct as its summing all of the populations across all the years.\n\n(base_plot &lt;- gapminder %&gt;%\n  filter(continent == \"Africa\") %&gt;%\n  ggplot(aes(x = pop, y = reorder(country, pop))) +\n  geom_col() +\n  scale_x_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6)) +\n  theme_classic() +\n  labs(title = \"Population from 1952 to 2007 in Africa\", \n       x = \"Population\", \n       y = \"Country\"))\n\n\n\n\n\n(plot_to_animate &lt;- base_plot +\n  labs(subtitle = \"Year: {frame_time}\") + # label subtitle with year\n  transition_time(year) + # gif over year\n  ease_aes()) # makes the transitions smoother\n\n\n# set parameters for your animation\nanimated_plot &lt;- animate(plot = plot_to_animate, \n                         duration = 10, # number of seconds for whole animation\n                         fps = 10, # framerate, frames/sec\n                         start_pause = 20, # show first time for 20 frames\n                         end_pause = 20, # show end for 20 frames\n                         width = 700, # width in pixels\n                         height = 700, # height in pixels\n                         renderer = magick_renderer()) # program for rendering\n\n\n\nPrint\nPrint your animation.\n\nanimated_plot\n\n\n\n\n\n\n\n\nSave\nSave your animation.\n\n# save it\nanim_save(filename = \"gapminder_gif.gif\",\n          animation = last_animation())"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#ggradar",
    "href": "modules/module4/13_leftovers/13_leftovers.html#ggradar",
    "title": "Leftover tidbits",
    "section": "ggradar üì°",
    "text": "ggradar üì°\nThe package ggradar allows you to create radar plots, which allow the plotting of multidimensional data on a two dimension chart. Typically with these plots, the goal is to compare the variables on the plot across different groups. We are going to try this out with the coffee tasting data from the distributions recitation.\nInstall ggradar if you don‚Äôt already have it. This package is not available on CRAN for the newest version of R, so we can use devtools and install_github() to install it. You could also try using install.packages() and see if that works for you.\n\ndevtools::install_github(\"ricardo-bion/ggradar\",\n                         dependencies = TRUE)\n\n\nlibrary(ggradar)\nlibrary(scales) # for scaling data\n\n# load coffee data from distributions recitation\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n# extract out df on coffee_ratings\ncoffee &lt;- tuesdata$coffee_ratings\n\n# what are the column names again?\ncolnames(coffee)\n\n [1] \"total_cup_points\"      \"species\"               \"owner\"                \n [4] \"country_of_origin\"     \"farm_name\"             \"lot_number\"           \n [7] \"mill\"                  \"ico_number\"            \"company\"              \n[10] \"altitude\"              \"region\"                \"producer\"             \n[13] \"number_of_bags\"        \"bag_weight\"            \"in_country_partner\"   \n[16] \"harvest_year\"          \"grading_date\"          \"owner_1\"              \n[19] \"variety\"               \"processing_method\"     \"aroma\"                \n[22] \"flavor\"                \"aftertaste\"            \"acidity\"              \n[25] \"body\"                  \"balance\"               \"uniformity\"           \n[28] \"clean_cup\"             \"sweetness\"             \"cupper_points\"        \n[31] \"moisture\"              \"category_one_defects\"  \"quakers\"              \n[34] \"color\"                 \"category_two_defects\"  \"expiration\"           \n[37] \"certification_body\"    \"certification_address\" \"certification_contact\"\n[40] \"unit_of_measurement\"   \"altitude_low_meters\"   \"altitude_high_meters\" \n[43] \"altitude_mean_meters\" \n\n\nWe are going to wrangle the data to facilitate plotting. We are using rescale as we need the data for each attribute to be between 0 and 1.\n\n# tidy data to summarize easily\n(coffee_summary_long &lt;- coffee %&gt;%\n  select(species, aroma:cupper_points) %&gt;% # first column is the groups\n  pivot_longer(cols = aroma:cupper_points, # our favorite - tidy data to faciliate summarizing\n               names_to = \"attribute\",\n               values_to = \"score\") %&gt;% \n  group_by(attribute) %&gt;% # perform operations by species and attribute pairs\n  mutate(across(where(is.numeric), rescale)) %&gt;% # rescale data that is numeric\n  ungroup() |&gt; # get rid of grouping by attribute\n  group_by(species, attribute) |&gt; # group now by species and attribute\n  summarize(mean_score = mean(score)))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 √ó 3\n# Groups:   species [2]\n   species attribute     mean_score\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n 1 Arabica acidity            0.861\n 2 Arabica aftertaste         0.853\n 3 Arabica aroma              0.864\n 4 Arabica balance            0.859\n 5 Arabica body               0.876\n 6 Arabica clean_cup          0.983\n 7 Arabica cupper_points      0.750\n 8 Arabica flavor             0.851\n 9 Arabica sweetness          0.990\n10 Arabica uniformity         0.983\n11 Robusta acidity            0.875\n12 Robusta aftertaste         0.872\n13 Robusta aroma              0.880\n14 Robusta balance            0.862\n15 Robusta body               0.875\n16 Robusta clean_cup          0.993\n17 Robusta cupper_points      0.776\n18 Robusta flavor             0.864\n19 Robusta sweetness          0.768\n20 Robusta uniformity         0.990\n\n\nggradar takes wide data though, so we are going to pivot back to wide data.\n\n# go back to wide\ncoffee_summary_wide &lt;- coffee_summary_long %&gt;%\n  pivot_wider(names_from = \"attribute\",\n              values_from = \"mean_score\")\n\n\nggradar(coffee_summary_wide)\n\n\n\n\nWe are going to fix our labels and chanage some parameters on the plot to make it look nicer. You can also do this with coding (instead of manually) using the function to_any_case() in the package snakecase.\n\n# write code to get nicer looking label names\n# create a vector of our variable names\n\n# create a vector of the column names\nvariables &lt;- colnames(coffee_summary_wide)\n\n# remove the first observation (species) since we don't want that one\nvariables &lt;- variables[-1]\n\n# use the function to_any_case() from the package snakecase\n# to convert to \"sentence\" case\n# install.packages(\"snakecase\")\ncoffee_labels &lt;- snakecase::to_any_case(variables, case = \"sentence\")\n\n# how do they look?\ncoffee_labels\n\n [1] \"Acidity\"       \"Aftertaste\"    \"Aroma\"         \"Balance\"      \n [5] \"Body\"          \"Clean cup\"     \"Cupper points\" \"Flavor\"       \n [9] \"Sweetness\"     \"Uniformity\"   \n\n\nOr to do this manually\n\n# set our pretty coffee labels\n# ggradar plots in alphabetical order so that is how we will label here\ncoffee_labels &lt;- c(\"Acidity\",\n                   \"Aftertaste\",\n                   \"Aroma\",\n                   \"Balance\",\n                   \"Body\",\n                   \"Clean cup\",\n                   \"Cupper points\",\n                   \"Flavor\",\n                   \"Sweetness\",\n                   \"Uniformity\")\n\n\nggradar(coffee_summary_wide,\n        axis.labels = coffee_labels,\n        legend.position = \"bottom\",\n        axis.label.size = 3,\n        grid.label.size = 5) +\n  theme(legend.key = element_rect(fill = NA, color = NA),\n        plot.title = element_text(size = 16),\n        legend.text = element_text(size = 12)) +\n  labs(title = \"Difference in average coffee cupper score \\nin Arabica and Robusta beans\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#heatmaps",
    "href": "modules/module4/13_leftovers/13_leftovers.html#heatmaps",
    "title": "Leftover tidbits",
    "section": "Heatmaps üü•‚¨úÔ∏èüü¶",
    "text": "Heatmaps üü•‚¨úÔ∏èüü¶\n\nInstall\n\ninstall.packages(\"pheatmap\")\n\n\n\nLoad libraries\n\nlibrary(pheatmap)\n\n\n\nPlot\n\npheatmap(mtcars)\n\n\n\n\n\npheatmap(mtcars, \n         scale = \"column\",\n         cluster_rows = TRUE) # cluster rows based on similarity\n\n\n\n\n\n\nConplexHeatmap\nThe package ComplexHeatmap allows more customized and complicated heatmaps to be produced. If you are interested in making heatmaps, this package is worth to check out."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#useful-resources",
    "href": "modules/module4/13_leftovers/13_leftovers.html#useful-resources",
    "title": "Leftover tidbits",
    "section": "Useful resources",
    "text": "Useful resources\n\ngghighlight\npatchwork\ngganimate\nggradar\npheatmap\nComplexHeatmap"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html",
    "href": "modules/module4/12_manhattan/12_manhattan.html",
    "title": "Manhattan Plots",
    "section": "",
    "text": "Today we are going to continue putting it together in Module 4. Today‚Äôs material is on making Manhattan plots, which is a commonly used plot type for visualizing the result of genome wide association studies (GWAS). The name comes from its resemblance to the skyscrapers in Manhattan, poking above the background of the rest of the buildings.\n\n\n\n\n\nFigure from Wikipedia\n\n\n\n\nThe plot visualizes the relationship between a trait and genetic markers. The x-axis shows the position on each chromosome, and the y-axis shows the negative log (usually log10) p-value of the quantitative response of a trait to that specific marker. Negative log10 p-value is used because a significant p-value is always small, and this transformation converts low p-value to a number that can be seen easily among the background of non-significant associations.\nIf you work in genetics/genomics, it is likely you will create Manhattan plots. Even if you think you‚Äôll never make one of these types of plots, its a useful activity to see additional ways of customizing your plots.\n\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels\nlibrary(tidyverse) # everything\n\n\n\nToday we are going to continue to use some different real research data collected by Emma Bilbrey from my team where we conducted many GWAS in apple. This work was published in 2021 in New Phytologist and can be found here. Click on Sections and go to Supporting Information and you‚Äôll find Table S16. This data is more complex than a typical GWAS so we are only going to use a small portion of it.\nWe will be reading in Table S16 which includes the -log10 p-values for the GWAS conducted across all apples for all features found in the LC-MS negative ionization mode metabolomics dataset.\nThe data is present in a .csv file, so we will use the function read_csv() from the tidyverse. We want to import Supplemental Table 16.\nThis will take a second, its a big file.\n\ngwas &lt;- read_csv(\"data/nph17693-sup-0007-tables16.csv\") # be patient\n\n\n\nRows: 11165 Columns: 4704\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4704): Index, Linkage_Group, Genetic_Distance, X885.2037_2.98177, X525....\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat are the dimensions of this dataframe? What kind of object is it?\n\ndim(gwas)\n\n[1] 11165  4704\n\nclass(gwas)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nBecause this dataframe is so big, if we use head(gwas) we will get a print out of the first 6 rows, and all the columns. In thi case there are 4704 columns so that will be unwieldy.\nEmma came up with a simple way to approach this when she was writing her code, she wrote herself a little function that she could use regularly to extract out the first 5 rows, and the first 5 columns, without having to index each time.\nIf we wanted to just see the first 5 rows, the first 5 columns we could do this:\n\ngwas[1:5,1:5]\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\nhead_short &lt;- function(x){\n  x[1:5,1:5] # print first 5 rows and columns of an object\n  } \n\nNow instead of indexing all the time, we can just run head_short() which I think is easier. We will talk a little bit more about writing functions later today.\n\nhead_short(gwas)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\n\n\nLet me provide you a little bit of info about this data:\n\nIndex provides a unique number identifier for each SNP\nLinkage_Group indicates the chromosome\nGenetic_Distance gives you the physical genetic distance on each chromosome\nThe rest of the columns represent a metabolomic feature, and the data in each cell represents the -log10 p-values for the relationship between that SNP and that feature in the GWAS.\n\nWe will write a little code to see this better.\nHow many markers are included here?\n\nnrow(gwas)\n\n[1] 11165\n\n\nHow many linkage groups do we have? (Each linkage group is a chromosome.)\n\nunique(gwas$Linkage_Group)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n\n\nWhat is the range of Genetic_Distance and Index for each chromosome?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  dplyr::summarize(min_genetic_distance = min(Genetic_Distance),\n            max_genetic_distance = max(Genetic_Distance),\n            min_index = min(Index),\n            max_index = max(Index))\n\n# A tibble: 17 √ó 5\n   Linkage_Group min_genetic_distance max_genetic_distance min_index max_index\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1             1                0                     63.1         1       663\n 2             2                0                     78.4       664      1687\n 3             3                0                     74.0      1688      2630\n 4             4                0.004                 65.5      2635      3432\n 5             5                0                     77.8      3433      4530\n 6             6                0                     75.3      4533      5266\n 7             7                0.001                 82.4      5270      5934\n 8             8                0                     68.5      5936      6792\n 9             9                0.292                 67.0      6793      7623\n10            10                0                     81.3      7624      8648\n11            11                0                     80.9      8652      9728\n12            12                0.382                 65.4      9731     10719\n13            13                0                     71.4     10721     11558\n14            14                0                     64.4     11560     12365\n15            15                0                    112.      12366     13605\n16            16                0.001                 67.5     13610     14428\n17            17                0                     71.8     14431     15260\n\n\nOk here we can see Index does not repeat, but Genetic_Distance restarts with each chromosome."
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#introduction",
    "href": "modules/module4/12_manhattan/12_manhattan.html#introduction",
    "title": "Manhattan Plots",
    "section": "",
    "text": "Today we are going to continue putting it together in Module 4. Today‚Äôs material is on making Manhattan plots, which is a commonly used plot type for visualizing the result of genome wide association studies (GWAS). The name comes from its resemblance to the skyscrapers in Manhattan, poking above the background of the rest of the buildings.\n\n\n\n\n\nFigure from Wikipedia\n\n\n\n\nThe plot visualizes the relationship between a trait and genetic markers. The x-axis shows the position on each chromosome, and the y-axis shows the negative log (usually log10) p-value of the quantitative response of a trait to that specific marker. Negative log10 p-value is used because a significant p-value is always small, and this transformation converts low p-value to a number that can be seen easily among the background of non-significant associations.\nIf you work in genetics/genomics, it is likely you will create Manhattan plots. Even if you think you‚Äôll never make one of these types of plots, its a useful activity to see additional ways of customizing your plots.\n\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels\nlibrary(tidyverse) # everything\n\n\n\nToday we are going to continue to use some different real research data collected by Emma Bilbrey from my team where we conducted many GWAS in apple. This work was published in 2021 in New Phytologist and can be found here. Click on Sections and go to Supporting Information and you‚Äôll find Table S16. This data is more complex than a typical GWAS so we are only going to use a small portion of it.\nWe will be reading in Table S16 which includes the -log10 p-values for the GWAS conducted across all apples for all features found in the LC-MS negative ionization mode metabolomics dataset.\nThe data is present in a .csv file, so we will use the function read_csv() from the tidyverse. We want to import Supplemental Table 16.\nThis will take a second, its a big file.\n\ngwas &lt;- read_csv(\"data/nph17693-sup-0007-tables16.csv\") # be patient\n\n\n\nRows: 11165 Columns: 4704\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4704): Index, Linkage_Group, Genetic_Distance, X885.2037_2.98177, X525....\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat are the dimensions of this dataframe? What kind of object is it?\n\ndim(gwas)\n\n[1] 11165  4704\n\nclass(gwas)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nBecause this dataframe is so big, if we use head(gwas) we will get a print out of the first 6 rows, and all the columns. In thi case there are 4704 columns so that will be unwieldy.\nEmma came up with a simple way to approach this when she was writing her code, she wrote herself a little function that she could use regularly to extract out the first 5 rows, and the first 5 columns, without having to index each time.\nIf we wanted to just see the first 5 rows, the first 5 columns we could do this:\n\ngwas[1:5,1:5]\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\nhead_short &lt;- function(x){\n  x[1:5,1:5] # print first 5 rows and columns of an object\n  } \n\nNow instead of indexing all the time, we can just run head_short() which I think is easier. We will talk a little bit more about writing functions later today.\n\nhead_short(gwas)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\n\n\nLet me provide you a little bit of info about this data:\n\nIndex provides a unique number identifier for each SNP\nLinkage_Group indicates the chromosome\nGenetic_Distance gives you the physical genetic distance on each chromosome\nThe rest of the columns represent a metabolomic feature, and the data in each cell represents the -log10 p-values for the relationship between that SNP and that feature in the GWAS.\n\nWe will write a little code to see this better.\nHow many markers are included here?\n\nnrow(gwas)\n\n[1] 11165\n\n\nHow many linkage groups do we have? (Each linkage group is a chromosome.)\n\nunique(gwas$Linkage_Group)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n\n\nWhat is the range of Genetic_Distance and Index for each chromosome?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  dplyr::summarize(min_genetic_distance = min(Genetic_Distance),\n            max_genetic_distance = max(Genetic_Distance),\n            min_index = min(Index),\n            max_index = max(Index))\n\n# A tibble: 17 √ó 5\n   Linkage_Group min_genetic_distance max_genetic_distance min_index max_index\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1             1                0                     63.1         1       663\n 2             2                0                     78.4       664      1687\n 3             3                0                     74.0      1688      2630\n 4             4                0.004                 65.5      2635      3432\n 5             5                0                     77.8      3433      4530\n 6             6                0                     75.3      4533      5266\n 7             7                0.001                 82.4      5270      5934\n 8             8                0                     68.5      5936      6792\n 9             9                0.292                 67.0      6793      7623\n10            10                0                     81.3      7624      8648\n11            11                0                     80.9      8652      9728\n12            12                0.382                 65.4      9731     10719\n13            13                0                     71.4     10721     11558\n14            14                0                     64.4     11560     12365\n15            15                0                    112.      12366     13605\n16            16                0.001                 67.5     13610     14428\n17            17                0                     71.8     14431     15260\n\n\nOk here we can see Index does not repeat, but Genetic_Distance restarts with each chromosome."
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#manhattan-plot-chlorogenic-acid",
    "href": "modules/module4/12_manhattan/12_manhattan.html#manhattan-plot-chlorogenic-acid",
    "title": "Manhattan Plots",
    "section": "Manhattan plot: chlorogenic acid",
    "text": "Manhattan plot: chlorogenic acid\nAt its core, a Manhattan plot is a scatter plot. The data we are working with has 4701 traits, which here are relative metabolite abundance. We are going to pick one metabolite to start working with.\nWe will start with the feature that represents chlorogenic acid, a caffeoyl-quinic acid you find in apples. The column we want is X353.09194_2.23795. The data is already present as the -log10 p-value for the relationship between allelic variation at that marker, and relative abundance of chlorogenic acid.\n\n# rename X353.09194_2.23795 to chlorogenic_acid\ngwas &lt;- gwas %&gt;%\n  rename(chlorogenic_acid = `X353.09194_2.23795`)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point()\n\n\n\n\nSee how color is plotted on a continuous scale? This is because Linkage_Group is a continuous, numeric variable. Since each chromosome is actually discrete, let‚Äôs convert Linkage_Group to a factor and then plot again.\n\nLinkage_Group as a factor\n\n# make Linkage_Group a factor\ngwas$Linkage_Group &lt;- as.factor(gwas$Linkage_Group)\n\n# first pass manhattan plot\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point()\n\n\n\n\nBetter but this really isn‚Äôt what we want. We want our x-axis to indicate the chromosome number in the middle of the block of that chromosome, not label by Index which just is a key for linking back to each specific marker.\n\n\nSet axis\nIf we want to label the x-axis with breaks for each chromosome, we have to do some wrangling first. Just like we did some calculations in the lesson on adding statistics, we will calculate some min, center, and max for each chromosome so we know where to put the labels.\n\n(set_axis &lt;- gwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  dplyr::summarize(min = min(Index),\n            max = max(Index),\n            center = (max - min)/2))\n\n# A tibble: 17 √ó 4\n   Linkage_Group   min   max center\n   &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 1                 1   663   331 \n 2 2               664  1687   512.\n 3 3              1688  2630   471 \n 4 4              2635  3432   398.\n 5 5              3433  4530   548.\n 6 6              4533  5266   366.\n 7 7              5270  5934   332 \n 8 8              5936  6792   428 \n 9 9              6793  7623   415 \n10 10             7624  8648   512 \n11 11             8652  9728   538 \n12 12             9731 10719   494 \n13 13            10721 11558   418.\n14 14            11560 12365   402.\n15 15            12366 13605   620.\n16 16            13610 14428   409 \n17 17            14431 15260   414.\n\n\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n# set breaks and labels using set_axis  \n  scale_x_continuous(breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  theme_classic() +\n  theme(legend.position = \"none\") + # legend not really necessary\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"GWAS of chlorogenic acid in apple\")\n\n\n\n\n\n\nAlternate colors\nHaving a rainbow of colors is not really necessary here, and in fact telling exactly where chromosome 15 ends and 16 begins is difficult because the colors are so similar.\nWhat you will see in a lot of papers is people simply alternate the colors of their points by chromosome so you can easily tell which points belong to which chromosome.\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n# alternating colors by chromosome, black and darkgray\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nRemoving that annoying front gap\nThe gap between chromosome 1 and the y-axis of the plot sort of bothers me. Let‚Äôs remove it.\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(expand = c(0,0), # remove gap between y-axis and chr1\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"grey52\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nAdd p-value hline\nBecause we have so many SNPs, we are making a lot of comparisons that require a multiple-testing correction or we run the risk of an enormous number of false-positives. We can use the conservative Bonferroni correction, which takes our p-value, and the total number of comparisons we are making, creating a new adjusted value that our p-values need to be less than to be considered significant. In this case, the number of comparisons are our number of rows, in this case 11165\n\n# what would the pvalue cut off with a bonferroni correction be?\nbonferroni_pval &lt;- -log10(0.05/nrow(gwas))\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nColor sig points\nWe might want to better see the points (i.e., SNPs) that are significantly related to chlorogenic acid content by coloring them differently from the other points. Think about how you could also use this in volcano plots, or any other time you want to highlight some points on a plot.\n\n# select all SNPs with -log10 pvalue &gt; bonferroni cutoff for chlorogenic acid\nchlorogenic_acid_sig &lt;- gwas %&gt;%\n  filter(chlorogenic_acid &gt; bonferroni_pval) %&gt;%\n  select(Index, Linkage_Group, Genetic_Distance, chlorogenic_acid)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n# another geom_point layer with only the sig points and make them red\n  geom_point(data = chlorogenic_acid_sig, \n             aes(x = Index, y = chlorogenic_acid), color = \"red\") +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\nLabel most sig marker\nWe might be interested to know the marker that has the most significant association with chlorogenic acid content, and label it on our plot.\n\n# select which SNP has the smallest pvalue.\nsmallest_pval &lt;- chlorogenic_acid_sig %&gt;% \n  filter(chlorogenic_acid == max(chlorogenic_acid))\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_point(data = chlorogenic_acid_sig, \n             aes(x = Index, y = chlorogenic_acid), color = \"red\") +\n  geom_label_repel(data = smallest_pval,\n                   aes(x = Index, y = chlorogenic_acid, label = glue(\"Index: {Index}\"))) +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#investigating-other-traits",
    "href": "modules/module4/12_manhattan/12_manhattan.html#investigating-other-traits",
    "title": "Manhattan Plots",
    "section": "Investigating other traits",
    "text": "Investigating other traits\nIn this study, we conducted a series of GWAS on thousands of metabolomic features in apple. What if we wanted to see Manhattan plots for certain features based on how important we could predict they would be? For example, what if we want to see the Manhattan plot for the feature with biggest -log10p-value? Or the feature that has a significant association with the largest number of markers?\nTo make this wrangling easier, we will convert our data, as we have many times before, from wide to long with pivot_longer().\n\nWide to long (again)\n\n# pivoting, our favorite\ngwas_tidy &lt;- gwas %&gt;%\n  pivot_longer(cols = starts_with(\"X\"), # all the metabolomic features\n               names_to = \"feature\",\n               values_to = \"neg_log10_p\")\n\n# how did it go?\nhead(gwas_tidy)\n\n# A tibble: 6 √ó 6\n  Index Linkage_Group Genetic_Distance chlorogenic_acid feature      neg_log10_p\n  &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n1     1 1                            0            0.361 X885.2037_2‚Ä¶       0.176\n2     1 1                            0            0.361 X525.1583_3‚Ä¶       0.117\n3     1 1                            0            0.361 X569.17408_‚Ä¶       0.252\n4     1 1                            0            0.361 X739.17477_‚Ä¶       0.250\n5     1 1                            0            0.361 X600.12641_‚Ä¶       0.323\n6     1 1                            0            0.361 X349.0664_1‚Ä¶       0.283\n\n\n\n\nSet p-value cutoff\nWe can make another df that includes only the features that have at least one marker where there is a significant p-value.\n\n# make df of associations that pass bonferroni correction\ngwas_tidy_bonferroni &lt;- gwas_tidy %&gt;%\n  filter(neg_log10_p &gt; bonferroni_pval)\n\n# how many unique features are there?\nlength(unique(gwas_tidy_bonferroni$feature))\n\n[1] 962\n\n# how many unique markers are there?\nlength(unique(gwas_tidy_bonferroni$Index))\n\n[1] 544\n\n\nThere are 962 unique features/metabolite that have a Bonferroni adjusted significant p-value with at least one marker. There are 544 unique markers that have a Bonferroni adjusted significant p-value with at least one feature/metabolite.\n\n\nData investigating\nWhat features are associated with the largest number of markers?\n\ngwas_tidy_bonferroni %&gt;%\n  group_by(feature) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 962 √ó 2\n# Groups:   feature [962]\n   feature                n\n   &lt;chr&gt;              &lt;int&gt;\n 1 X417.13237_1.82968    46\n 2 X349.15073_1.79191    44\n 3 X601.13217_2.40546    34\n 4 X593.12835_2.53465    31\n 5 X291.0768_2.44657     30\n 6 X591.1485_2.86273     30\n 7 X637.09169_2.78692    30\n 8 X661.08791_2.10005    30\n 9 X137.02484_2.44808    29\n10 X561.13983_2.53357    29\n# ‚Ñπ 952 more rows\n\n\nWow, the marker X417.13237_1.82968 has significant associations with 46 markers. What would that Manhattan plot look like?\n\ngwas_tidy %&gt;%\n  filter(feature == \"X417.13237_1.82968\") %&gt;%\n  ggplot(aes(x = Index, y = neg_log10_p, color = Linkage_Group)) +\n  geom_point() +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for 417.13237 m/z at retention time 1.82968 in Apple\")"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#making-many-plots-at-once",
    "href": "modules/module4/12_manhattan/12_manhattan.html#making-many-plots-at-once",
    "title": "Manhattan Plots",
    "section": "Making many plots at once",
    "text": "Making many plots at once\nWhat if we want to make Manhattan plots for the 50 features/metabolites that are associated with the most markers? This is probably too many plots to facet, so we can do some calculations, write a function to make plots, and apply it over our dataframe.\nFirst, how many significant associations with a Bonferroni multiple testing correction are there?\n\n# make df of associations that pass bonferroni correction\ngwas_tidy_bonferroni &lt;- gwas_tidy %&gt;%\n  filter(neg_log10_p &gt; bonferroni_pval)\n\n# how many unique features are this?\ngwas_tidy_bonferroni %&gt;%\n  count(feature) \n\n# A tibble: 962 √ó 2\n   feature                 n\n   &lt;chr&gt;               &lt;int&gt;\n 1 X1000.22158_2.71331    12\n 2 X1000.72569_2.72017     2\n 3 X1001.23392_2.70506     5\n 4 X1008.71962_2.91507     3\n 5 X1008.72084_2.64352     2\n 6 X1009.22592_2.91262     5\n 7 X1009.72353_2.64479     2\n 8 X1010.22369_2.64604     2\n 9 X1010.72865_2.64538     1\n10 X1014.70219_2.12784     2\n# ‚Ñπ 952 more rows\n\n# how many unique markers are there?\ngwas_tidy_bonferroni %&gt;%\n  count(Index) \n\n# A tibble: 544 √ó 2\n   Index     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   170     1\n 2   217     1\n 3   218     1\n 4   233     2\n 5   294     1\n 6   311     1\n 7   341     2\n 8   368     1\n 9   386     4\n10   520     6\n# ‚Ñπ 534 more rows\n\n\nWhich features are associated with the largest number of markers?\n\ngwas_tidy_bonferroni %&gt;%\n  count(feature) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 962 √ó 2\n   feature                n\n   &lt;chr&gt;              &lt;int&gt;\n 1 X417.13237_1.82968    46\n 2 X349.15073_1.79191    44\n 3 X601.13217_2.40546    34\n 4 X593.12835_2.53465    31\n 5 X291.0768_2.44657     30\n 6 X591.1485_2.86273     30\n 7 X637.09169_2.78692    30\n 8 X661.08791_2.10005    30\n 9 X137.02484_2.44808    29\n10 X561.13983_2.53357    29\n# ‚Ñπ 952 more rows\n\n\nWhich markers are associated with the largest number of features?\n\ngwas_tidy_bonferroni %&gt;%\n  count(Index) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 544 √ó 2\n   Index     n\n   &lt;dbl&gt; &lt;int&gt;\n 1 13684   320\n 2 13685   318\n 3 13681   317\n 4 13715   298\n 5 13657   223\n 6 13660   223\n 7 13675   221\n 8 13630   219\n 9 13623   199\n10 13617   198\n# ‚Ñπ 534 more rows\n\n\nWe will make a new df that includes only the 50 features with the most makers associated with them.\n\n# create a df with only the top 50 features with the most marker associations\ntop50 &lt;- gwas_tidy_bonferroni %&gt;%\n  count(feature) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 50)\n\n# what does that look like?\nhead(top50)\n\n# A tibble: 6 √ó 2\n  feature                n\n  &lt;chr&gt;              &lt;int&gt;\n1 X417.13237_1.82968    46\n2 X349.15073_1.79191    44\n3 X601.13217_2.40546    34\n4 X593.12835_2.53465    31\n5 X291.0768_2.44657     30\n6 X591.1485_2.86273     30\n\n\nNow we can filter our dataset to only include our top 50 features\n\n# filter the whole dataset to include only top 50 features\ngwas_top50_long &lt;- gwas_tidy %&gt;%\n  filter(feature %in% top50$feature)\n\n\nWriting a function to plot\nThen we can write a function to plot, where we will iterate across feature_of_interest. Here, feature_of_interest is just the name I‚Äôve assigned here, but you could easily call it x or i or whatever.\n\n# write a function to make your plots across the features of interest\nmanhattan_plot &lt;- function(feature_of_interest){\n  gwas_top50_long %&gt;% # our df with only the top 50, but long\n  filter(feature == feature_of_interest) %&gt;% # pick the feature_of_interest only\n  ggplot(aes(x = Index, y = neg_log10_p, color = Linkage_Group)) +\n  geom_point() + \n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"gray\"),17)) +\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = glue(\"{feature_of_interest}\")) + # here we glue the feature name in the title\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5))\n  }\n\nBefore trying to use our new function manhattan_plot on 50 features, let‚Äôs try it out on one. We can provide our feature of interest as a string.\n\nmanhattan_plot(\"X599.12186_2.10421\")\n\n\n\n\nNow that our function is working for one feature, let‚Äôs create a vector of features to iterate over, and plot. I am calling that vector features_to_plot. It contains the unique feature names from gwas_top50_long.\n\nfeatures_to_plot &lt;- unique(gwas_top50_long$feature)\n\n\n\nApplying the function with map().\nOnce we have our function written, we can apply that function over a vector using purrr:map(). What map() does is take a function and apply it to a vector. In this case, the function is manhattan_plot (the function that makes a Manhattan plot), and it is applied over features_to_plot, the vector of features we want a plot for.\n\n# create an object called my_plots which is a list \n# the list contains our 50 plots\nmy_plots &lt;- map(features_to_plot, # vector to apply over\n            manhattan_plot) # what function to use\n\n# print the first 6\nmy_plots[1:6]\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\nSaving out plots\nBut you can print them all, save particular ones using ggsave(), or do what we are going to do here, which is save each of them to a new folder, each as their own .svg because why use raster when you can vectorize.\nFirst we will create a vector of what we want our file names to look like, and apply them to be the names of our plots.\n\n# use str_c to combine two character vectors\n# here, features_to_plot and adding .svg so the file name \n# includes the extension type\n# then set that as the names for my_plots\nnames(my_plots) &lt;- str_c(features_to_plot, \".svg\")\n\nThen, we will save.\n\n# use pwalk to \"walk\" across the different plots and save them\n# to use the path subfolder \"img\" you need to create that in your working dir\npwalk(list(names(my_plots), my_plots), # what to iterate over and output\n      ggsave, # what the function is\n      path = \"img/\") # where they should go\n\nNow all of your plots are in your working directory. Remember, you need to add the directory img if you want to save with the code I‚Äôm using here."
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#useful-resources",
    "href": "modules/module4/12_manhattan/12_manhattan.html#useful-resources",
    "title": "Manhattan Plots",
    "section": "Useful resources",
    "text": "Useful resources\n\npurrr:map()"
  },
  {
    "objectID": "rmds/03_Rmd_recitation.html",
    "href": "rmds/03_Rmd_recitation.html",
    "title": "RMarkdown Recitation",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation.html#r-markdown-recitation",
    "href": "rmds/03_Rmd_recitation.html#r-markdown-recitation",
    "title": "RMarkdown Recitation",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HCS 7100: Data Visualization in R, Autumn 2025",
    "section": "",
    "text": "Instructor: Jessica Cooperstone  Teaching assistant: Daniel Quiroz Moreno\n\nThis course aims to introduce students to the principles and practice of data visualization. Students will learn fundamental principles of data visualization and create figures that appropriately and ethically represent their data. Data visualizations will be created in the R programming environment, using tools including the grammar of graphics implemented in ggplot2. In the process of creating visualizations, students will also become familiar with data handling and wrangling in R.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html",
    "href": "assignments/modules/module2/module_2_assignment.html",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on R Markdown, wrangling, ggplot101, and ggplot102.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, October 7, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#introduction",
    "href": "assignments/modules/module2/module_2_assignment.html#introduction",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on R Markdown, wrangling, ggplot101, and ggplot102.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, October 7, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#writing-in-markdown-1-1-pt",
    "href": "assignments/modules/module2/module_2_assignment.html#writing-in-markdown-1-1-pt",
    "title": "Module 2 Assignment",
    "section": "Writing in Markdown 1 (1 pt)",
    "text": "Writing in Markdown 1 (1 pt)\nUsing coding in text, write a sentence in markdown that pulls from this data how many total PhDs were awarded in 2017. If you want to make some calculations in a code chunk first that is ok."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-1-3-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-1-3-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 1 (3 pts)",
    "text": "Visualization 1 (3 pts)\nMake a chart to visualize of the total number of PhDs awarded for each broad_field across the total time period of this data. You pick the type of chart that you think is appropriate, and make sure your plot is appropriately labelled and you are happy with how it looks. Hint, to do this you‚Äôll probably have to do some data wrangling first."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-2-2-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-2-2-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 2 (2 pts)",
    "text": "Visualization 2 (2 pts)\nPick the field that most closely matches the area of your degree. Make a line graph (with points for each datapoint) that shows how the number of PhDs awarded in your field has changed from 2008 to 2017. Make sure your x-axis indicates each year for which you have data, your graph is appropriately labelled, and you think it is aesthetically pleasing."
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-3-2-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-3-2-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 3 (2 pts)",
    "text": "Visualization 3 (2 pts)\nPick at least 3 additional fields (you can use more if you like) that are adjacent to your Ph.D.¬†field. Make a faceted plot to show the number of degrees awarded in each of these disciplines across the same time period. Make sure you label your plot appropriately and you think it is aesthetic (i.e., if you have squished strip text you want to fix that)."
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html",
    "href": "assignments/modules/module4/module_4_assignment.html",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "This is your assignment for Module 4 Putting It All Together, focused on the material you learned in the lectures and recitation activities on PCA, Manhattan plots, interactive plots, and the leftovers.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing. Starting on this assignment, I will be considering for overall format and readability of your assignment as part of your grade. I am doing this because the format of your report will be considered for your final capstone assignment. This means you should have reasonable headers and header levels, understandable flow between plots and code, and use Markdown language when appropriate.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, December 9, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is the same we used in the ggplot102 recitation that includes information about dog breed trait information from the American Kennel Club.\nDownload the data using the code below. Don‚Äôt use the code from week 5 recitation.\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_traits_fixed.csv')\n\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/trait_description.csv')\n\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_rank_all.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(gghighlight)"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#introduction",
    "href": "assignments/modules/module4/module_4_assignment.html#introduction",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "This is your assignment for Module 4 Putting It All Together, focused on the material you learned in the lectures and recitation activities on PCA, Manhattan plots, interactive plots, and the leftovers.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing. Starting on this assignment, I will be considering for overall format and readability of your assignment as part of your grade. I am doing this because the format of your report will be considered for your final capstone assignment. This means you should have reasonable headers and header levels, understandable flow between plots and code, and use Markdown language when appropriate.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, December 9, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is the same we used in the ggplot102 recitation that includes information about dog breed trait information from the American Kennel Club.\nDownload the data using the code below. Don‚Äôt use the code from week 5 recitation.\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_traits_fixed.csv')\n\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/trait_description.csv')\n\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_rank_all.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(gghighlight)"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#principal-components-analysis-pca-of-american-kennel-club-dog-bred-trait-data-4-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#principal-components-analysis-pca-of-american-kennel-club-dog-bred-trait-data-4-pts",
    "title": "Module 4 Assignment",
    "section": "1. Principal components analysis (PCA) of American Kennel Club dog bred trait data (4 pts)",
    "text": "1. Principal components analysis (PCA) of American Kennel Club dog bred trait data (4 pts)\nRun a PCA on breed_traits for all of the numeric data present in that dataset. Create the following plots and make them of publication quality:\n\nA scree plot\nA scores plot\nA loadings plot\nA two panel plot that has the scores plot and the scree plot together"
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#make-your-pca-plot-interactive-2-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#make-your-pca-plot-interactive-2-pts",
    "title": "Module 4 Assignment",
    "section": "2. Make your PCA plot interactive (2 pts)",
    "text": "2. Make your PCA plot interactive (2 pts)\nMake your PCA scores plot interactive, and so that when you hover each point, you can see what the name of that dog breed is (and only the breed of that dog)."
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#see-how-your-pca-related-to-breed-popularity-2-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#see-how-your-pca-related-to-breed-popularity-2-pts",
    "title": "Module 4 Assignment",
    "section": "3. See how your PCA related to breed popularity (2 pts)",
    "text": "3. See how your PCA related to breed popularity (2 pts)\nUsing breed_traits and breed_rank_all, label the points that show data for the top 10 dog breeds in 2020 and color them different from the rest of the points. Your plot does not need to be interactive."
  },
  {
    "objectID": "assignments/capstone.html",
    "href": "assignments/capstone.html",
    "title": "Capstone assignment",
    "section": "",
    "text": "At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nThere will be 1 capstone assignment.\nDue Dates:\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nCapstone plan\nTuesday, October 28, 2025\n\n\nCapstone\nFriday, December 12, 2025\n\n\n\n\n\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/capstone.html#capstone-assignment",
    "href": "assignments/capstone.html#capstone-assignment",
    "title": "Capstone assignment",
    "section": "",
    "text": "At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nThere will be 1 capstone assignment.\nDue Dates:\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nCapstone plan\nTuesday, October 28, 2025\n\n\nCapstone\nFriday, December 12, 2025\n\n\n\n\n\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/reflections.html",
    "href": "assignments/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "After each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday August 20, the reflection for that class is due on Tuesday August 28 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/reflections.html#reflections",
    "href": "assignments/reflections.html#reflections",
    "title": "Reflections",
    "section": "",
    "text": "After each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday August 20, the reflection for that class is due on Tuesday August 28 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html",
    "href": "assignments/modules/module3/module_3_assignment.html",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "This is your assignment for Module 3, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nInclude information about both what you are doing and why you are doing it.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, November 4, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)"
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#introduction",
    "href": "assignments/modules/module3/module_3_assignment.html#introduction",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "This is your assignment for Module 3, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nInclude information about both what you are doing and why you are doing it.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, November 4, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)"
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#data-distributions-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#data-distributions-visualization-3-pts",
    "title": "Module 3 Assignment",
    "section": "1. Data distributions visualization (3 pts)",
    "text": "1. Data distributions visualization (3 pts)\nCreate a visualization that shows the distribution of tuition costs (both in_state_tuition and out_of_state_tuition) across public, private, and for-profit universities and colleges. You can use whatever type of plot you think is appropriate to show this distribution across different types of universities. Your plot should be publication ready quality."
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#adding-statistics-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#adding-statistics-visualization-3-pts",
    "title": "Module 3 Assignment",
    "section": "2. Adding statistics visualization (3 pts)",
    "text": "2. Adding statistics visualization (3 pts)\nMake a plot that shows the difference in early_career_pay across private and public universities/colleges. Is there any statistical difference in pay across these two categories of institutions? Is the same true for mid_career_pay? This can be either one or two plots, its up to you. Make sure you are doing the right statistical test appropriate for your data."
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#understanding-correlations-visualization-2-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#understanding-correlations-visualization-2-pts",
    "title": "Module 3 Assignment",
    "section": "3. Understanding correlations visualization (2 pts)",
    "text": "3. Understanding correlations visualization (2 pts)\nMake a visualization that investigates and then visualizes correlation between early_career_pay, mid_career_pay and university tuition (both in state and out of state) showing correlation coefficients. Show how this is different across public and private universities. If you feel like you want to make a couple plots to display this relationship, that is fine."
  },
  {
    "objectID": "assignments/modules/module1/module_1_assignment.html",
    "href": "assignments/modules/module1/module_1_assignment.html",
    "title": "Module 1 Assignment",
    "section": "",
    "text": "This is your assignment for Module 1.\nPlease upload to Carmen:\n\n1 good visualization, along with 1 paragraph about why it is good\n1 bad visualization, along with 1 paragraph about why it is bad.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Monday, September 1, 2025 at 11:59pm so we can talk about them in class the next day."
  },
  {
    "objectID": "assignments/modules/module1/module_1_assignment.html#introduction",
    "href": "assignments/modules/module1/module_1_assignment.html#introduction",
    "title": "Module 1 Assignment",
    "section": "",
    "text": "This is your assignment for Module 1.\nPlease upload to Carmen:\n\n1 good visualization, along with 1 paragraph about why it is good\n1 bad visualization, along with 1 paragraph about why it is bad.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Monday, September 1, 2025 at 11:59pm so we can talk about them in class the next day."
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus, Autumn 2025",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D.\nEmail address: cooperstone dot 1 at osu dot edu (preferred contact method)\nPhone number: 614-292-2843 (non-preferred contact method)\nTA: Daniel Quiroz Moreno, quirozmoreno dot 1 at osu dot edu\nOffice hours: TBD"
  },
  {
    "objectID": "syllabus/syllabus.html#instructor",
    "href": "syllabus/syllabus.html#instructor",
    "title": "Syllabus, Autumn 2025",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D.\nEmail address: cooperstone dot 1 at osu dot edu (preferred contact method)\nPhone number: 614-292-2843 (non-preferred contact method)\nTA: Daniel Quiroz Moreno, quirozmoreno dot 1 at osu dot edu\nOffice hours: TBD"
  },
  {
    "objectID": "syllabus/syllabus.html#course-meeting-time-and-place",
    "href": "syllabus/syllabus.html#course-meeting-time-and-place",
    "title": "Syllabus, Autumn 2025",
    "section": "Course meeting time and place",
    "text": "Course meeting time and place\nTuesdays synchronously from 4:10-5:55 pm in:\n\nHowlett Hall 116 (Columbus)\nWilliams Hall 123 (Wooster)\nVirtually via Zoom https://go.osu.edu/dataviz-zoom"
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Syllabus, Autumn 2025",
    "section": "Course description",
    "text": "Course description\nThis course aims to introduce students to the principles and practice of data visualization. Students will learn fundamental principles of data visualization and create figures that appropriately and ethically represent their data. Data visualizations will be created in the R programming environment, using tools including the grammar of graphics implemented in ggplot2. In the process of creating visualizations, students will also become familiar with data handling and wrangling in R.\nCourse learning outcomes\nBy the end of this course, students should successfully be able to:\n\nRecall and describe the fundamental goals and principles of data visualization.\nDistinguish between good and bad visualizations, and understand how to make those are ineffective more effective.\nLearn to use R, R Markdown, and ggplot to make clear, descriptive, and aesthetic visualization.\nApply principles learned in class, both theoretical and technical, to create effective visualizations."
  },
  {
    "objectID": "syllabus/syllabus.html#course-delivery",
    "href": "syllabus/syllabus.html#course-delivery",
    "title": "Syllabus, Autumn 2025",
    "section": "Course delivery",
    "text": "Course delivery\nMode of delivery: This course is a hybrid-delivered course where I provide ~ 50 min of lecture material, followed by ~1 hour of recitation activities to do in-class (with assistance from the instructor and the TA). You can attend class in person or virtually via Zoom. You are also welcome to come to class with questions about the weekly videos, or problems you are currently encountering in creating your visualizations.\nAttendance and participation requirements: This is a hybrid course, but is taught synchronously, meaning it is expected that you attend class, either in person or virtually (via Zoom) during its meeting time. I will not take attendance. If circumstances require you to miss class, it will be expected you watch the recorded sessions, and catch up on material on your own. I have found that students who attend classes more quickly and completely master course content.\nClass recordings: To help you master material, and to better accommodate students, classes will be recorded, and recordings uploaded directly after class to Carmen. You can find a link to a OneDrive folder with the recorded lectures on the Syllabus page on Carmen."
  },
  {
    "objectID": "syllabus/syllabus.html#course-schedule",
    "href": "syllabus/syllabus.html#course-schedule",
    "title": "Syllabus, Autumn 2025",
    "section": "Course schedule",
    "text": "Course schedule\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-08-26\n1: Principles\nPrinciples of data visualization\n\n\n2025-09-02\n1: Principles\nGood and bad visualizations\n\n\n2025-09-09\n2: Coding fundamentals\nR Markdown for reproducible research\n\n\n2025-09-16\n2: Coding fundamentals\nWrangling, the basics\n\n\n2025-09-23\n2: Coding fundamentals\nggplot 101\n\n\n2025-09-30\n2: Coding fundamentals\nThemes, labels, facets (ggplot 102)\n\n\n2025-10-07\n3: Data exploration\nData distributions\n\n\n2025-10-14\n3: Data exploration\nCorrelations\n\n\n2025-10-21\nCapstone prep\nCapstone plan prep, open session\n\n\n2025-10-28\n3: Data exploration\nAnnotating statistics\n\n\n2025-11-04\n4: Putting it together\nPrincipal components analysis\n\n\n2025-11-11\nNo class, Veterans Day\n‚Äì\n\n\n2025-11-18\n4: Putting it together\nInteractive plots\n\n\n2025-11-25\n4: Putting it together\nManhattan plots and making lots of plots at once (asynchronous)\n\n\n2025-12-02\n4: Putting it together\nggplot extension packages\n\n\n2025-12-09\nCapstone prep\nCapstone assignment, open session\n\n\n\n\n\n\n\n\n\n\n\nNon-standard days\n\n\n\nThere is no class on November 11 for Veterans Day. On November 25 (the short week of Thanksgiving), I will provide to you the lecture material asynchronously and you can go through the recitation material on your own. Daniel and I will be available to answer questions during office hours, or at the beginning of the next class period."
  },
  {
    "objectID": "syllabus/syllabus.html#course-resources",
    "href": "syllabus/syllabus.html#course-resources",
    "title": "Syllabus, Autumn 2025",
    "section": "Course resources",
    "text": "Course resources\nThere are no required textbooks for this course, though you will find many of the recommend texts and resources belowvery useful.\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nIntroduction to Data Science by Rafael Irizarry\nData Visualization with R by Rob Kabacoff\nData Visualization, A practical introduction by Kieran Healy\nHands-On Data Visualization by Jack Dougherty and Ilya Ilyankou\nFundamentals of Data Visualization by Claus O. Wilke\nggplot2, Elegant Graphics for Data Analysis by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nModern Data Science with R by Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton\nR Markdown Cookbook by Yihui Xie, Christophe Dervieux, Emily Riederer\nThe tidyverse style guide\nA ggplot2 tutorial for beautiful plotting in R by C√©dric Scherer\nRStudio cheatsheets, including ggplot2, dplyr, tidyr, readr and other data import packages, stringr for managing character strings, forcats for managing factors, and R Markdown"
  },
  {
    "objectID": "syllabus/syllabus.html#required-software",
    "href": "syllabus/syllabus.html#required-software",
    "title": "Syllabus, Autumn 2025",
    "section": "Required software",
    "text": "Required software\n\nR: We will use the R programming environment for this class https://www.r-project.org/ (free). You can do so many things in R (including building this course website).\nRStudio Desktop: This IDE (integrated development environment) allows a user-friendly interface with the R programming environment, which we will use in class as well. You must have R before you download RStudio (free).\nMicrosoft Office 365: All Ohio State students are now eligible for free Microsoft Office 365. Full instructions for downloading and installation can be found at go.osu.edu/office365help."
  },
  {
    "objectID": "syllabus/syllabus.html#prior-r-experience",
    "href": "syllabus/syllabus.html#prior-r-experience",
    "title": "Syllabus, Autumn 2025",
    "section": "Prior R experience",
    "text": "Prior R experience\nYou do not need to be an R expert for this class, but I will assume working-level knowledge of R programming. If you have no experience with R, but would still like to take this class, you can. I ask then you get yourself up to speed by taking this free online class https://www.edx.org/course/data-science-r-basics (audit only) before the start of the 3rd week of class. The course will take 8-16 hours to complete so please leave yourself enough time to do so before week 3. Tips and tricks in R will be scattered throughout the course material."
  },
  {
    "objectID": "syllabus/syllabus.html#grading",
    "href": "syllabus/syllabus.html#grading",
    "title": "Syllabus, Autumn 2025",
    "section": "Grading",
    "text": "Grading\n\nHow your grade is calculated\n\n\n\n\n\n\n\nAssignment category (how many assignments)\nPoints\n\n\n\n\nModule assignments (4)\n32 (8 points per assignment)\n\n\nRecitations (8, you pick which ones)\n8 (1 point per recitation submitted)\n\n\nClass reflections (10, you pick which ones)\n20 (2 points per reflection)\n\n\nCapstone assignment (1 plan, 1 assignment)\n10 for the capstone plan, 30 for the assignment\n\n\n\nSee the Assignments tab for additional information.\n\n\nAssignment descriptions\n\nModule assignments\nDescription: After each module, there will be an assignment to provide practice for the techniques learned in class. Assignments will be posted at least one week prior to their due date, and due dates can be found on Carmen.\nGrading: Each part of the assignment will have a certain number of points associated with it, provided along with the assignment.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nRecitation submissions\nDescription: I ask you submit 8 of 11 recitations to Carmen to show you have made a good faith effort to engage with the course material.\nGrading: I will mark these at 0 or 1 points, with 1 point given for completion of at least 70% of the assignment.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own. Generative AI is not to be used.\n\n\nClass reflections\nAfter each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday September 1, the reflection for that class is due on Tuesday September 8 by 11:59pm.\nGrading: Reflections will be graded as follows:\n\nFull credit: Reflections thoughtfully engage with course content, demonstrate the student thought about material and how it would (or wouldn‚Äôt) be relevant to their work and development. This is the level of engagement I expect for this course.\nHalf credit: Reflections are superficial and demonstrate minimal engagement with the course content. A half credit grade indicates better engagement is required for the next reflection. I do not expect to assign these grades often.\n\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nCapstone assignment\nDescription: At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nGrading: Guidance will be provided for the grading of the capstone assignment. Assignments completed outside R Markdown, or not knitted to a .html are not acceptable.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nDue dates\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nReflections\n1 week after each class\n\n\nRecitations\nSunday after each class\n\n\nModule 1: Good and bad visualizations\nMonday, September 1, 2025\n\n\nModule 2: Coding Fundamentals\nTuesday, October 7, 2025\n\n\nModule 3: Data Exploration\nTuesday, November 4, 2025\n\n\nModule 4: Putting it together\nTuesday, December 9, 2025\n\n\nCapstone plan\nTuesday, October 28, 2025\n\n\nCapstone\nFriday, December 12, 2025\n\n\n\n\n\n\n\n\nLate assignments\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.\n\n\nGrading scale\n\n\n\n\n\nScore\nGrade\n\n\n\n\n93‚Äì100\nA\n\n\n90‚Äì92.9\nA-\n\n\n87‚Äì89.9\nB+\n\n\n83‚Äì86.9\nB+\n\n\n80‚Äì82.9\nB-\n\n\n77‚Äì79.9\nC+\n\n\n73‚Äì76.9\nC\n\n\n70‚Äì72.9\nC-\n\n\n67‚Äì69.9\nD+\n\n\n60‚Äì66.9\nD\n\n\nBelow 60\nE\n\n\n\n\n\n\n\nInstructor feedback and response time\n\nGrading and feedback: For assignments, you can generally expect feedback within 7 days.\nEmail: I will reply to emails within 48 hours on days when class is in session at the university."
  },
  {
    "objectID": "syllabus/syllabus.html#other-course-policies",
    "href": "syllabus/syllabus.html#other-course-policies",
    "title": "Syllabus, Autumn 2025",
    "section": "Other course policies",
    "text": "Other course policies\n\nDiscussion and communication guidelines\nI expect all communication will be respectful and thoughtful.\n\n\nAcademic Misconduct/Academic Integrity\nAcademic integrity is essential to maintaining an environment that fosters excellence in teaching, research, and other educational and scholarly activities. Thus, The Ohio State University and the Committee on Academic Misconduct (COAM) expect that all students have read and understand the University‚Äôs Code of Student Conduct, and that all students will complete all academic and scholarly assignments with fairness and honesty. Students must recognize that failure to follow the rules and guidelines established in the University‚Äôs Code of Student Conduct and this syllabus may constitute Academic Misconduct.\nThe Ohio State University‚Äôs Code of Student Conduct (Section 3335-23-04) defines academic misconduct as: Any activity that tends to compromise the academic integrity of the University, or subvert the educational process. Examples of academic misconduct include (but are not limited to) plagiarism, collusion (unauthorized collaboration), copying the work of another student, and possession of unauthorized materials during an examination. Ignorance of the University‚Äôs Code of Student Conduct is never considered an excuse for academic misconduct, so I recommend that you review the Code of Student Conduct and, specifically, the sections dealing with academic misconduct.\nIf I suspect that a student has committed academic misconduct in this course, I am obligated by University Rules to report my suspicions to the Committee on Academic Misconduct. If COAM determines that you have violated the University‚Äôs Code of Student Conduct (i.e., committed academic misconduct), the sanctions for the misconduct could include a failing grade in this course and suspension or dismissal from the University.\nIf you have any questions about the above policy or what constitutes academic misconduct in this course, please contact me.\n\n\nCreating an environment free from harassment, discrimination, and sexual misconduct\nThe Ohio State University is committed to building and maintaining a community to reflect diversity and to improve opportunities for all. All Buckeyes have the right to be free from harassment, discrimination, and sexual misconduct. Ohio State does not discriminate on the basis of age, ancestry, color, disability, ethnicity, gender, gender identity or expression, genetic information, HIV/AIDS status, military status, national origin, pregnancy (childbirth, false pregnancy, termination of pregnancy, or recovery therefrom), race, religion, sex, sexual orientation, or protected veteran status, or any other bases under the law, in its activities, academic programs, admission, and employment. Members of the university community also have the right to be free from all forms of sexual misconduct: sexual harassment, sexual assault, relationship violence, stalking, and sexual exploitation.\nTo report harassment, discrimination, sexual misconduct, or retaliation and/or seek confidential and non-confidential resources and supportive measures, contact the Office of Institutional Equity by: 1. Online reporting form at equity.osu.edu, 2. Call 614-247-5838 or TTY 614-688-8605, 3. Or Email equity@osu.edu\n\n\nDiversity\nThe Ohio State University affirms the importance and value of diversity of people and ideas. We believe in creating equitable research opportunities for all students and to providing programs and curricula that allow our students to understand critical societal challenges from diverse perspectives and aspire to use research to promote sustainable solutions for all. We are committed to maintaining an inclusive community that recognizes and values the inherent worth and dignity of every person; fosters sensitivity, understanding, and mutual respect among all members; and encourages each individual to strive to reach their own potential. The Ohio State University does not discriminate on the basis of age, ancestry, color, disability, gender identity or expression, genetic information, HIV/AIDS status, military status, national origin, race, religion, sex, gender, sexual orientation, pregnancy, protected veteran status, or any other bases under the law, in its activities, academic programs, admission, and employment.\nIn addition, this course adheres to The Principles of Community adopted by the College of Food, Agricultural, and Environmental Sciences. These principles are located on the Carmen site for this course; and can also be found at https://go.osu.edu/principlesofcommunity. For additional information on Diversity, Equity, and Inclusion in CFAES, contact the CFAES Office for Diversity, Equity, and Inclusion (https://equityandinclusion.cfaes.ohio-state.edu/). If you have been a victim of or a witness to a bias incident, you can report it online and anonymously (if you choose) at https://equity.osu.edu/.\n\n\nCounseling and Consultation Services/Mental Health\nAs a student you may experience a range of issues that can cause barriers to learning, such as strained relationships, increased anxiety, alcohol/drug problems, feeling down, difficulty concentrating and/or lack of motivation. These mental health concerns or stressful events may lead to diminished academic performance or reduce a student‚Äôs ability to participate in daily activities. The Ohio State University offers services to assist you with addressing these and other concerns you may be experiencing. If you or someone you know are suffering from any of the aforementioned conditions, you can learn more about the broad range of confidential mental health services available on campus via the Office of Student Life Counseling and Consultation Services (CCS) by visiting ccs.osu.edu or calling (614) 292- 5766. CCS is located on the 4th Floor of the Younkin Success Center and 10th Floor of Lincoln Tower. You can reach an on-call counselor when CCS is closed at (614) 292-5766 and 24 hour emergency help is also available through the 24/7 National Suicide Prevention Hotline at 1-(800)-273-TALK or at suicidepreventionlifeline.org\nDavid Wirt, wirt.9@osu.edu, is the CFAES embedded mental health counselor in Columbus. He is available for new consultations and to establish routine care. To schedule with David, please call 614-292-5766. Students should mention their affiliation with CFAES when setting up a phone screening.\nDr.¬†Schaad, schaad.15@osu.edu, is the CFAES embedded mental health counselor in Wooster. She is available for new consultations and to establish routine care. To schedule with Dr.¬†Schaad, please call 614-292-5766. Students should mention their affiliation with CFAES when setting up a phone screening.\n\n\nLand Acknowledgement\nWe would like to acknowledge the land that The Ohio State University occupies is the ancestral and contemporary lands of the Shawnee, Potawatomi, Delaware, Miami, Peoria, Seneca, Wyandotte, Ojibwe and Cherokee peoples. The university resides on land ceded in the 1795 Treaty of Greeneville and the forced removal of tribes through the Indian Removal Act of 1830. We honor the resiliency of these tribal nations and recognize the historical contexts that have and continue to affect the Indigenous peoples of this land.\n\n\nAccessibility accomodations\nThe university strives to make all learning experiences as accessible as possible. In light of the current pandemic, students seeking to request COVID-related accommodations may do so through the university‚Äôs request process, managed by Student Life Disability Services. If you anticipate or experience academic barriers based on your disability (including mental health, chronic, or temporary medical conditions), please let me know immediately so that we can privately discuss options. To establish reasonable accommodations, I may request that you register with Student Life Disability Services. After registration, make arrangements with me as soon as possible to discuss your accommodations so that they may be implemented in a timely fashion.\nSLDS contact information: slds@osu.edu; 614-292-3307; slds.osu.edu; 098 Baker Hall, 113 W. 12th Avenue."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html",
    "href": "rmds/03_Rmd_recitation_solutions.html",
    "title": "R Markdown Recitation Solutions",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html#r-markdown-recitation",
    "href": "rmds/03_Rmd_recitation_solutions.html#r-markdown-recitation",
    "title": "R Markdown Recitation Solutions",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html#second-biggest-header",
    "href": "rmds/03_Rmd_recitation_solutions.html#second-biggest-header",
    "title": "R Markdown Recitation Solutions",
    "section": "Second biggest header",
    "text": "Second biggest header\n\nThird biggest header\n\n\nFour biggest header\nYou get the idea.\nAdd a hyperlink to our class website\nAdd an image. Note you will do this differently if you are adding an image from internet vs one you have on your local machine. Also remember your working directory is the location of your Rmd and you may want to have a directory called img where images are stored.\n\n\n\nThis is my dog Nacho\n\n\nAdd a block quote\n\nHere is an important block quote\n\nMake a bulleted list\n\nThing 1\nThing 2\nThing 3\nThis also works\nTo make\nA bulleted list\n\n\nYou can also\nMake\nNumbered lists"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html",
    "title": "Manhattan Plots Recitation",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html#introduction",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html#introduction",
    "title": "Manhattan Plots Recitation",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html#investigate-your-data.",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html#investigate-your-data.",
    "title": "Manhattan Plots Recitation",
    "section": "Investigate your data.",
    "text": "Investigate your data.\n\nWhat are your columns?\n\n\nHow many markers are there?\n\n\nHow are the markers distributed across the chromosomes?"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html#make-a-manhattan-plot.",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html#make-a-manhattan-plot.",
    "title": "Manhattan Plots Recitation",
    "section": "Make a Manhattan plot.",
    "text": "Make a Manhattan plot.\nColor by chromosome, make sure the x-axis breaks are appropriate, be sure your y-axis is -log10 pvalue. Label the top 3 most significant points with their SNP number."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html",
    "title": "Leftover tidbits recitation",
    "section": "",
    "text": "Today‚Äôs recitation materials are on a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\nlibrary(gghighlight) # for highlighting\nlibrary(gganimate) # animating plots\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\nlibrary(scales) # for easy scaling\nlibrary(plotly) # for ggplotly\nlibrary(glue) # for easy pasting\n\nlibrary(gapminder) # for data for viz2"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#introduction",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#introduction",
    "title": "Leftover tidbits recitation",
    "section": "",
    "text": "Today‚Äôs recitation materials are on a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\nlibrary(gghighlight) # for highlighting\nlibrary(gganimate) # animating plots\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\nlibrary(scales) # for easy scaling\nlibrary(plotly) # for ggplotly\nlibrary(glue) # for easy pasting\n\nlibrary(gapminder) # for data for viz2"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#really-start-using-an-rproject",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#really-start-using-an-rproject",
    "title": "Leftover tidbits recitation",
    "section": "Really start using an Rproject üìΩÔ∏è",
    "text": "Really start using an Rproject üìΩÔ∏è\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nIf you don‚Äôt have a Rproject for class, set one up."
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-1",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-1",
    "title": "Leftover tidbits recitation",
    "section": "Visualization 1",
    "text": "Visualization 1\nWe are going to interrogate a dataset from Gapminder that includes information about Happiness Scores collected across different countries and years.\nCreate a visualization that shows the happiness scores for all countries from 2008 to 2010. Highlight in some way the top 3 countries with the highest happiness scores per continent.\nI‚Äôve put the data on Github so you can easily download it with the code below. Note, the question asks you to make a plot considering continent so I‚Äôve also provided you a key that has each country, and the continent to which it belows for you to join together.\n\nLoad data\n\nhappiness &lt;- read_csv(\"https://github.com/jcooperstone/dataviz-site/raw/master/4_12_leftovers/data/hapiscore_whr.csv\")\n\ncountry_continent &lt;- read_csv(\"https://github.com/jcooperstone/dataviz-site/raw/master/4_12_leftovers/data/country_continent.csv\")"
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-2",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-2",
    "title": "Leftover tidbits recitation",
    "section": "Visualization 2",
    "text": "Visualization 2\nRecreate a plot in the vein of the one here. You can make the same interactive plot (use the data from 2007, which is slightly older and different from what you see in the online plot), or choose to animate it over year. Or do both.\nUse the data gapminder::gapminder which you can access from R."
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "",
    "text": "Get familiar with plotly\nDiscuss options for interactive plots"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#installing-plotly",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#installing-plotly",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Installing plotly",
    "text": "Installing plotly\n\n# From CRAN\ninstall.packages(\"plotly\")"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#ggplotly",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#ggplotly",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "ggplotly()",
    "text": "ggplotly()\n\n\n\n\n\n\n\n\n\nImage retrieved from this blog."
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#description",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#description",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Description",
    "text": "Description\nThis function converts a ggplot2::ggplot() object to a plotly object.\nggplotly(\n  p = ggplot2::last_plot(),\n  width = NULL,\n  height = NULL,\n  tooltip = \"all\",\n  dynamicTicks = FALSE,\n  layerData = 1,\n  originalData = TRUE,\n  source = \"A\",\n  ...\n)\nNow, we are going to explore using plotly on ggplot objects using ggplotly()."
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#basic-scatter-plot",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#basic-scatter-plot",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Basic scatter plot",
    "text": "Basic scatter plot\n\nggpenguins &lt;- palmerpenguins::penguins |&gt; \n# note that x is the default first argument and y is the second\n# and this works even though we haven't said this explicitly\n  ggplot(aes(bill_length_mm, body_mass_g, color = species )) + \n  geom_point() + \n  scale_color_d3() + # from ggsci\n  theme_bw() +\n  labs(x = \"Bill length, in mm\",\n       y = \"Body mass, in g\",\n       color = \"Species\",\n       title = \"Palmer penguin body mass by bill length\")\n\nggpenguins\n\n\n\n\nThings you can do with an interactive plot:\n\nZoom in and out\nTurn off groups (click on the legend)\nDownload plot as a .png (click the camera)\n\n\nggplotly(ggpenguins)"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#pca",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#pca",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "PCA",
    "text": "PCA\n\n# Library here just to make emphasis \nlibrary(ggfortify)\n\nIn this case we are going to explore a new library ggfortify that accepts prcomp results and creates an automatic scores PCA plot.\nAfter you perform PCA analysis, you can use autoplot() to create a ggplot2 object for using in the ggplotly() function.\n\ndf &lt;- iris[1:4] # Extract numeric variables\n\npca_res &lt;- prcomp(df, scale = TRUE) # Run PCA\n\np &lt;- autoplot(pca_res, data = iris, colour = 'Species') + # PCA autoplot\n  scale_color_d3() +\n  labs(title = \"Scores plot of iris data\")\n\nggplotly(p)"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#boxplots",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#boxplots",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Boxplots",
    "text": "Boxplots\nData used here is midwest from ggplot2 which contains demographic information from census data collected in the 2000 US census. The variable percollege includes what percentage of the respondents are college educated.\n\nboxplot &lt;- ggplot(midwest, aes(x = state, y = percollege, color = state) ) + \n  geom_boxplot() +\n  scale_color_d3() +\n  theme_bw() + \n  coord_flip() +\n  labs(title = \"Percentage of college educated respondents\")\n\nggplotly(boxplot)"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#barplots",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#barplots",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Barplots",
    "text": "Barplots\n\nstack_barplot &lt;- ggplot(mpg, aes(x = class))   + \n  geom_bar(aes(fill = drv)) + \n  scale_fill_d3() + \n  theme_bw() +\n  labs(title = \"Class of cars in the mpg datase\")\n\nggplotly(stack_barplot)"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#tooltip",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#tooltip",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Tooltip",
    "text": "Tooltip\nTooltip is an argument that controls the text that is shown when you hover the mouse over data. By default, all aes mapping variables are shown. You can modify the order and the variables that are shown in the tooltip.\nIn the penguins data we have more variables that may want to include in the text shown in our plotly plot such as sex and island.\n\n# dt[seq(10),] subset the ten first row and then use glimpse to shorten the output\nglimpse(palmerpenguins::penguins[seq(10), ]) \n\nRows: 10\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n\n\n\nggplotly(ggpenguins)\n\n\n\n\n\n\n‚Äúcolour‚Äù is required and ‚Äúcolor‚Äù is not supported in ggplotly()\n\n\nggplotly(ggpenguins,\n         tooltip = \"colour\")"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#changing-hover-details",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#changing-hover-details",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Changing hover details",
    "text": "Changing hover details\nYou might not like the default hover text aesthetics, and can change them! You can do this using style and layout and adding these functions using the pipe (|&gt; or |&gt;).\nCode taken from OSU‚Äôs Code Club\n\n# setting fonts for the plot\nfont &lt;- list(\n  family = \"Arial\",\n  size = 15,\n  color = \"white\")\n\n# setting hover label specs\nlabel &lt;- list(\n  bgcolor = \"#3d1b40\",\n  bordercolor = \"transparent\",\n  font = font) # we can do this bc we already set font\n\n# amending our ggplotly call to include new fonts and hover label specs\nggplotly(ggpenguins, tooltip = \"colour\") |&gt;\n  style(hoverlabel = label) |&gt;\n  layout(font = font)"
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#helpful-resources",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#helpful-resources",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Helpful resources",
    "text": "Helpful resources\n\nUsing different fonts: link\nggfortify: link\nggsci: link"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html",
    "href": "modules/module4/10_pca/10_pca_recitation.html",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands compiled by f-imp and posted to Github.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#introduction",
    "href": "modules/module4/10_pca/10_pca_recitation.html#introduction",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands compiled by f-imp and posted to Github.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#run-a-pca",
    "href": "modules/module4/10_pca/10_pca_recitation.html#run-a-pca",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "1. Run a PCA",
    "text": "1. Run a PCA"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "2. Make a scree plot of the percent variance explained by each component",
    "text": "2. Make a scree plot of the percent variance explained by each component"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "3. Make a scores plot of samples, coloring each sample by its brand",
    "text": "3. Make a scores plot of samples, coloring each sample by its brand"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-loadings-plot-of-samples",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-loadings-plot-of-samples",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "4. Make a loadings plot of samples",
    "text": "4. Make a loadings plot of samples"
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "href": "modules/module4/10_pca/10_pca_recitation.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together.",
    "text": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together."
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\nBelwo are the packages that I used to make my plot.\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(rstatix)\nlibrary(ggpubr)\nlibrary(glue)\nlibrary(rcompanion)"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html#introduction",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\nBelwo are the packages that I used to make my plot.\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(rstatix)\nlibrary(ggpubr)\nlibrary(glue)\nlibrary(rcompanion)"
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "Is total cholesterol (TotChol) different by age (AgeDecade)?",
    "text": "Is total cholesterol (TotChol) different by age (AgeDecade)?\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint - you want to test your assumptions to see what tests to do. You might need to use different posthoc comparison methods than we did in class.\n\n\n\n\n\n\n\n\n\nNeed another hint? (Click to expand)\n\n\n\n\n\nAnother hint - the function rcompanion::cldList() will convert the resulting comparison table from a posthoc Dunn test to create a column with the letters indicating which groups are significantly different from each other."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html",
    "href": "modules/module3/07_distributions/07_distributions.html",
    "title": "Understanding Data Distributions",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions.html#introduction",
    "title": "Understanding Data Distributions",
    "section": "Introduction",
    "text": "Introduction\nWe will will building on our lesson on ggplot101 and ggplot102 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, geoms, facets, scales, labels, and themes. Today we are going to apply what we learned towards trying to better understanding our underlying data distributions.\nOften, we think about figure generation as the last part of the scientific process, something you do as you prepare a manuscript for publication. I hope to convince you that exploring your data, and making exploratory plots is a critical part of the data analysis and interpretation process.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load the packages we need.\n\nlibrary(tidyverse)\n\nToday we are using real research data from my group. We will be reading in the supplementary data from a paper written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 1. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloids &lt;- readxl::read_excel(\"tpg220192-sup-0002-supmat.xlsx\",\n                                sheet = \"S1 Raw Data Diversity Panel\")\n\n\nknitr::kable(head(alkaloids))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nYear\nEnvironment\nBlock\nGenotype\nPlot_Source\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nLatitude\nLongitude\nDehydrotomatidine\nTomatidine\nDehydrotomatine1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\n\n\n\n\n7805\n2018\nFreEarly18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.000000\n5.726010\n0.350331\n6.076341\n172.66244\n1.079190\n86.72742\n17.831892\n9.142607\n114.78111\n18.902399\n56.307182\n1.890053\n77.099634\n5.125904\n10.277325\n336.8893\n347.1666\n3.787979\n0.924195\n3.943230\n8.655404\n731.5675\n\n\n7898\n2017\nFre17\n2\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.169068\n0.000000\n0.000000\n0.000000\n55.47329\n0.000000\n53.32292\n13.630697\n4.841762\n71.79538\n3.557348\n4.107289\n0.000000\n7.664637\n2.905500\n5.548102\n199.6694\n205.2175\n8.978931\n1.897850\n6.794690\n17.671471\n360.8969\n\n\n7523\n2018\nFreLate18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.135675\n0.680554\n5.073552\n0.000000\n5.073552\n123.85835\n0.000000\n50.90989\n6.503939\n1.368847\n58.78268\n3.931461\n4.123222\n0.623340\n8.678023\n2.185082\n5.104115\n259.0177\n264.1218\n4.049145\n0.000000\n6.749386\n10.798531\n474.3143\n\n\n7724\n2017\nFre17\n1\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.054300\n0.497261\n19.419087\n0.000000\n19.419087\n239.01264\n0.000000\n36.02318\n8.557673\n7.483933\n52.06478\n3.341048\n16.415426\n1.057100\n20.813574\n0.000000\n0.000000\n203.0061\n203.0061\n1.678210\n0.000000\n2.349633\n4.027843\n538.8955\n\n\n7427\n2018\nFreLate18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.139454\n0.553801\n0.000000\n0.000000\n0.000000\n64.31783\n0.879435\n39.91027\n7.228388\n3.015298\n51.03339\n0.000000\n3.131685\n0.000000\n3.131685\n0.000000\n4.054211\n299.5687\n303.6229\n10.146857\n0.000000\n4.882339\n15.029197\n437.8283\n\n\n7854\n2018\nFreEarly18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.049700\n0.262174\n3.737579\n0.000000\n3.737579\n68.44913\n0.000000\n23.86864\n13.506299\n1.456982\n38.83192\n4.657902\n4.259007\n0.605729\n9.522638\n9.832149\n11.595595\n459.5205\n471.1161\n6.839930\n0.486236\n5.595751\n12.921917\n614.7233\n\n\n\n\n\nThis dataset has 605 observations, with data about different steroidal alkaloids in the fruits of different tomato germplasm grown in 3 locations across 2 years. There is also some other metadata too."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#geoms-for-distributions",
    "href": "modules/module3/07_distributions/07_distributions.html#geoms-for-distributions",
    "title": "Understanding Data Distributions",
    "section": "Geoms for distributions",
    "text": "Geoms for distributions\n\ngeom_col()\nOften, people use bar charts, representing the height or the length of the bar as proportional to the average value that it represents. These charts are sometimes called dynamite plots because they resemble (when they have an error bar with whisker) those cartoon style dynamite sticks. Pow!\nHowever, these bar charts, even if you add a standard deviation/error, really can hide the true distribution of your data, and for this reason, I and others hope you don‚Äôt select to make them.\nAside: You may be thinking ‚ÄúJess you asked us to make one of these in Module 2 homework‚Äù and I did but also that was a little different. The plot I asked you to make shows the number of degrees awarded, a value for which there really is no distribution. So in that case we are using a bar plot to show something different than a bar plot which is meant to show somehow an average/median and distribution.\nI hope after today, you see that there is always a better chart type to make than a bar chart. But I will show you how to make them anyway.\nBefore we plot, let‚Äôs calculate some summary statistics so we know what we should expect.\n\nalkaloids %&gt;%\n  group_by(Class) %&gt;%\n  summarize(mean_tomatine = mean(Tomatine))\n\n# A tibble: 5 √ó 2\n  Class                 mean_tomatine\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 Cultivated Cherry              235.\n2 Cultivated Processing          330.\n3 S. pimpinellifolium            685.\n4 Wide Cross Hybrid              534.\n5 Wild Cherry                   4928.\n\n\n\n# this is wrong but an easy mistake to make\n# this is not what we want\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_col()\n\n\n\n\nJust calling geom_col() does not give us what we want. Look at the y-axis scale and how out of line this is with our summary statistics. The reason for this is that geom_col() defaults to position = \"stack\" which will just sum the alkaloid content across all the observations. Even changing to position = \"identity\" does not work. This is because we are plotting a transformation of the data (calculation of the mean) which these geoms are not doing.\nWe can calculate manually by generating the summary values and then piping that into our ggplot call.\n\nalkaloids %&gt;%\n  group_by(Class) %&gt;%\n  summarize(mean_tomatine = mean(Tomatine)) %&gt;%\n  ggplot(aes(x = Class, y = mean_tomatine)) +\n  geom_col()\n\n\n\n\n\n\nstat_summary()\nAn easier way to do this would be just with stat_summary(), which does not require the calculation of summary statistic first.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\n\nReordering x-variables\nNote in these plots the ordering of the x-axis categories ‚Äì they are alphabetical. This is the ggplot default. There are many reasons why this might not be the most compelling ordering for your data. You may want to order from lowest to highest mean, or in this case, I want to order the tomatoes from most cultivated on the left, to most wild on the right, since this is the prevailing theme of our paper.\nWe can do this in two ways:\nSimply reorder the plot.\n\n# set what the order is\nalkaloids_order &lt;- c(\"Cultivated Processing\",\n                     \"Cultivated Cherry\",\n                     \"Wide Cross Hybrid\",\n                     \"Wild Cherry\",\n                     \"S. pimpinellifolium\")\n\n# plot and re-level within aes()\nalkaloids %&gt;%\n  ggplot(aes(x = factor(Class, levels = alkaloids_order), y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\nChange the levels of the data so the reordering happens to every plot in the future.\n\n# what type of variable is Class?\nclass(alkaloids$Class)\n\n[1] \"character\"\n\n# convert to factor, and set levels\nalkaloids$Class &lt;- factor(alkaloids$Class,\n                          levels = c(\"Cultivated Processing\",\n                                     \"Cultivated Cherry\",\n                                     \"Wide Cross Hybrid\",\n                                     \"Wild Cherry\",\n                                     \"S. pimpinellifolium\"))\n\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\nMy tendency would be to re-level the data if I always want to use the same order, and just re-level the plot if I only want to do this once or twice.\nYou can also re-order factors based on characteristics of your data using different functions that are part of the forcats package.\n\n\n\ngeom_boxplot()\nA boxplot has the benefit of showing you more than the median and the standard deviation, so you can better see the true distribution of your data. In geom_boxplot():\n\nlower whisker = smallest observation greater than or equal to lower hinge - 1.5 * IQR\nlower hinge/bottom line of box part of boxplot = 25% quantile\nmiddle = median, 50% quantile\nupper hinge/top line of box part of boxplot = 75% quantile\nupper whisker = largest observation less than or equal to upper hinge + 1.5 * IQR\n\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot()\n\n\n\n\nOne reason why this is really importantly different from the bar plot is look at the number of outliers we are seeing for Wild Cherry. You don‚Äôt capture this at all with the median/mean bar plots.\nBecause of the scale of this data, it might be beneficial to log transform the y-axis.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot() +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning in scale_y_continuous(trans = \"log10\"): log-10 transformation\nintroduced infinite values.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\ngeom_jitter()\ngeom_jitter() is a shortcut for geom_point(position = \"jitter\"), but is common enough that the shortcut exists. It is often nice to jitter on top of a boxplot. Note, if you don‚Äôt want the outliers from geom_boxplot() to be plotted twice, you should indicate outlier.shape = NA.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning in scale_y_continuous(trans = \"log10\"): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nJittering introduces a small amount of variation into your points so they‚Äôre easier to see. A width of 0 is no horizontal jitter. A height of 0 is no vertical jitter. Typically you don‚Äôt want veritcal jitter so that the points retain their fidelity on the y-axis (which is where their concentration is plotted). I basically always use geom_jitter(height = 0) for plots where I want to retain y-axis fidelity.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(height = 0) +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning in scale_y_continuous(trans = \"log10\"): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\ngeom_histogram()\nWe could also look at these distribution more like histograms and it provides to us some additional information. When coupled with faceting, this can be very powerful.\n\nalkaloids %&gt;%\n  ggplot(aes(x = Tomatine)) +\n  geom_histogram(bins = 75) + # default is bins = 30\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(vars(Class))\n\n\n\n\n\nalkaloids %&gt;%\n  ggplot(aes(x = Tomatine)) +\n  geom_density() +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(vars(Class))\n\n\n\n\n\n\ngg:ridges::geom_density_ridges()\nI really like the function geom_density_ridges() which is a part of the ggplot add-on package ggridges. It allows you to create ridgeline plots to show distributes in a single non-faceted plot.\n\nlibrary(ggridges) # for ridgeline plots\nlibrary(scales) # for comma format\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +\n  geom_density_ridges(alpha = 0.5) +\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\")\n\n\n\n\nYou can also use the function geom_density_ridges() which will allow you to easily map quantiles or other functions on top of your ridges.\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +\n  stat_density_ridges(alpha = 0.5,\n                      quantile_lines = TRUE,\n                      quantiles = 2) + # break into 2 groups, therefore median\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\",\n       caption = \"Black line represents tomato class median concent\")\n\nPicking joint bandwidth of 0.198\n\n\n\n\n\n\nChanging class labels\nI am bothered by the fact that S. pimpinellifolium (a species of wild tomato) is not indicated in italics. We don‚Äôt want to italicize all of the labels, just S. pimpinellifolium. Let‚Äôs fix that.\nWe can start by creating a vector of the labels how we want them to appear in the plot.\n\nclass_labels &lt;- c(\"Cultivated Processing\", \n                  \"Cultivated Cherry\",\n                  \"Wide Cross Hybrid\", \n                  \"Wild Cherry\",\n                  expression(italic(\"S. pimpinellifolium\")))\n\nclass_labels\n\nexpression(\"Cultivated Processing\", \"Cultivated Cherry\", \"Wide Cross Hybrid\", \n    \"Wild Cherry\", italic(\"S. pimpinellifolium\"))\n\n\nThen we can use one of the scale_*() functions to change our y-axis scale labels to how we want them to be.\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +\n  geom_density_ridges(alpha = 0.5) +\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  scale_y_discrete(labels = class_labels) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\")\n\nPicking joint bandwidth of 0.198\n\n\n\n\n\nIf for example your variables were mapped to color or fill, you could do this using scale_color_manual() or scale_fill_manual(), respectively.\n\n\n\nggdist functions\nAnother ggplot extension package ggdist has cool geoms you can integrate into ggplots to visualize distributions. I think these work better than geom_dotplot().\nSometimes using geom_jitter() when you have a lot of data points can look a bit messy. I think in this case, using geom_dots() works very well. The default orientation is is layout = \"bin\"\n\nlibrary(ggdist)\n\n\nAttaching package: 'ggdist'\n\n\nThe following objects are masked from 'package:ggridges':\n\n    scale_point_color_continuous, scale_point_color_discrete,\n    scale_point_colour_continuous, scale_point_colour_discrete,\n    scale_point_fill_continuous, scale_point_fill_discrete,\n    scale_point_size_continuous\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Tomatine, y = Class)) +  \n  geom_dots() +\n  scale_x_continuous(trans = \"log10\", labels = comma) + \n  scale_y_discrete(labels = class_labels) +\n  labs(x = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       y = \"\",\n       title = \"Distribution of alkaloid content found among \\ntomatoes of different classes\")\n\n\n\n\nYou can really change the feel of the plot by changing the orientation between horizontal and vertical. If you want to use the orientation layout = \"swarm\" you need the package ggbeeswarm. This is also a nice package that performs similarly to ggdist but has less functionality which is why I‚Äôm covering ggdist here.\n\nlibrary(ggbeeswarm) # required for layout = \"swarm\"\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_dots(side = \"both\", layout = \"swarm\") + # requires ggbeeswarm\n  scale_x_discrete(labels = class_labels) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  labs(x = \"\",\n       y = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       title = \"Distribution of alkaloid content found among tomatoes of different classes\")\n\n\n\n\nYou can also use stat_dotsinterval() which will by default add the median and the interquartile range (though you can change exactly what you want to be displayed).\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_dotsinterval(side = \"both\") +\n  scale_x_discrete(labels = class_labels) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  labs(x = \"\",\n       y = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       title = \"Distribution of alkaloid content found among tomatoes of different classes\")\n\n\n\n\nDon‚Äôt forget we can keep layering. We can always map other aethetics to our plot (e.g.¬†shape = as.factor(Year), and we include as.factor() because Year is a character datatype).\n\nalkaloids %&gt;%\n  filter(Tomatine != 0) %&gt;%\n  ggplot(aes(x = Class, y = Tomatine, shape = as.factor(Year))) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  geom_dots(side = \"both\") +\n  theme_ggdist() +\n  theme(legend.position = c(.18, .99),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(5, 5, 5, 5)) +\n  labs(shape = \"Year\",\n       y = \"Alpha-tomatine (¬µg/100 g fresh weight)\") \n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\n‚Ñπ Please use the `legend.position.inside` argument of `theme()` instead."
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#useful-resources",
    "href": "modules/module3/07_distributions/07_distributions.html#useful-resources",
    "title": "Understanding Data Distributions",
    "section": "Useful resources",
    "text": "Useful resources\n\nggplot2 cheatsheet\nggplot2 documentation\nggplot2: elegant graphics for data analysis by Hadley Wickham\nA really compehensive list of resources compiled by Erik Gahner Larsen\nggridges\nggdist\nggbeeswarm\nPast ggplot Code Clubs:\n\nVisualizing Data by Michael Broe\nggplot round 2 by me\nFaceting, multi-plots, and animating\nVisualizing Data by Michael Broe a second one\nggplot round 2 a second one by me"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html",
    "title": "Visualizing Correlations Recitation",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#introduction",
    "title": "Visualizing Correlations Recitation",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-measures-of-blood-pressure",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-measures-of-blood-pressure",
    "title": "Visualizing Correlations Recitation",
    "section": "1. How correlated are different measures of blood pressure?",
    "text": "1. How correlated are different measures of blood pressure?\nIn the NHANES dataset, there are 3 measurements for each systolic (the first/top number) and diastolic blood (the second/bottom number) pressure, and an average for each. How reproducible is each type of blood pressure measurement over the 3 samplings? Make visualizations to convey your findings."
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "title": "Visualizing Correlations Recitation",
    "section": "2. How correlated are different physical measurements, health, and lifestyle variables?",
    "text": "2. How correlated are different physical measurements, health, and lifestyle variables?\nIn the NHANES dataset, there are data for subject BMI, Pulse, BPSysAve, BPDiaAve, TotalChol.\nCreate a series of plots/plot to show the relationship between these variables with each other."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html",
    "title": "Wrangling your data ü§† Recitation",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class. When you go to the links below, click on the Download Raw File icon (the down arrow over a turned open bracket) at the top right of the file\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#introduction",
    "title": "Wrangling your data ü§† Recitation",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class. When you go to the links below, click on the Download Raw File icon (the down arrow over a turned open bracket) at the top right of the file\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#explore-your-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#explore-your-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Explore your data",
    "text": "Explore your data\nWrite some code that lets you explore that is in these two datasets.\nHow many observations there in each dataset?\nWhat years do the data contain information for?"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#modifying-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#modifying-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Modifying data",
    "text": "Modifying data\nCreate a new dataset for life_expectancy that only includes observed data (i.e., remove the projected data after 2022)."
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#calculating-summaries",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#calculating-summaries",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Calculating summaries",
    "text": "Calculating summaries\nWhat country has the highest average happiness index in 2022?\nWhat about overall average highest index?\nHow many countries had an average life expectancy over 80 years in 2022?\nWhat countries are in the top 10 percentile for happiness? What about the bottom? What about for life expectancy? You can calculate this for the most recent data, for the mean, or really for whatever you want. Remember there are lots of ways to do this.\nClick the button Show on the right if you need a hint\n\n# Hint - try using the functions in the `slice_()` family.\n\nWhich country has had their happiness index increase the most from 2012 to 2022? Which dropped the most?"
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#joining-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#joining-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Joining data",
    "text": "Joining data\nTry joining the happiness and life_expectancy datasets together and use the different *_join() functions so you can see how they differ. Check their dimensions and look at them. Think about how you might want to do different joins in different situations.\nIf you wanted to create a plot that allowed you to see the correlation between happiness score and life expectancy in 2022, which joined dataset would you use and why?"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "",
    "text": "Nacho (Jess‚Äôs dog, left) along with his friends Petunia (middle) and Inu (right) waiting for dinner"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Introduction",
    "text": "Introduction\nWe will practice what we learned this week in ggplot102 on:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data. Today we will be looking again at some different data from the Tidy Tuesday project (here is the Github repo) about dog breeds.\n\ninstall.packages(\"tidytuesdayR\")\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nWe will be using the data that is from February 1, 2022, so let‚Äôs download it. The readme for this data is here.\n\ntuesdata &lt;- ???\n\nLet‚Äôs look at it. How can you do that?"
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigating",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigating",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Investigating",
    "text": "Investigating\nWrite code to determine what the 5 most popular dog breeds in 2020 were.\nWhat are the 5 most popular and the 5 least popular dogs across this time frame? There are many ways to do this.\n\n\n\n\n\n\nNeed a hint about how to do this? (Click to expand)\n\n\n\n\n\nCreate a new variable that is a sum of all the ranks from 2013, allowing a composite score of the popularity of each dog breed across this time period."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-1",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-1",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Visualization 1",
    "text": "Visualization 1\nCreate a plot where you take the 12 most popular dogs from 2020, and plot their popularity rank from 2013 to 2020.\n\n\n\n\n\n\nNeed a hint about how to do this? (Click to expand)\n\n\n\n\n\nTo facet, you need to have the variable you want to facet in one column."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-2",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-2",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Visualization 2",
    "text": "Visualization 2\nAlter the aesthetics of this plot until you think it looks good."
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigate-more",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigate-more",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Investigate more",
    "text": "Investigate more\nWhat dog has jumped in the rankings most from 2013 to 2020? What has dropped the most?"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html",
    "title": "ggplot 101 (and üçÖ)",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#what-is-the-tidyverse",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#what-is-the-tidyverse",
    "title": "ggplot 101 (and üçÖ)",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe package ggplot2 is a part of a larger collection of packages called ‚Äúthe tidyverse‚Äù that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier.\nWe can install just ggplot2 or install all of the packages in the core tidyverse (which is what I‚Äôd recommend since we will use the others too), which include:\n\ndplyr: for data manipulation\nggplot2: a ‚Äúgrammar of graphics‚Äù for creating beautiful plots\nreadr: for reading in rectangular data (i.e., Excel-style formatting)\ntibble: using tibbles as modern/better dataframes\nstringr: handling strings (i.e., text or stuff in quotes)\nforcats: for handling categorical variables (i.e., factors) (meow!)\ntidyr: to make ‚Äútidy data‚Äù\npurrr: for enhancing functional programming (also meow!)\n\nWe will be using many of these other packages in this course, but will talk about them as we go. There are more tidyverse packages outside of these core eight, and we will talk about some of them another time.\n\ntl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are required, but I think you‚Äôll find many tidyverse approaches easier and more intuitive than using base R.\n\nYou can find here some examples of comparing tidyverse and base R syntax."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#installing-ggplot-tidyverse",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#installing-ggplot-tidyverse",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Installing ggplot & tidyverse",
    "text": "Installing ggplot & tidyverse\nTo install packages in R that are on the Comprehensive R Archive Network (CRAN), you can use the function install.packages().\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\nWe only need to install packages once. But, every time we want to use them, we need to ‚Äúload‚Äù them, and can do this using the function library().\n\ntl:dr install.packages() once, library() every time."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#data",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#data",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Data",
    "text": "Data\nThe first argument passed to your plot is the data. How did I know that? It‚Äôs in the documentation.\n\n?ggplot()\n\nThe simplest ggplot code you can write, just using the ggplot() function and indicating the data we want to use. Because data is the default first argument, you can actually omit the data = part of this code and it will work just the same.\n\nggplot(data = garden_harvest)\n\n\n\n\nWhy do we not see a plot? Well we haven‚Äôt told R what to plot! We are getting the first ‚Äúbase‚Äù layer of the plot.\nYou can also pipe |&gt; or %&gt;%, the data to the ggplot function. When reading code, you can interpret the pipe as ‚Äúand then.‚Äù Here, take the garden_harvest_tomato data, and then, run ggplot(). Writing code in this way is my preference so I tend to code like this. We talked in more detail about the pipe last week, so you can go back there and read more if you like.\n\ngarden_harvest_tomato %&gt;% \n  ggplot()\n\n\n\n\nStill nothing. Well that‚Äôs what we would expect."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#aesthetic-mappings-aes",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#aesthetic-mappings-aes",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Aesthetic mappings aes()",
    "text": "Aesthetic mappings aes()\nNow that we‚Äôve indicated our data, we can add aesthetics mapping so we can work towards actually see a plot. We want to make a line plot where on the x-axis we have the date (date), and on the y-axis we have how much tomato was harvested (weight).\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight))\n\n\n\n\nSo we have progressed from a blank plot, but we still do not have a plot by basically anyone‚Äôs defintion. Why not?\nEven though we have indicated to R our data and aesthetic mappings, we have not indicated what precisely to do with our data. We have said what we want on x and y (and now we can see those labelled appearing) but we have not indicated what type of plot we want. And, we can do that in the next step, by adding a geom_."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#geoms-geom_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#geoms-geom_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Geoms geom_",
    "text": "Geoms geom_\nNow let‚Äôs indicate what type of plot we want. In this example, we are going to make a line plot, and to do that we will use geom_line()\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line()\n\n\n\n\nWe have a plot! It‚Äôs not a really good plot, but its a plot and we can work from here.\nYou can see that what R has done is take each date, and plotted the total weight of tomatoes harvested on that day. What we can see from this part is that in the beginning of the season, there is little tomato production (this is not surprising to anyone who knows about horticulture or has grown tomatoes before), and production increases as the season progresses. We don‚Äôt see any harvest after mid-October which makes sense because Dr.¬†Lendway lives in Minnesota and probably there was a frost that killed the plants (hence no more üçÖ).\nA note about aesthetic mappings now that we have introduced geoms -aes() can go in two places:\n\nin the ggplot() call, and this means they will inherit for every layer of the plot\nin a specific geom_, and those aesthetics will only be for that specific geom.\n\nSo we can make the same plot we saw above by mapping aesthetics within geom_line().\n\ngarden_harvest_tomato %&gt;%\n  ggplot() +\n  geom_line(aes(x = date, y = weight))\n\n\n\n\nLet‚Äôs say we wanted to see how the harvest of different varieties looks over the summer? We can take the variable variety and map it to the aesthetic color.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line()\n\n\n\n\nThis is till not a beautiful plot, but you are able to see now how you can map a variable of the data (here, variety) to an aesthetic (color).\nAnother important thing to notice here is that now the data is grouped by variety. We are seeing 12 lines instead of 1. This happens automatically under the hood. This ‚Äògrouping‚Äô will be maintained across additional geoms because it is in the global aesthetic mappings for the plot.\nWe can also add more than one geom. Let‚Äôs try adding geom_point() so we can better see exactly which times were sampled in this dataset.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point()\n\n\n\n\nTo more fully make the point about global vs aesthetic mappings, let‚Äôs look at an example.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(color = variety)) +\n  geom_point()\n\n\n\n\nHere, we can see that how the line layer is being grouped by variety, while the points are not. This is because the aesthetic mappings for one geom don‚Äôt inherit to the next one. If we want to also color points by variety, we need to either 1) set this as the global aesthetic mapping or 2) also set aes(color = variety) in geom_point() too.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(color = variety)) +\n  geom_point(aes(color = variety))\n\n\n\n\n\nMapping vs.¬†‚Äòsetting‚Äô\nIf you want to map a variable to an aesthetic, it MUST be within the aes() statement. If you just want to change the color to ‚Äúblue‚Äù for example, it should be outside the aes() statement. Look at the difference.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(color = \"blue\")\n\n\n\n\nLook what happens if we put color = \"blue\" inside the aes() statement.\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight, color = \"blue\")) +\n  geom_line()\n\n\n\n\n\ntl:dr if mapping a variable to an aesthetic, inside aes(), if not, then outside.\n\n\n\nMore about aesthetics\nIt‚Äôs hard to talk about how to map to aesthetics before you add a geom, which is why this content is in this section.\nSo far we have talked about mapping aesthetics to x, y, and color. Below is a list of other aesthetics you can map to:\n\ncolor (or colour if that suits you better) and fill\nIn general color controls the outside/line, and fill controls the inside of a shape. Some geoms will work only with color or fill, and work with both. There are a millions ways to control the color, including by using the R color names (don‚Äôt forget to put them in quotes), or hex codes (e.g., ‚ÄúFF0000‚Äù for red). There are a ton of different color palettes in R like color brewer and I‚Äôd recommend you to think about using colors that are color blind friendly like viridis. Picking a color palette allows continuity across your presentation/manuscript, and can help in the interpretation of your data (e.g., having a divergent scale where darker means more abundant, or pairing colors like light and medium blue to indicate which samples have some kind of relationship).\n\n\nlinetype\nYou can change the style of a line based on a variable\n\ngarden_harvest_tomato %&gt;%\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(linetype = variety))\n\n\n\n\nWow this is a disaster but you can see the point about mapping variables to linetype.\nHere are some different linetypes you can select from:\n\n\n\n\n\nFigure from ggplot documentation\n\n\n\n\n\n\nsize\nYou can also map variables to size. This could be useful if you wanted to say make your points bigger when a fold change is bigger, or bigger when a value is more significant. Below is an example of mapping weight to size in our example dataset.\n\ngarden_harvest_tomato %&gt;%\n  filter(variety %in% c(\"Mortgage Lifter\", \"Brandywine\")) %&gt;%\n  ggplot(aes(x = variety, y = date, size = weight)) +\n  geom_point()\n\n\n\n\n\n\nshape\nYou can also map variables to shape. This could be useful if you want points of one treatment on a scatterplot to be circles, a second treatment triangles, etc. You can combine mapping to shape and color together which is good for those who are concerned about black and white printability, but also easy differentiability when viewed on a computer.\n\n\n\n\n\nFigure from sthda\n\n\n\n\nShapes 0-20 accept only a color aesthetics. Shapes 21-25 accept both a color and fill aesthetic, where color controls the color of the outside of the shape, and fill controls the color of the inside of the shape. I basically always use shapes 21-25 (actually I almost always just use 21).\n\n\nalpha\nSetting alpha allows you to map a variable to the transparency of a part of your plot. So for example, if you had a correlation plot, you could make a strong correlation really dark, while a weak relationship lighter. Alpha can range between 0 and 1, where 0 is totally transparent, and 1 is completely opaque.\nThe next sections we will go over in more detail next week but I want to introduce the idea of these additional layers very briefly."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#scales-scale_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#scales-scale_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Scales scale_",
    "text": "Scales scale_\nUsing scales allows you to control how the data are linked to the visual properties of your plot.\nScales allow you to pick colors, shapes, alphas, lines, transformations (e.g.¬†scaling your axes to a log scale), and others. You can also use scales to set the limits of your plots."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#facets-facet_wrap-and-facet_grid",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#facets-facet_wrap-and-facet_grid",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Facets facet_wrap() and facet_grid()",
    "text": "Facets facet_wrap() and facet_grid()\nFaceting allows you to look at your plots using small multiples, to compare plots that might be otherwise crowded or hard to interpret.\nFaceting can be done using facet_wrap() or facet_grid()."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#coordinates-coord_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#coordinates-coord_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Coordinates coord_",
    "text": "Coordinates coord_\nOften the coordinate system used for your plot will be a simple Cartesian system using x and y. But sometimes, like for making maps or other specialized plots, you will want to change how x and y map to your coordinate system."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#labels-labs",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#labels-labs",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Labels labs()",
    "text": "Labels labs()\nHaving good labels helps your reader (and you, when you come back to the plot in the future) understand what its all about.\nIn the labs() function, you can indicate:\n\nx for the x-axis label\ny for the y-axis label\ntitle for a title\nsubtitle for a subtitle underneath your title\ncaption for a caption\n\nIn theme() you can change characteristics of these labels like their size, fonts, justfication, etc."
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#themes-theme-and-theme_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#themes-theme-and-theme_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Themes theme() and theme_",
    "text": "Themes theme() and theme_\nThemes will control all the non-data parts of your plot. There are some pre-set ‚Äúcomplete‚Äù themes that you can recognize as they‚Äôll be called theme_XXX(), and you can adjust any theme parameters by setting parameters within theme(). There are probably 50 parameters you can set within theme() and they include text size, axis label orientation, the presence of a legend, and many others."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html",
    "title": "R Markdown for Reproducible Research",
    "section": "",
    "text": "Figure from Allison Horst"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#setting-future-you-up-for-success",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#setting-future-you-up-for-success",
    "title": "R Markdown for Reproducible Research",
    "section": "Setting future you up for success",
    "text": "Setting future you up for success\nHow often do you conduct some kind of data analysis, get some results, ignore the project for 6 months, then return back to your data and realize you can‚Äôt figure out exactly what you did?\nThis does not need to happen to you. Be kind to your future self and take steps to avoid this avoidable problem.\nJust like a lab notebook helps you document all of the steps you take in your wet lab work, R Markdown can function like a lab notebook for all your data analyses."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#what-is-r-markdown",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#what-is-r-markdown",
    "title": "R Markdown for Reproducible Research",
    "section": "What is R Markdown?",
    "text": "What is R Markdown?\nRMarkdown provides a framework for saving and executing code, and sharing your results. R Markdown files have the file format .Rmd.\n\n\n\nWhat is R Markdown? from RStudio, Inc. on Vimeo.\n\n\nA minute long introduction to R Markdown\n\nYou can do so many things in R Markdown, from making reports that include text, code, code annotations, figures, tables etc., to creating this course website!\nIf you‚Äôve never used R Markdown before, you can download it using the chunk below. Unlike other R packages, you don‚Äôt need to use library(rmarkdown) to load the package each time you want to use R Markdown.\n\ninstall.packages(\"rmarkdown\")"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#why-i-love-r-markdown",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#why-i-love-r-markdown",
    "title": "R Markdown for Reproducible Research",
    "section": "Why I love R Markdown",
    "text": "Why I love R Markdown\nBasically everything I do in R uses R Markdown. I really value to ability to easily add text and annotate code so that future me, my team, or collaborators can understand what I‚Äôve done and why. I try to write my code in such a way that it could be read by anyone, and is ready to be pushed to our lab‚Äôs Github repositories to act as supplementary materials for our publications. It helps others to be able to truly see what we‚Äôve done, and I think makes science more reproducible and open.\n\n\n\n\n\nFigure from Allison Horst"
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#open-an-r-markdown-document",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#open-an-r-markdown-document",
    "title": "R Markdown for Reproducible Research",
    "section": "Open an R Markdown document",
    "text": "Open an R Markdown document\nOpen up RStudio, go to File &gt; New File &gt; R Markdown. Change the name of the title to something meaningful to you, mine will be called ‚ÄúTrying R Markdown‚Äù.\n\n\n\n\n\n\n\n\n\n\nSaving our file\nWe gave our file a title, but if you look at the top left corner of our new document, you‚Äôll see it‚Äôs called ‚ÄúUntitled1‚Äù. Let‚Äôs change the name to something easier for our future selves to recognize.\n\n\n\n\n\n\n\n\n\nYou can go to File &gt; Save as and place this new R Markdown with your other course materials, and save it with a meaningful name.\n\nAlways having issues with setting your working directory? R Markdown solves this problem! The default working directory is the location of the saved R Markdown file. Voila!\n\n\n\nAn example\nYou‚Äôll note when you create your template document, it is not blank. So you get a sense of what these documents will look like when they are ‚Äúrun,‚Äù let‚Äôs do that with the template doc.\nIn the taskbar of your R Markdown document you will see a button called Knit in your task bar (there is a little ball of yarn with knitting needles next to it). If you click it, R will run all of the code in your R Markdown file, and default compile it to a .html file (though you can select to compile to other file formats).\n\n\n\n\n\n\n\n\n\nLet‚Äôs compare what our document looks like when viewing it in RStudio (left), and after it is knitted (right).\n\n\n\n\n\n\n\n\n\nIf you have a little bit of R experience, you can begin to see how (some of) the content on the left related to the knitted document on the right. We see text, code chunks (but not all of them), and the output of code.\nNow that we have seen a template R Markdown and have 10,000 foot view as to what it is, we can start going through what the different pieces of the document are."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#yaml-header",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#yaml-header",
    "title": "R Markdown for Reproducible Research",
    "section": "YAML Header",
    "text": "YAML Header\nThe YAML (Yet Another Markdown Language, or YAML ain‚Äôt markup language, if you want to learn more about this name and its origins, you can read about it at this stack overflow post) is at the top of your document and is surrounded by ---.\nThe YAML is where you can set the content that will show up on the top of your knitted document.\n\ntitle: ‚ÄúYour title but put it in quotes‚Äù\nauthor: ‚ÄúThe author and still in quotes‚Äù\ndate: the date you want at the top of your doc in quotes. If you want this to be today‚Äôs date (whatever that is) you can use ‚ÄúSys.Date()‚Äù\noutput: will indicate the format of your compiled document. I would recommend for this class you use html_document as it is the richest format. Your output will be a .html file, which you can save or share.\n\nHere‚Äôs a simple example.\n\n---\ntitle: \"This is my descriptive title\"\nauthor: \"Jess\"\ndate: \"May 10, 2022\"\noutput: html_document\n---\n\nIn the YAML, you can also set options that govern how your document will be compiled within output. For example, you can add a table of contents, make that toc float, add a theme, number your sections, and add a button that allows someone to click and access your .Rmd from your knitted .html file. This last one is especially nice because it allows you to send one viewable document, and if someone wants to edit it, they can download and do so easily. This is how I will ask you to submit your class assignments.\nHere‚Äôs an example of what a more customized YAML could look like.\n\n---\ntitle: \"This is my descriptive title\"\nauthor: \"Jess\"\ndate: \"August 9, 2024\"\noutput: \n  html_document: # knit to a .html doc\n    toc: true # creates a table of contents\n    toc_float: true # has that TOC float so you can see it even when you scroll\n    number_sections: true # number your sections\n    theme: flatly # set a global theme\n    code_download: true # insert the code download button\n---\n\nBe sure you pay attention to the indents (which are 2 or 4 spaces, and not tabs), as the YAML is picky here. If your indents are not correct, you will get an error when you knit. Also, if you are missing a colon, your document will knit weirdly or not at all.\n\n\n\n\n\n\nPro tip to avoid a tab/space debacle\n\n\n\nYou can set in RStudio to insert spaces when you click tab by going to Preferences &gt; Code &gt; Use spaces for tab (and indicate 2).\n\n\nAbove are just some of the options that I like to put in my YAML, but there are tons more. Additional output options that are explained on the second page of the RStudio R Markdown cheatsheet."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#text",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#text",
    "title": "R Markdown for Reproducible Research",
    "section": "Text",
    "text": "Text\nUnlike an R script (.R), where R by default interprets anything as code (and material that isn‚Äôt code needed to be commented out by using #), in an R Markdown, the default is text (and code exists only within code chunks or backticks).\nThe text portion of the document is written in a language called Markdown (which is why this format is called R Markdown). The philosophy of Markdown is that it is easy to both write and read. If you want to learn more about markup languages I‚Äôd recommend the this brief explanation by Michael Broe from a past Code Club Session and the Markup language wikipedia page.\nIf we look back to our template R Markdown, we can see there is text written in the same way that we would write in Word document, or an email, and we recognize immediately as text (i.e., the sentence at line 24).\n\n\n\n\n\n\n\n\n\nBut we can also see markup that is perhaps not immediately, recognizable, for example, the **Knit** on line 16. In this case, two asterisks around a word will make it compile to be bolded (second paragraph in the right photo).\nBelow I‚Äôm compiling some commonly used markdown syntax.\n\n\n\n\n\nFigure from R Markdown Reference Guide\n\n\n\n\nNote, the headers are useful and will indicate the levels in your table of contents. You want to use them, and make them meaningful for your document.\nYou can use Markdown to insert tables, images, mathematical formulas, block quotes, and almost anything else you‚Äôd like. You can even write your papers and dissertation in R Markdown. The old version of this course website is made with distill and R Markdown. My lab website is made with R Markdown and the hugo Aper√≥ theme. Quarto and .qmd documents are a slightly updated version of RMarkdown with some new functionality (here you can find some discussion about the different between the two). Here are some links where you can find lots of other Markdown syntactical information:\n\nMarkdown Guide\nR Markdown reference guide\nR Studio R Markdown Cheatsheet\njust google for what you want\n\nSo how is this useful for this course and making your own data analyses more reproducible? You can embed text along with your code, where you provide introductory information, your rationale for data analysis decision making, links and more information about interpreting your code and its output, provide context as to your results, and anything else that would aid your data‚Äôs interpretation.\nAnd, when you want to make a small change, you can do so, knit, and everything else automatically updates."
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#code",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#code",
    "title": "R Markdown for Reproducible Research",
    "section": "Code",
    "text": "Code\nCode chunks are the parts of your R Markdown document where code lives. You can insert a new code chunk by:\n\nusing the keyboard shortcut Cmd + Option + I (Mac) or Ctrl + Alt + I (Windows)\ntyping ```{r} and ``` (and your code goes in between)\nusing the Add Chunk command in the editor toolbar and select R\n\nCode chunks look like this:\n\n\n\n\n\n\n\n\n\nThe code goes in the empty line, and there can be more than 1 bit of code per chunk though I would say if you start having to scroll in your chunk its probably too long.\n\nthe gear allows you to modify the chunk options (we are going to talk more about this)\nthe triangle with the line below it runs all code chunks that come previous to this chunk\nthe play button runs the current chunk\n\nYou can still add comments within a code chunk, but you need to comment them out using #.\n# here is my in chunk annotation\nsome_function()\nWhen you knit your R Markdown, this process will run all of the code in your document. This means if you have code that throws errors or doesn‚Äôt work, your document will not knit. This is some of why I am asking you to knit for your final assignments - all your code needs to work!\nYou can also embed code inline (i.e., within your text).\n\n\n\n\n\n\n\nRaw\nRendered\n\n\n\n\nThere are `r 365*24` hours in a year\nThere are 8760 hours in a year\n\n\nThere are `r nrow(cars)` observations (i.e.¬†rows) in the cars dataset\nThere are 50 observations (i.e.¬†rows) in the cars dataset\n\n\n\nThink about how you could use this ‚Äì embed information from your data analysis (e.g, p-values) within your narrative text without having to hard-code/type it in manually.\n\nAdding options to your code chunks\nYou add options to your code chunks between the {}. This gives R additional instructions regarding running your code and compiling your document. Here are some common examples:\n\necho = FALSE runs your code chunk, displays output, but does not display code in your final doc (this is useful if you want to show a figure but not the code used to create it)\neval = FALSE does not run your code, but does display it in your final doc\ninclude = FALSE runs your code but does not display the code or its output in your final doc\nmessage = FALSE prevents messages from showing up in your final doc\nwarning = FALSE prevents earnings from showing up in your final doc\nfig.height = X and fig.width = Y will allow you to specify the dimensions of your figures (in inches)\nfig.align = can be set to ‚Äúleft‚Äù, ‚Äúright‚Äù, or ‚Äúcenter‚Äù\nfig.cap = \"Your figure caption\" will allow you to set a figure caption\nfig.alt = \"Your alt text\" will allow you to set alt text for screen readers\ncache = TRUE will cache results, meaning if you have a chunk that takes a long time to run, if you haven‚Äôt changed anything and you knit again, the code won‚Äôt run again but access the cache."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#introductions",
    "href": "modules/module1/01_principles/01_principles.html#introductions",
    "title": "1 - Principles of Data Visualization",
    "section": "Introductions üëã",
    "text": "Introductions üëã\n\n\nName\nProgram\nWhy you decided to take this class\nOne thing you hope to learn"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#teaching-team",
    "href": "modules/module1/01_principles/01_principles.html#teaching-team",
    "title": "1 - Principles of Data Visualization",
    "section": "Teaching Team",
    "text": "Teaching Team\nInstructor: Jessica Cooperstone\n‚úâÔ∏è cooperstone.1@osu.edu\n\n\nTA: Daniel Quiroz Moreno\n‚úâÔ∏è quirozmoreno.1@osu.edu\n\n\nOffice hours: go.osu.edu/dataviz-times"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#website",
    "href": "modules/module1/01_principles/01_principles.html#website",
    "title": "1 - Principles of Data Visualization",
    "section": "Website",
    "text": "Website\nIf you have found these slides, you‚Äôve made it to the website! (Good job.)\n\n\nAll course materials will be posted to, or linked to from www.rdataviz.com"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#syllabus",
    "href": "modules/module1/01_principles/01_principles.html#syllabus",
    "title": "1 - Principles of Data Visualization",
    "section": "Syllabus",
    "text": "Syllabus\n\nA full version of the syllabus can be found on Carmen\nA trimmed version of the syllabus can be found on our course site"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#attendance",
    "href": "modules/module1/01_principles/01_principles.html#attendance",
    "title": "1 - Principles of Data Visualization",
    "section": "Attendance",
    "text": "Attendance\n\n\n\nClass will taught in a hybrid, synchronous manner, meaning I expect you to attend class during class time. This attendance can happen in person, or virtually via Zoom I have found that students who attend in person are more engaged, and tend to master material more quickly. But, it is up to you how you want to attend.\n\nI will record class time for those who want to 1) revisit material or 2) can‚Äôt attend (this should be uncommon). These recordings are not to replace coming to class."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#how-class-will-be",
    "href": "modules/module1/01_principles/01_principles.html#how-class-will-be",
    "title": "1 - Principles of Data Visualization",
    "section": "How class will be?",
    "text": "How class will be?\n\nA combination of lecture, code run-throughs, live coding, and hands-on exercises.\nBring a laptop (not tablet) to class with R and RStudio downloaded\nCome with your questions!\nEngage as much as you can!"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#previous-programming-experience",
    "href": "modules/module1/01_principles/01_principles.html#previous-programming-experience",
    "title": "1 - Principles of Data Visualization",
    "section": "Previous programming experience",
    "text": "Previous programming experience\nYou do not need to be an R expert for this class, but I will assume working-level knowledge of R programming. If you have no experience with R, but would still like to take this class, you can. I ask then you get yourself up to speed by taking this free online class https://www.edx.org/course/data-science-r-basics (audit only) before the start of the 3rd week of class."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#assigments",
    "href": "modules/module1/01_principles/01_principles.html#assigments",
    "title": "1 - Principles of Data Visualization",
    "section": "Assigments",
    "text": "Assigments\n\nModule assignments: After each module, there will be an assignment to provide practice for the techniques learned in class.\nClass reflections: After 10 of the 15 weeks, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nRecitation submissions: I ask you submit 8 of 11 recitations to Carmen to show you have made a good faith effort to engage with the course material. I will mark these at 0 or 1 points, with 1 point given for completion of at least 70% of the assignment.\nCapstone assignment: At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#late-assignments",
    "href": "modules/module1/01_principles/01_principles.html#late-assignments",
    "title": "1 - Principles of Data Visualization",
    "section": "Late assignments",
    "text": "Late assignments\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule",
    "href": "modules/module1/01_principles/01_principles.html#schedule",
    "title": "1 - Principles of Data Visualization",
    "section": "üóì Schedule",
    "text": "üóì Schedule\nThis is our tentative class schedule - but subject to change depending on our pacing, and your interests!"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-1",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-1",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 1)",
    "text": "üóìÔ∏è Schedule (part 1)\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-08-26\n1: Principles\nPrinciples of data visualization\n\n\n2025-09-02\n1: Principles\nGood and bad visualizations"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-2",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-2",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 2)",
    "text": "üóìÔ∏è Schedule (part 2)\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-09-09\n2: Coding fundamentals\nR Markdown for reproducible research\n\n\n2025-09-16\n2: Coding fundamentals\nWrangling, the basics\n\n\n2025-09-23\n2: Coding fundamentals\nggplot 101\n\n\n2025-09-30\n2: Coding fundamentals\nThemes, labels, facets (ggplot 102)"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-3",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-3",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 3)",
    "text": "üóìÔ∏è Schedule (part 3)\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-10-07\n3: Data exploration\nData distributions\n\n\n2025-10-14\n3: Data exploration\nCorrelations\n\n\n2025-10-28\n3: Data exploration\nAnnotating statistics"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-4",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-4",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 4)",
    "text": "üóìÔ∏è Schedule (part 4)\nNovember 11 is Veterans Day (no class)  November 25 will be asynchronous\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-11-04\n4: Putting it together\nPrincipal components analysis\n\n\n2025-11-18\n4: Putting it together\nInteractive plots\n\n\n2025-11-25\n4: Putting it together\nManhattan plots and making lots of plots at once (asynchronous)\n\n\n2025-12-02\n4: Putting it together\nggplot extension packages"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#capstone-prep",
    "href": "modules/module1/01_principles/01_principles.html#capstone-prep",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Capstone prep",
    "text": "üóìÔ∏è Capstone prep\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-10-21\nCapstone prep\nCapstone plan prep, open session\n\n\n2025-12-09\nCapstone prep\nCapstone assignment, open session"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#there-may-be-a-data-dinosaur",
    "href": "modules/module1/01_principles/01_principles.html#there-may-be-a-data-dinosaur",
    "title": "1 - Principles of Data Visualization",
    "section": "There may be a data dinosaur ü¶ñ",
    "text": "There may be a data dinosaur ü¶ñ\n\nFigure by Alberto Cairo"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-understand-distribution",
    "href": "modules/module1/01_principles/01_principles.html#to-understand-distribution",
    "title": "1 - Principles of Data Visualization",
    "section": "To understand distribution",
    "text": "To understand distribution\nAnscombe‚Äôs quartet üéª"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-discover-data-secrets",
    "href": "modules/module1/01_principles/01_principles.html#to-discover-data-secrets",
    "title": "1 - Principles of Data Visualization",
    "section": "To discover data secrets",
    "text": "To discover data secrets\n\nFigures from Justin Matejka and George Fitzmaurice"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-convey-our-message",
    "href": "modules/module1/01_principles/01_principles.html#to-convey-our-message",
    "title": "1 - Principles of Data Visualization",
    "section": "To convey our message",
    "text": "To convey our message\n\nBilbrey et al., New Phytologist, 2021"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#the-data-visualization-process",
    "href": "modules/module1/01_principles/01_principles.html#the-data-visualization-process",
    "title": "1 - Principles of Data Visualization",
    "section": "The data visualization process",
    "text": "The data visualization process\n\n\nFigure adapted from one by Rick Scavetta"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability",
    "href": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability",
    "title": "1 - Principles of Data Visualization",
    "section": "Simple changes improve interpretability",
    "text": "Simple changes improve interpretability"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability-1",
    "href": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Simple changes improve interpretability",
    "text": "Simple changes improve interpretability"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues",
    "href": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues",
    "title": "1 - Principles of Data Visualization",
    "section": "Encoding data with easy-to-process visual clues",
    "text": "Encoding data with easy-to-process visual clues\nLength is easier to see than angles or areas."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues-1",
    "href": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Encoding data with easy-to-process visual clues",
    "text": "Encoding data with easy-to-process visual clues\nLength is easier to see than angles or areas."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#color-scales-should-be-intuitive-and-accessible",
    "href": "modules/module1/01_principles/01_principles.html#color-scales-should-be-intuitive-and-accessible",
    "title": "1 - Principles of Data Visualization",
    "section": "Color scales should be intuitive and accessible",
    "text": "Color scales should be intuitive and accessible\n\n\nThese are not."
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-1",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-2",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-2",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-1",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-2",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-2",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti",
    "href": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti",
    "title": "1 - Principles of Data Visualization",
    "section": "Avoid figure spaghetti üçù",
    "text": "Avoid figure spaghetti üçù"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti-1",
    "href": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Avoid figure spaghetti üçù",
    "text": "Avoid figure spaghetti üçù"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#be-consistent-among-figures",
    "href": "modules/module1/01_principles/01_principles.html#be-consistent-among-figures",
    "title": "1 - Principles of Data Visualization",
    "section": "Be consistent among figures",
    "text": "Be consistent among figures\n\nUse the same color schemes/shapes across figures\nIf you‚Äôre ordering/grouping, do so in the same manner"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#make-sure-your-plot-has-a-clear-message",
    "href": "modules/module1/01_principles/01_principles.html#make-sure-your-plot-has-a-clear-message",
    "title": "1 - Principles of Data Visualization",
    "section": "Make sure your plot has a clear message üçï",
    "text": "Make sure your plot has a clear message üçï"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#marie-kondo-your-plots",
    "href": "modules/module1/01_principles/01_principles.html#marie-kondo-your-plots",
    "title": "1 - Principles of Data Visualization",
    "section": "Marie Kondo your plots",
    "text": "Marie Kondo your plots\nDeclutter, and keep only parts that are informative (and spark joy) üòª\n\nFrom https://socviz.co/lookatdata.html"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#oral-presentation-and-publication-figures-might-not-be-the-same",
    "href": "modules/module1/01_principles/01_principles.html#oral-presentation-and-publication-figures-might-not-be-the-same",
    "title": "1 - Principles of Data Visualization",
    "section": "Oral presentation and publication figures might not be the same",
    "text": "Oral presentation and publication figures might not be the same"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#what-should-you-think-about-when-making-visualizations",
    "href": "modules/module1/01_principles/01_principles.html#what-should-you-think-about-when-making-visualizations",
    "title": "1 - Principles of Data Visualization",
    "section": "What should you think about when making visualizations?",
    "text": "What should you think about when making visualizations?\n\nWho are you talking to? üì¢\nWhat are you trying to convey? üìù\nHow can you fairly represent your data? üöØ"
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#next-class",
    "href": "modules/module1/01_principles/01_principles.html#next-class",
    "title": "1 - Principles of Data Visualization",
    "section": "Next class",
    "text": "Next class\nSubmit (through Carmen) by Monday 9/1/2025 at 11:59pm:\n\n1 good visualization (and a paragraph on why its good)\n1 bad visualization (and a paragraph on why its bad)\n\n\nWe will go through these next week. Daniel will pick the best good and the best bad visualizations and there will be prizes! üéâ\n\n\n\n01 Principles, ¬© Jessica Cooperstone, 2025"
  }
]