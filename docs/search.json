[
  {
    "objectID": "modules/module1/02_good-and-bad/02_goodbad.html#what-was-easier-to-find",
    "href": "modules/module1/02_good-and-bad/02_goodbad.html#what-was-easier-to-find",
    "title": "2 - Good and Bad Data Visualizations",
    "section": "What was easier to find?",
    "text": "What was easier to find?\n\nGood üëè visualizations\nBad üò° visualizations",
    "crumbs": [
      "Module 1 - Principles",
      "Week 2 - Good and bad visualizations"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd_recitation.html#r-projects",
    "href": "modules/module2/03_rmarkdown/03_Rmd_recitation.html#r-projects",
    "title": "R Markdown for Reproducible Research Recitation",
    "section": "R Projects",
    "text": "R Projects\nBefore we start with R Markdown, start by creating yourself an R Project for this course.\nYou can do this by going to File &gt; New Project. You can create the project within a New Directory (you don‚Äôt already have a folder for your course materials) or within an Existing Directory. Create your project at the top level of this directory.\nI would recommend that you do not Save workspace to .RData on exit (this can lead to reproducibility problems).\nYou can learn more about R Projects here.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "We are going to practice using ggplot2 today, focusing wrangling data, mapping variables to aesthetics, and adding geoms.\nWe are going to use data from the TidyTuesday project. For this recitation, we are going to use the Giant Pumpkins data which is collected from the Great Pumpkin Commonwealth. You can learn more about how the data is structured here.\nToday, you are going to make this plot:\n\n\n\nOur plot for today\n\n\n\nQuestion: How can we replicate this plot?\n\n\n\nWork with real messy data\n\nImport data from github\nModify variables types\nSelect observations with certain values\nWrangle some more\nPractice plotting\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\n\n\nWhen you open the github page you will see a file called pumpkins.csv. You also are introduced about the details of the data (i.e., variables, variable types, descriptions), as well as how to import the it.\nFirst thing first, we are going to import the data by reading the csv file with the Github link provided. You can also read the data in by downloading it manually, saving it, and then loading it.\n# load libraries\nlibrary(tidyverse)\n\n# Import giant pumpkins data\npumpkins_raw &lt;- readr::read_csv('WHAT-GOES-HERE??')\nOnce we have imported our data, how can you check it out?\n\nglimpse(pumpkins_raw)\n\nRows: 28,065\nColumns: 14\n$ id                &lt;chr&gt; \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2‚Ä¶\n$ place             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"‚Ä¶\n$ weight_lbs        &lt;chr&gt; \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1‚Ä¶\n$ grower_name       &lt;chr&gt; \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen‚Ä¶\n$ city              &lt;chr&gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo‚Ä¶\n$ state_prov        &lt;chr&gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc‚Ä¶\n$ country           &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"‚Ä¶\n$ gpc_site          &lt;chr&gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum‚Ä¶\n$ seed_mother       &lt;chr&gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar‚Ä¶\n$ pollinator_father &lt;chr&gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open‚Ä¶\n$ ott               &lt;chr&gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", ‚Ä¶\n$ est_weight        &lt;chr&gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141‚Ä¶\n$ pct_chart         &lt;chr&gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0‚Ä¶\n$ variety           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nDo some of these variables contain more than one piece of information?\n\nWhat is embedded within the variable id?\nWhat type of info does id contain?\nWhat types of variables are place and weight_lbs? Are there any limitations to plotting these variable types?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#goals-of-this-recitation",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#goals-of-this-recitation",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "Work with real messy data\n\nImport data from github\nModify variables types\nSelect observations with certain values\nWrangle some more\nPractice plotting\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\n\n\nWhen you open the github page you will see a file called pumpkins.csv. You also are introduced about the details of the data (i.e., variables, variable types, descriptions), as well as how to import the it.\nFirst thing first, we are going to import the data by reading the csv file with the Github link provided. You can also read the data in by downloading it manually, saving it, and then loading it.\n# load libraries\nlibrary(tidyverse)\n\n# Import giant pumpkins data\npumpkins_raw &lt;- readr::read_csv('WHAT-GOES-HERE??')\nOnce we have imported our data, how can you check it out?\n\nglimpse(pumpkins_raw)\n\nRows: 28,065\nColumns: 14\n$ id                &lt;chr&gt; \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2‚Ä¶\n$ place             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"‚Ä¶\n$ weight_lbs        &lt;chr&gt; \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"139.00\", \"1‚Ä¶\n$ grower_name       &lt;chr&gt; \"Ellenbecker, Todd & Sequoia\", \"Razo, Steve\", \"Ellen‚Ä¶\n$ city              &lt;chr&gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo‚Ä¶\n$ state_prov        &lt;chr&gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc‚Ä¶\n$ country           &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"‚Ä¶\n$ gpc_site          &lt;chr&gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum‚Ä¶\n$ seed_mother       &lt;chr&gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar‚Ä¶\n$ pollinator_father &lt;chr&gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open‚Ä¶\n$ ott               &lt;chr&gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", ‚Ä¶\n$ est_weight        &lt;chr&gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141‚Ä¶\n$ pct_chart         &lt;chr&gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0‚Ä¶\n$ variety           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n\n\nDo some of these variables contain more than one piece of information?\n\nWhat is embedded within the variable id?\nWhat type of info does id contain?\nWhat types of variables are place and weight_lbs? Are there any limitations to plotting these variable types?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#turn-one-character-column-into-two",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#turn-one-character-column-into-two",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Turn one character column into two ‚úÇÔ∏è",
    "text": "Turn one character column into two ‚úÇÔ∏è\nFrom both looking at the data, and reading about the variable id on the documentation page, you can see that it contains two pieces of information. To be able to interact with them separately, we need to separate this column into two columns (i.e.¬†year and type).\nTry doing this with the function separate() from the tidyr package. Or you can use separate_wider_delim().\npumpkins_raw %&gt;%\n   separate(WHAT-GOES-HERE)",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#select-observations-by-their-values",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#select-observations-by-their-values",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Select observations by their values üéÉ",
    "text": "Select observations by their values üéÉ\nNow that you separated the year and crop type, keep only the data for Giant Pumpkins.\n\n\nNeed a hint?\n\nHint, you can use the filter() function from the dplyr package.\n\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\npumpkins_raw %&gt;%\n   filter(...predicate/condition...) \nNow that you hae kept only the Giant Pumpkins, retain only the observations that were the winners (i.e.¬†those in first place).\npumpkins_raw %&gt;%\n   filter(...predicate/condition...) %&gt;%\n   filter(...predicate/condition...)",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#remove-pesky-strings",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#remove-pesky-strings",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Remove pesky strings üòë",
    "text": "Remove pesky strings üòë\nIf we were to try and plot our data as it is now we would not get our desired outcome. But try it anyway.\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  code-to-plot\nWhat is weird about this y-axis?\n\n\n\n\n\n\n\n\n\nIf you take a look at the variables of the weight_lbs column, it contains a commas as the thousand separator. R does not recognize this as a number (and instead views it as a character) so and it has to be removed prior changing the column type. There are a few ways to do this.\nLet‚Äôs practice handling strings by removing the comma. You can use str_remove() function from the tidyverse package stringr. Here is an example of how str_remove() works.\n\nwrong_number &lt;- \"700,057.58\"\nwrong_number\n\n[1] \"700,057.58\"\n\n\nUsing str_remove()\n\nstringr::str_remove(string = wrong_number, pattern = \",\")\n\n[1] \"700057.58\"\n\n\nRemember, we don‚Äôt want to just remove the thousands place comma in one number, we want to edit the dataset to remove the comma.\nIn this case, you can embed str_remove() within the mutate() function, which can create new variables or modify existing ones. In our case, we want to modify the weight_lbs variable to remove the comma. Give it a try.\n\n\n\n\n\nIllustration taken from https://www.allisonhorst.com\n\n\n\n\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) \nCommas, gone! üëèüëèüëè",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#convert-character-to-numeric",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#convert-character-to-numeric",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Convert character to numeric üî¢",
    "text": "Convert character to numeric üî¢\nNow the comma is gone, you can simply change the variable weight_lbs from a character to numeric, so it can be plotted like a number., to change the column type, we are going to use the as.numeric() function. Here‚Äôs some example about how to use as.numeric().\n\nright_number_chr &lt;- stringr::str_remove(string = wrong_number, pattern = \",\")\n\nright_number_number &lt;- as.numeric(right_number_chr)\nclass(right_number_number)\n\n[1] \"numeric\"\n\n\nLet‚Äôs add this to our growing pipe.\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) %&gt;%\n  mutate(variable = as.numeric(arguments-here))",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#where-are-the-lines",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#where-are-the-lines",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Where are the lines?",
    "text": "Where are the lines?\nWhy do you think the lines aren‚Äôt showing up?\n\n\nNeed a hint?\n\nHint - look at what variable type year is.\n\nHow can you fix this? Hint, you can change year to either numeric or a date. Try using some functions from the package lubridate or the function as.Date().\npumpkins_raw %&gt;%\n  code-to-separate %&gt;%\n  code-to-filter %&gt;%\n  mutate(variable = str_remove(arguments-here)) %&gt;%\n  mutate(variable = as.numeric(arguments-here)) %&gt;%\n  mutate(do-something-with-your-date) %&gt;%\n  code-to-plot",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#playing-around",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation.html#playing-around",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Playing around",
    "text": "Playing around\nTry using different geoms besides geom_point() and geom_line(). Which might make sense in this situation?\nCan you color all the lines blue?\nCan you color the data based on year?\nCan you change color and change shape based on country?\nCan you make a plot showing the distribution of weights of all giant pumpkins entered in 2021?\nCan you make a boxplot showing the distribution of weights of all giant pumpkins across all years? Also can you add all the datapoints on top of the boxplot? Is this a good idea? Might there be a better geom to use than a boxplot?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html",
    "title": "ggplot 101 (and üçÖ)",
    "section": "",
    "text": "Figure from Allison Horst",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#what-is-the-tidyverse",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#what-is-the-tidyverse",
    "title": "ggplot 101 (and üçÖ)",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe package ggplot2 is a part of a larger collection of packages called ‚Äúthe tidyverse‚Äù that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier.\nWe can install just ggplot2 or install all of the packages in the core tidyverse (which is what I‚Äôd recommend since we will use the others too), which include:\n\ndplyr: for data manipulation\nggplot2: a ‚Äúgrammar of graphics‚Äù for creating beautiful plots\nreadr: for reading in rectangular data (i.e., Excel-style formatting)\ntibble: using tibbles as modern/better dataframes\nstringr: handling strings (i.e., text or stuff in quotes)\nforcats: for handling categorical variables (i.e., factors) (meow!)\ntidyr: to make ‚Äútidy data‚Äù\npurrr: for enhancing functional programming (also meow!)\n\nWe will be using many of these other packages in this course, but will talk about them as we go. There are more tidyverse packages outside of these core eight, and we will talk about some of them another time.\n\ntl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are required, but I think you‚Äôll find many tidyverse approaches easier and more intuitive than using base R.\n\nYou can find here some examples of comparing tidyverse and base R syntax.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#installing-ggplot-tidyverse",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#installing-ggplot-tidyverse",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Installing ggplot & tidyverse",
    "text": "Installing ggplot & tidyverse\nTo install packages in R that are on the Comprehensive R Archive Network (CRAN), you can use the function install.packages().\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\nWe only need to install packages once. But, every time we want to use them, we need to ‚Äúload‚Äù them, and can do this using the function library().\n\ntl:dr install.packages() once, library() every time.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#data",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#data",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Data",
    "text": "Data\nThe first argument passed to your plot is the data. How did I know that? It‚Äôs in the documentation.\n\n?ggplot()\n\nThe simplest ggplot code you can write, just using the ggplot() function and indicating the data we want to use. Because data is the default first argument, you can actually omit the data = part of this code and it will work just the same.\n\nggplot(data = garden_harvest)\n\n\n\n\n\n\n\n\nWhy do we not see a plot? Well we haven‚Äôt told R what to plot! We are getting the first ‚Äúbase‚Äù layer of the plot.\nYou can also pipe |&gt; or %&gt;%, the data to the ggplot function. When reading code, you can interpret the pipe as ‚Äúand then.‚Äù Here, take the garden_harvest_tomato data, and then, run ggplot(). Writing code in this way is my preference so I tend to code like this. We talked in more detail about the pipe last week, so you can go back there and read more if you like.\n\ngarden_harvest_tomato |&gt; \n  ggplot()\n\n\n\n\n\n\n\n\nStill nothing. Well that‚Äôs what we would expect.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#aesthetic-mappings-aes",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#aesthetic-mappings-aes",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Aesthetic mappings aes()",
    "text": "Aesthetic mappings aes()\nNow that we‚Äôve indicated our data, we can add aesthetics mapping so we can work towards actually see a plot. We want to make a line plot where on the x-axis we have the date (date), and on the y-axis we have how much tomato was harvested (weight).\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight))\n\n\n\n\n\n\n\n\nSo we have progressed from a blank plot, but we still do not have a plot by basically anyone‚Äôs defintion. Why not?\nEven though we have indicated to R our data and aesthetic mappings, we have not indicated what precisely to do with our data. We have said what we want on x and y (and now we can see those labelled appearing) but we have not indicated what type of plot we want. And, we can do that in the next step, by adding a geom_.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#geoms-geom_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#geoms-geom_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Geoms geom_",
    "text": "Geoms geom_\nNow let‚Äôs indicate what type of plot we want. In this example, we are going to make a line plot, and to do that we will use geom_line()\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe have a plot! It‚Äôs not a really good plot, but its a plot and we can work from here.\nYou can see that what R has done is take each date, and plotted the total weight of tomatoes harvested on that day. What we can see from this part is that in the beginning of the season, there is little tomato production (this is not surprising to anyone who knows about horticulture or has grown tomatoes before), and production increases as the season progresses. We don‚Äôt see any harvest after mid-October which makes sense because Dr.¬†Lendway lives in Minnesota and probably there was a frost that killed the plants (hence no more üçÖ).\nA note about aesthetic mappings now that we have introduced geoms -aes() can go in two places:\n\nin the ggplot() call, and this means they will inherit for every layer of the plot\nin a specific geom_, and those aesthetics will only be for that specific geom.\n\nSo we can make the same plot we saw above by mapping aesthetics within geom_line().\n\ngarden_harvest_tomato |&gt;\n  ggplot() +\n  geom_line(aes(x = date, y = weight))\n\n\n\n\n\n\n\n\nLet‚Äôs say we wanted to see how the harvest of different varieties looks over the summer? We can take the variable variety and map it to the aesthetic color.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis is till not a beautiful plot, but you are able to see now how you can map a variable of the data (here, variety) to an aesthetic (color).\nAnother important thing to notice here is that now the data is grouped by variety. We are seeing 12 lines instead of 1. This happens automatically under the hood. This ‚Äògrouping‚Äô will be maintained across additional geoms because it is in the global aesthetic mappings for the plot.\nWe can also add more than one geom. Let‚Äôs try adding geom_point() so we can better see exactly which times were sampled in this dataset.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nTo more fully make the point about global vs aesthetic mappings, let‚Äôs look at an example.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(color = variety)) +\n  geom_point()\n\n\n\n\n\n\n\n\nHere, we can see that how the line layer is being grouped by variety, while the points are not. This is because the aesthetic mappings for one geom don‚Äôt inherit to the next one. If we want to also color points by variety, we need to either 1) set this as the global aesthetic mapping or 2) also set aes(color = variety) in geom_point() too.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(color = variety)) +\n  geom_point(aes(color = variety))\n\n\n\n\n\n\n\n\n\nMapping vs.¬†‚Äòsetting‚Äô\nIf you want to map a variable to an aesthetic, it MUST be within the aes() statement. If you just want to change the color to ‚Äúblue‚Äù for example, it should be outside the aes() statement. Look at the difference.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(color = \"blue\")\n\n\n\n\n\n\n\n\nLook what happens if we put color = \"blue\" inside the aes() statement.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = \"blue\")) +\n  geom_line()\n\n\n\n\n\n\n\n\n\ntl:dr if mapping a variable to an aesthetic, inside aes(), if not, then outside.\n\n\n\nMore about aesthetics\nIt‚Äôs hard to talk about how to map to aesthetics before you add a geom, which is why this content is in this section.\nSo far we have talked about mapping aesthetics to x, y, and color. Below is a list of other aesthetics you can map to:\n\ncolor (or colour if that suits you better) and fill\nIn general color controls the outside/line, and fill controls the inside of a shape. Some geoms will work only with color or fill, and work with both. There are a millions ways to control the color, including by using the R color names (don‚Äôt forget to put them in quotes), or hex codes (e.g., ‚ÄúFF0000‚Äù for red). There are a ton of different color palettes in R like color brewer and I‚Äôd recommend you to think about using colors that are color blind friendly like viridis. Picking a color palette allows continuity across your presentation/manuscript, and can help in the interpretation of your data (e.g., having a divergent scale where darker means more abundant, or pairing colors like light and medium blue to indicate which samples have some kind of relationship).\n\n\nlinetype\nYou can change the style of a line based on a variable\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line(aes(linetype = variety))\n\n\n\n\n\n\n\n\nWow this is a disaster but you can see the point about mapping variables to linetype.\nHere are some different linetypes you can select from:\n\n\n\n\n\nFigure from ggplot documentation\n\n\n\n\n\n\nsize\nYou can also map variables to size. This could be useful if you wanted to say make your points bigger when a fold change is bigger, or bigger when a value is more significant. Below is an example of mapping weight to size in our example dataset.\n\ngarden_harvest_tomato |&gt;\n  filter(variety %in% c(\"Mortgage Lifter\", \"Brandywine\")) |&gt;\n  ggplot(aes(x = variety, y = date, size = weight)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nshape\nYou can also map variables to shape. This could be useful if you want points of one treatment on a scatterplot to be circles, a second treatment triangles, etc. You can combine mapping to shape and color together which is good for those who are concerned about black and white printability, but also easy differentiability when viewed on a computer.\n\n\n\n\n\nFigure from sthda\n\n\n\n\nShapes 0-20 accept only a color aesthetics. Shapes 21-25 accept both a color and fill aesthetic, where color controls the color of the outside of the shape, and fill controls the color of the inside of the shape. I basically always use shapes 21-25 (actually I almost always just use 21).\n\n\nalpha\nSetting alpha allows you to map a variable to the transparency of a part of your plot. So for example, if you had a correlation plot, you could make a strong correlation really dark, while a weak relationship lighter. Alpha can range between 0 and 1, where 0 is totally transparent, and 1 is completely opaque.\nThe next sections we will go over in more detail next week but I want to introduce the idea of these additional layers very briefly.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#scales-scale_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#scales-scale_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Scales scale_",
    "text": "Scales scale_\nUsing scales allows you to control how the data are linked to the visual properties of your plot.\nScales allow you to pick colors, shapes, alphas, lines, transformations (e.g.¬†scaling your axes to a log scale), and others. You can also use scales to set the limits of your plots.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#facets-facet_wrap-and-facet_grid",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#facets-facet_wrap-and-facet_grid",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Facets facet_wrap() and facet_grid()",
    "text": "Facets facet_wrap() and facet_grid()\nFaceting allows you to look at your plots using small multiples, to compare plots that might be otherwise crowded or hard to interpret.\nFaceting can be done using facet_wrap() or facet_grid().",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#coordinates-coord_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#coordinates-coord_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Coordinates coord_",
    "text": "Coordinates coord_\nOften the coordinate system used for your plot will be a simple Cartesian system using x and y. But sometimes, like for making maps or other specialized plots, you will want to change how x and y map to your coordinate system.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#labels-labs",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#labels-labs",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Labels labs()",
    "text": "Labels labs()\nHaving good labels helps your reader (and you, when you come back to the plot in the future) understand what its all about.\nIn the labs() function, you can indicate:\n\nx for the x-axis label\ny for the y-axis label\ntitle for a title\nsubtitle for a subtitle underneath your title\ncaption for a caption\n\nIn theme() you can change characteristics of these labels like their size, fonts, justfication, etc.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101.html#themes-theme-and-theme_",
    "href": "modules/module2/05_ggplot101/05_ggplot101.html#themes-theme-and-theme_",
    "title": "ggplot 101 (and üçÖ)",
    "section": "Themes theme() and theme_",
    "text": "Themes theme() and theme_\nThemes will control all the non-data parts of your plot. There are some pre-set ‚Äúcomplete‚Äù themes that you can recognize as they‚Äôll be called theme_XXX(), and you can adjust any theme parameters by setting parameters within theme(). There are probably 50 parameters you can set within theme() and they include text size, axis label orientation, the presence of a legend, and many others.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 5 - ggplot101"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "",
    "text": "Figure from Allison Horst",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Introduction",
    "text": "Introduction\nWe will will build upon our last lesson on ggplot101 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, and geoms. Today we will focus on some of the other more commonly adjusted layers:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data.\n\nlibrary(tidyverse)\nlibrary(gardenR)\n\nAnd let‚Äôs remember whats in garden_harvest.\n\nglimpse(garden_harvest)\n\nRows: 781\nColumns: 5\n$ vegetable &lt;chr&gt; \"lettuce\", \"radish\", \"lettuce\", \"lettuce\", \"radish\", \"lettuc‚Ä¶\n$ variety   &lt;chr&gt; \"reseed\", \"Garden Party Mix\", \"reseed\", \"reseed\", \"Garden Pa‚Ä¶\n$ date      &lt;date&gt; 2020-06-06, 2020-06-06, 2020-06-08, 2020-06-09, 2020-06-11,‚Ä¶\n$ weight    &lt;dbl&gt; 20, 36, 15, 10, 67, 12, 9, 8, 53, 19, 14, 10, 48, 58, 8, 121‚Ä¶\n$ units     &lt;chr&gt; \"grams\", \"grams\", \"grams\", \"grams\", \"grams\", \"grams\", \"grams‚Ä¶",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#facets",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#facets",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Facets",
    "text": "Facets\nFaceting allows to create small multiples of plots, enabling the easy comparison across the entirety of your data. A benefit of plots like this is they are all structured the same way, so once you understand one, you can begin to look at trends across groups/treatments/conditions simply and easily.\nHere is a more infographic example of using small multiples.\n\n\n\n\n\nFigure from Five Thirty Eight\n\n\n\n\nSo we can easily see that states with more of a maroon color have a lower than average life expectancy, while those that are higher than average are orange. We also can see easily where each state is on the map, so we can begin to understand how geography is related to life expectancy. We can also see which states have gotten better (i.e.¬†their people live longer) with time, and those that haven‚Äôt. And this is all with a quick glance!\nIf we look back to the plot we were using as our example last week, can see how we have a plot faceted by tomato variety.\nFirst lets select only the data for tomatoes.\n\n# filter data to include only tomatoes \n# filter() is a useful function from dplyr (part of tidyverse)\n# it allows us to select observations based on their values\ngarden_harvest_tomato &lt;- garden_harvest |&gt;\n  filter(vegetable == \"tomatoes\")\n\nLet‚Äôs remember what our base plot is currently looking like.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) \n\n\n\n\n\n\n\n\nSee how crowded this is? I think faceting might help us better see our data by variety.\nThere are two functions that allow you to facet:\n\nfacet_wrap: allows to lay out your facets in a wrapped type. You can use facet_wrap if you have 1 variable you‚Äôd like to facet on.\nfacet_grid: allows you to lay out your facets in a grid. You can use facet_grid if you have 1 or 2 variables you‚Äôd like to facet on.\n\nThere are a few different sets of syntax that work for faceting, but I think this is the most intuitive.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety))\n\n\n\n\n\n\n\n\nWe will get a very reasonably different looking plot with facet_grid with the default settings.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_grid(vars(variety))\n\n\n\n\n\n\n\n\nNote because you have provided only one variable, ggplot has put that facet in one row.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_grid(cols = vars(variety))\n\n\n\n\n\n\n\n\nWe can make the faceting go by column, but this is also looks bad in this case\nHowever, you might be thinking now that if you have two variables, and you want to facet by the combination of them, you could do that with facet_grid. Here is an example with the mpg dataset from the tidyverse (since there isn‚Äôt really good data to demonstrate this from garden_harevst).\n\nmpg |&gt;\n  ggplot(aes(x = cty, y = hwy)) + # city and highway gas mileage\n  geom_point() +\n  facet_grid(cols = vars(class), # category of car\n             rows = vars(drv)) # type of drive train, 4 wheel, front, rear\n\n\n\n\n\n\n\n\nThe default in both facet_wrap and facet_grid are for the x and y-axis to be fixed and constant among all the plots. This is often what you want to take advance of the comparisons between small multiples, but this is something you can change if you want. You can adjust the scales within facets to:\n\nscales = \"fixed\": both the x- and y-axes are fixed for all plots to be the same (this is the default)\nscales = \"free\": both the x- and y-axes are set per plot\nscales = \"free_x\": the x-axis scales are free between plots\nscales = \"free_y\": the y-axis scales are free between plots\n\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety), scales = \"free\")\n\n\n\n\n\n\n\n\nDo note how this affects how easy it is to compare among the facets now. Also note that in this case, since we have all the same x-axis labels between the plots, when we set scales = \"free\" it really only changes the y, making it functionally equivalent to scales = \"free_y\". This will not hold true in other situations.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#scales",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#scales",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Scales",
    "text": "Scales\nUsing scales allows you to control how the data are linked to the visual properties of your plot. Some books will include labels as a part of scales but I‚Äôm going to cover them separately.\nScales allow you to pick colors, shapes, alphas, lines, transformations (e.g.¬†scaling your axes to a log scale), and others. You can also use scales to set the limits of your plots.\nScales functions start with scale_.\nHere are some common things you might do with the scale_ functions.\n\nPosition scales\nYou can set position scales for dates/times (like we have here), x and y data, binned data, continuous data, and for discrete data.\nHere is one example for date/time data, which is what we have here. The date-time POSIX standards are listed here.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_x_date(date_labels = \"%m/%y\")\n\n\n\n\n\n\n\n\nHere is another example (which isn‚Äôt very good) about how you can also use scales to log transform your axes. Remember you are not actually transforming your data, you are just transforming the axis labels.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\nColor scales\nYou can set color scales for continuous and binned colour data, sequential, diverging and qualitative data using ColorBrewer, and perceptually uniform scales using viridis from viridisLite\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_color_brewer(palette = \"Set3\")\n\n\n\n\n\n\n\n\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\nYou can play around with scales to see all you can do with it.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#labels",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#labels",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Labels",
    "text": "Labels\nHaving good labels helps your reader (and you, when you come back to the plot in the future) understand what its all about.\nIn the labs() function, you can indicate:\n\nx for the x-axis label\ny for the y-axis label\ntitle for a title\nsubtitle for a subtitle underneath your title\ncaption for a caption\n\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) + \n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\",\n       alt = \"A plot showing 12 varieties of tomatoes and how much of each of them Dr. Lisa Lendway harvested in her home garden in 2022. The biggest producers were amish paste and better boy, which had earlier season peaks, and mortgage lifter, old german, and volunteer plants were more productive towards the end of the season.\")\n\n\n\n\n\n\n\n\nYou can also use get_alt_text() to pull the alt-text for an image. This will come back with an empty string if there is no alt-text provided.\nIn theme() you can change characteristics of these labels like their size, fonts, justification, etc.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#themes",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#themes",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Themes",
    "text": "Themes\nThemes will control all the non-data parts of your plot. There are some pre-set ‚Äúcomplete‚Äù themes that you can recognize as they‚Äôll be called theme_*(), and you can adjust any theme parameters by setting parameters within theme(). There are probably 50 parameters you can set within theme() and they include text size, axis label orientation, the presence of a legend, and many others.\n\nComplete themes from ggplot\nThere are some pre-set complete themes that control the look of the non-data displays. Below are some examples. theme_grey() is the default ggplot2 theme.\n\ntheme_minimal()\nThis is the one I use the most.\n\nbase_plot &lt;- garden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) + \n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\nbase_plot + theme_minimal()\n\n\n\n\n\n\n\n\n\n\ntheme_classic()\nThis is another nice lightweight theme.\n\nbase_plot + theme_classic()\n\n\n\n\n\n\n\n\n\n\ntheme_bw()\nIn black and white.\n\nbase_plot + theme_bw()\n\n\n\n\n\n\n\n\n\n\ntheme_dark()\nFor dark-mode aficionados.\n\nbase_plot + theme_dark()\n\n\n\n\n\n\n\n\n\n\ntheme_void()\nHere is a theme with very little if you really want only the bare bones.\n\nbase_plot + theme_void()\n\n\n\n\n\n\n\n\n\n\n\nComplete themes from other packages\nThe packages ggthemes and hrbrthemes have some nice themes you might be interested in.\n\nlibrary(ggthemes)\nlibrary(hrbrthemes)\n\n\ntheme_tufte()\nAnother lightweight theme\n\nbase_plot + theme_tufte()\n\n\n\n\n\n\n\n\n\n\ntheme_excel()\nIn case you find yourself wishing your plots looked more Excel 2005.\n\nbase_plot + theme_excel()\n\n\n\n\n\n\n\n\n\n\ntheme_ipsum()\nYou need to have Roboto Condensed for this.\n\nbase_plot + theme_ipsum()\n\n\n\n\n\n\n\n\n\n\n\nModify components of a theme\nIf there is a part of the non-data components of your plot you want to change, chances are you do this using theme(). You can also start with a complete theme and then modify from there. This is what I do most of the time.\nThere are more than 40 unique theme elements that can be modified to control the appearance of a plot.\nYou can find the complete list of theme elements in the ggplot2 documentation. Let‚Äôs play around a little bit.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\n\n\n\n\n\n\n\nThe legend here is duplicative, let‚Äôs remove it.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Month, in 2020\",\n       y = \"Weight (g)\",\n       title = \"Total harvest weight of tomatoes by day in summer 2020\",\n       subtitle = \"Collected by Dr. Lisa Lendway (and from the package gardenR)\")\n\n\n\n\n\n\n\n\nWhat if we wanted to make the strip text background black, and the strip text white?\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        strip.text = element_text(color = \"white\"),\n        strip.background = element_rect(fill = \"black\"))\n\n\n\n\n\n\n\n\nRemember that ggplot works on layers and these layers are added in the order you indicate. That means if you write something code that negates or edits something that comes above, the lower code will prevail.\n\ngarden_harvest_tomato |&gt;\n  ggplot(aes(x = date, y = weight, color = variety)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(vars(variety)) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(color = \"white\"),\n        strip.background = element_rect(fill = \"black\")) +\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\nSetting an active theme\nIf you know you want to use one theme for all your plots, you can set all the parameters for that theme using theme_set() and theme_update() and then your theme will carry for all the plots you make going forward.\n\nmy_theme &lt;- theme_set(theme_classic())\n\nbase_plot",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#saving-your-plots",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#saving-your-plots",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Saving your plots",
    "text": "Saving your plots\nYou probably won‚Äôt want to save all your plots, but you definitely will want to save some of them. The function ggsave() makes this each. I like to save images as .svg as these are vectorized and have unlimited resolution. You could also adjust the file extension to save it in the format you like.\n\nggsave(plot = base_plot,\n       filename = \"img/my_plot.svg\",\n       width = 9,\n       height = 6)\n\nI like to set the code chunk options for my chunks where I am saving plots to eval = FALSE this way I don‚Äôt accidentally save over figures I don‚Äôt intend to. If I want to save the plot, I can do so manually.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102.html#useful-resources",
    "href": "modules/module2/06_ggplot102/06_ggplot102.html#useful-resources",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (and still üçÖ)",
    "section": "Useful resources",
    "text": "Useful resources\n\nggplot2 cheatsheet\nggplot2 documentation\nggplot2: elegant graphics for data analysis by Hadley Wickham\nA really compehensive list of resources compiled by Erik Gahner Larsen\nPast ggplot Code Clubs:\n\nVisualizing Data by Michael Broe\nggplot round 2 by me\nFaceting, multi-plots, and animating\nVisualizing Data by Michael Broe a second one\nggplot round 2 a second one by me",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html",
    "href": "modules/module2/04_wrangling/04_wrangling.html",
    "title": "Wrangling your data ü§†, the basics",
    "section": "",
    "text": "Figure from Allison Horst",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling.html#introduction",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Introduction",
    "text": "Introduction\nThis is a new lecture from the previous delivery of this course. In the last offering, I found that the process of wrangling data was by far the thing that people had the most trouble with. In recitations, and for module assignments, I would provide data in a way that would need some adjustment before visualization can be made - and if I‚Äôm being honest, I heard a lot of rumblings about this.\nStill, I am going to leave in the course activities that required data to the wrangled before visualization. I am doing this because real data is mostly not structured precisely how it needs to be to make the visualizations you want. I want to provide you all some practice to get comfortable with using your data lassos. This is something you need to get comfortable with on your coding journey.\nBut, I have added in this extra lecture to explicitly go over what I think are the most useful wrangling functions and tools you can use in R. I hope this introduces you to some of what is possible with R, so it will trigger your memory later when you need to use it. You can also always come back to this page during the course.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#what-is-the-tidyverse",
    "href": "modules/module2/04_wrangling/04_wrangling.html#what-is-the-tidyverse",
    "title": "Wrangling your data ü§†, the basics",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\n‚ÄúThe tidyverse‚Äù is a collection of packages called that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier. We will be using mostly tidyverse functions in this class, with some base R syntax scattered throughout.\nThe ‚Äúcore tidyverse‚Äù contains the 9 packages below:\n\ndplyr: for data manipulation\nggplot2: a ‚Äúgrammar of graphics‚Äù for creating beautiful plots\nreadr: for reading in rectangular data (i.e., Excel-style formatting)\ntibble: using tibbles as modern/better dataframes\nstringr: handling strings (i.e., text or stuff in quotes)\nforcats: for handling categorical variables (i.e., factors) (meow!)\ntidyr: to make ‚Äútidy data‚Äù\npurrr: for enhancing functional programming (also meow!)\nlubridate: for working with dates and times\n\nWe will be using many of these other packages in this course, but will talk about them as we go. There are more tidyverse packages outside of this core, and we will talk about some of them another time.\n\ntl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are required, but I think you‚Äôll find many tidyverse approaches easier and more intuitive than using base R.\n\nYou can find here some examples of comparing tidyverse and base R syntax.\nToday we will be mostly talking through functions that live within the dplyr package.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#installing-ggplot-tidyverse",
    "href": "modules/module2/04_wrangling/04_wrangling.html#installing-ggplot-tidyverse",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Installing ggplot & tidyverse",
    "text": "Installing ggplot & tidyverse\nTo install packages in R that are on the Comprehensive R Archive Network (CRAN), you can use the function install.packages().\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggplot2\")\n\nWe only need to install packages once. But, every time we want to use them, we need to ‚Äúload‚Äù them, and can do this using the function library().\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIt‚Äôs a good habit to not ignore warnings/messages that R gives you.\n\ntl:dr install.packages() once, library() every time.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#loading-data",
    "href": "modules/module2/04_wrangling/04_wrangling.html#loading-data",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Loading data",
    "text": "Loading data\nIn class, we will use a combination of data embedded within R (or packages in R), from the internet, or data you import yourself. I am going to quickly go over ways to import common data types.\n\n.csv\nFiles saved as comma separated values are the most common data type I tend to import. The function read_csv() which is a part of the tidyverse package readr allows you to do this easily as it has a special function for this file type, as it is so common.\nMake sure that your file is within your working directory (or you have its relative or complete path), and you can install it (and save it) like this:\n\nsample_csv_data &lt;- read_csv(file = \"my-file-name.csv\")\n\n\n\n.xlsx\nThe second most common file type I import are those made in Excel. These files can either be converted to a .csv and then read in like we just went over, or you can load the package readxl and read files in directly. If you don‚Äôt already have readxl you can download it using install.packages().\n\nlibrary(readxl)\nsample_excel_data &lt;- read_excel(file = \"my-file-name.xlsx\",\n                                sheet = \"Sheet1\")\n\nHere you can find the readr cheatsheet.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#the-pipe",
    "href": "modules/module2/04_wrangling/04_wrangling.html#the-pipe",
    "title": "Wrangling your data ü§†, the basics",
    "section": "The pipe |>",
    "text": "The pipe |&gt;\nThe pipe |&gt; (which used to be written %&gt;%, and you will see this widely when googling/troubleshooting and sometimes see me default to this older syntax) is a tool that allows you to take the output of one function, and send it to the next function.\nYou can read the pipe as ‚Äúand then‚Äù - here is a theoretical example.\n\ntake_this_data |&gt;\n  then_this_function() |&gt;\n  then_another_function() |&gt; \n  finally_a_last_function()\n\nThe easiest way to see how the pipe works is with an example. We are going to use the dataset diamonds which comes pre-loaded when you load the tidyverse.\nWhat is in the dataset diamonds? We can get a ‚Äúglimpse‚Äù of it with the function glimpse, which is sort of like the tidyverse version of str().\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\n\nWhat if we want to see what is the average price of a diamond of a ‚ÄúPremium‚Äù diamong. There are a few ways we can do this.\n\n# one way\n# filter for only the premium diamonds\n# note the double equals\ndiamonds_premium &lt;- filter(diamonds, cut == \"Premium\")\n\n# calculate the mean using summarize\nsummarize(diamonds_premium, mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      4584.\n\n# or calculate mean using mean\n# the function mean() requires a vector\nmean(diamonds_premium$price)\n\n[1] 4584.258\n\n\nOr, we can use the pipe |&gt;. We are going to talk about summarize() in a minute.\n\ndiamonds |&gt;\n  filter(cut == \"Premium\") |&gt;\n  summarize(mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      4584.\n\n# if we want to use the function mean() we need to supply a vector\ndiamonds |&gt; \n  filter(cut == \"Premium\") |&gt;\n  pull(price) |&gt; # pulls out price as a vector since mean() requires a vector\n  mean()\n\n[1] 4584.258\n\n\nSome reasons I like the pipe:\n\nits easier to read (and doesn‚Äôt have a lot of nested parentheses)\nit doesn‚Äôt require you to create lots of interim objects which you won‚Äôt use again\nits easy to troubleshoot\n\n\n\n\n\n\n\nKeyboard shortcuts\n\n\n\nThe keyboard shortcut for |&gt; is Ctrl + Shift + M on Windows or Cmd + Shift + M on a Mac\n\n\n\n\n\n\n\n\nAre you seeing %&gt;%?\n\n\n\nIf you are seeing the old version of the pipe with your keyboard shortcut, go to Tools &gt; Global options &gt; Code &gt; Editing and check the box for ‚ÄúUse native pipe operator (|&gt;)‚Äù\n\n\nOf course you can assign the output of a pipe to something using the assignment operator &lt;- and then use it for other things.\nSome functions are not ‚Äúpipe friendly‚Äù meaning they will not work using pipes. This is often because the data is not the first argument passed to the function. All tidyverse functions work with piping.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#selecting-columns-with-select",
    "href": "modules/module2/04_wrangling/04_wrangling.html#selecting-columns-with-select",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nOften you will want to pick only certain columns in your dataframe, and you can do this with the function select(). You can pick columns by:\n\ntheir names\ntheir position (i.e., index)\ncharacteristics of that column\n\nLet‚Äôs select first by name.\n\ndiamonds |&gt; \n  select(carat, cut, price)\n\n# A tibble: 53,940 √ó 3\n   carat cut       price\n   &lt;dbl&gt; &lt;ord&gt;     &lt;int&gt;\n 1  0.23 Ideal       326\n 2  0.21 Premium     326\n 3  0.23 Good        327\n 4  0.29 Premium     334\n 5  0.31 Good        335\n 6  0.24 Very Good   336\n 7  0.24 Very Good   336\n 8  0.26 Very Good   337\n 9  0.22 Fair        337\n10  0.23 Very Good   338\n# ‚Ñπ 53,930 more rows\n\n\nNote that when you use the pipe, the potential column names will autofill for you after you type 3 letters. You can also hit tab to scroll through all the potential objects to select.\nWe can also select by index. In general I would recommend against this because its really hard to remember which column indices are which variables today, nevermind returning back to old code 1 year from now.\n\ndiamonds |&gt; \n  select(c(1, 2, 7)) # you could also use the colon syntax if your columns are sequential\n\n# A tibble: 53,940 √ó 3\n   carat cut       price\n   &lt;dbl&gt; &lt;ord&gt;     &lt;int&gt;\n 1  0.23 Ideal       326\n 2  0.21 Premium     326\n 3  0.23 Good        327\n 4  0.29 Premium     334\n 5  0.31 Good        335\n 6  0.24 Very Good   336\n 7  0.24 Very Good   336\n 8  0.26 Very Good   337\n 9  0.22 Fair        337\n10  0.23 Very Good   338\n# ‚Ñπ 53,930 more rows\n\n\nYou can also select using selection helpers like:\n\neverything(): picks all variables\nstarts_with(): starts with some prefix\ncontains(): contains a specific string\nwhere(): selects columns where the statement given in the argument is TRUE\n\nHere is an example of using where() to select only the columns that are numeric.\n\ndiamonds |&gt; \n  select(where(is.numeric))\n\n# A tibble: 53,940 √ó 7\n   carat depth table price     x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23  61.5    55   326  3.95  3.98  2.43\n 2  0.21  59.8    61   326  3.89  3.84  2.31\n 3  0.23  56.9    65   327  4.05  4.07  2.31\n 4  0.29  62.4    58   334  4.2   4.23  2.63\n 5  0.31  63.3    58   335  4.34  4.35  2.75\n 6  0.24  62.8    57   336  3.94  3.96  2.48\n 7  0.24  62.3    57   336  3.95  3.98  2.47\n 8  0.26  61.9    55   337  4.07  4.11  2.53\n 9  0.22  65.1    61   337  3.87  3.78  2.49\n10  0.23  59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nYou can find more helpers here.\nUsing select() will also set the order of your columns. More about this later.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#choosing-observations-with-filter",
    "href": "modules/module2/04_wrangling/04_wrangling.html#choosing-observations-with-filter",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Choosing observations with filter()",
    "text": "Choosing observations with filter()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nSometimes you want to select observations (rows) based on values. To do this you use filter(). Try not to confuse this with select().\n\nselect() picks columns, while filter() picks rows.\n\nThe function filter() will keep only observations that meet your filtering criteria.\nLet‚Äôs say we want to only keep the diamonds that are bigger than 3 carats.\n\ndiamonds |&gt; \n  filter(carat &gt; 3)\n\n# A tibble: 32 √ó 10\n   carat cut     color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.01 Premium I     I1       62.7    58  8040  9.1   8.97  5.67\n 2  3.11 Fair    J     I1       65.9    57  9823  9.15  9.02  5.98\n 3  3.01 Premium F     I1       62.2    56  9925  9.24  9.13  5.73\n 4  3.05 Premium E     I1       60.9    58 10453  9.26  9.25  5.66\n 5  3.02 Fair    I     I1       65.2    56 10577  9.11  9.02  5.91\n 6  3.01 Fair    H     I1       56.1    62 10761  9.54  9.38  5.31\n 7  3.65 Fair    H     I1       67.1    53 11668  9.53  9.48  6.38\n 8  3.24 Premium H     I1       62.1    58 12300  9.44  9.4   5.85\n 9  3.22 Ideal   I     I1       62.6    55 12545  9.49  9.42  5.92\n10  3.5  Ideal   H     I1       62.8    57 12587  9.65  9.59  6.03\n# ‚Ñπ 22 more rows\n\n\nHere I made use of the greater than &gt; sign, and there are other operators you could also use to help you filter.\n\n==: equal to (I usually read this as exactly equal to, and is different than using an equal sign in an equation)\n&lt;, &gt;: less than or greater than\n&lt;=, &gt;=: less than or equal to, great than or equal to\n&: and\n|: or\n!: not equal\nis.na: is NA\n\nYou can also layer your filtering.\n\ndiamonds |&gt; \n  filter(carat &gt; 3 & cut == \"Premium\")\n\n# A tibble: 13 √ó 10\n   carat cut     color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;   &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.01 Premium I     I1       62.7    58  8040  9.1   8.97  5.67\n 2  3.01 Premium F     I1       62.2    56  9925  9.24  9.13  5.73\n 3  3.05 Premium E     I1       60.9    58 10453  9.26  9.25  5.66\n 4  3.24 Premium H     I1       62.1    58 12300  9.44  9.4   5.85\n 5  3.01 Premium G     SI2      59.8    58 14220  9.44  9.37  5.62\n 6  4.01 Premium I     I1       61      61 15223 10.1  10.1   6.17\n 7  4.01 Premium J     I1       62.5    62 15223 10.0   9.94  6.24\n 8  3.67 Premium I     I1       62.4    56 16193  9.86  9.81  6.13\n 9  3.01 Premium I     SI2      60.2    59 18242  9.36  9.31  5.62\n10  3.04 Premium I     SI2      59.3    60 18559  9.51  9.46  5.62\n11  3.51 Premium J     VS2      62.5    59 18701  9.66  9.63  6.03\n12  3.01 Premium J     SI2      60.7    59 18710  9.35  9.22  5.64\n13  3.01 Premium J     SI2      59.7    58 18710  9.41  9.32  5.59",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#make-new-columns-with-mutate",
    "href": "modules/module2/04_wrangling/04_wrangling.html#make-new-columns-with-mutate",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Make new columns with mutate()",
    "text": "Make new columns with mutate()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nSometimes you want to make new columns based on existing variables and you can do this with mutate().\nFor example, we might want to create a new column called ‚Äúprice_per_carat‚Äù which we calculate by taking price and divide it by carat. Keep in mind this would be an easy way to log transform data.\n\ndiamonds |&gt; \n  mutate(price_per_carat = price/carat)\n\n# A tibble: 53,940 √ó 11\n   carat cut   color clarity depth table price     x     y     z price_per_carat\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n 1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43           1417.\n 2  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31           1552.\n 3  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31           1422.\n 4  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63           1152.\n 5  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75           1081.\n 6  0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48           1400 \n 7  0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47           1400 \n 8  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53           1296.\n 9  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49           1532.\n10  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39           1470.\n# ‚Ñπ 53,930 more rows\n\n\nMutated columns are by default put at the end of the dataframe. We can reorder simply using select().\n\ndiamonds |&gt; \n  mutate(price_per_carat = price/carat) |&gt; \n  select(price_per_carat, everything()) # put new column first, then everything\n\n# A tibble: 53,940 √ó 11\n   price_per_carat carat cut   color clarity depth table price     x     y     z\n             &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1           1417.  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2           1552.  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31\n 3           1422.  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 4           1152.  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63\n 5           1081.  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n 6           1400   0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7           1400   0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8           1296.  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53\n 9           1532.  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\n10           1470.  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nYou can also make new columns using conditional statements. For example, what if we want to create a new column that tells us if a diamond is more than $1000 called ‚Äúat_least_1000‚Äù. We will do this using if_else().\n\ndiamonds |&gt; \n  mutate(at_least_1000 = if_else(condition = price &gt;= 1000,\n                                  true = \"$1000 or more\",\n                                  false = \"less than $1000\")) |&gt; \n  select(at_least_1000, everything()) # move to front so we can see it\n\n# A tibble: 53,940 √ó 11\n   at_least_1000   carat cut   color clarity depth table price     x     y     z\n   &lt;chr&gt;           &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 less than $1000  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2 less than $1000  0.21 Prem‚Ä¶ E     SI1      59.8    61   326  3.89  3.84  2.31\n 3 less than $1000  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 4 less than $1000  0.29 Prem‚Ä¶ I     VS2      62.4    58   334  4.2   4.23  2.63\n 5 less than $1000  0.31 Good  J     SI2      63.3    58   335  4.34  4.35  2.75\n 6 less than $1000  0.24 Very‚Ä¶ J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7 less than $1000  0.24 Very‚Ä¶ I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8 less than $1000  0.26 Very‚Ä¶ H     SI1      61.9    55   337  4.07  4.11  2.53\n 9 less than $1000  0.22 Fair  E     VS2      65.1    61   337  3.87  3.78  2.49\n10 less than $1000  0.23 Very‚Ä¶ H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nIf you have more than two conditions, you can use case_when().\nIf you use mutate() to create a new column that has the same name as an existing column, it will override that current column.\nYou can find other mutate() helpers here.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#sorting-with-arrange",
    "href": "modules/module2/04_wrangling/04_wrangling.html#sorting-with-arrange",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Sorting with arrange()",
    "text": "Sorting with arrange()\nSometimes you just want to see a dataframe ordered by a particular column. We can do that easily with arrange().\n\ndiamonds |&gt; \n  arrange(price)\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nBy default, arrange() sorts from smallest to largest. We can change that if that‚Äôs what we want.\n\n# these are the same\ndiamonds |&gt; \n  arrange(-price)\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ‚Ñπ 53,930 more rows\n\ndiamonds |&gt; \n  arrange(desc(price))\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ‚Ñπ 53,930 more rows",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#computing-summaries-with-summarize",
    "href": "modules/module2/04_wrangling/04_wrangling.html#computing-summaries-with-summarize",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Computing summaries with summarize()",
    "text": "Computing summaries with summarize()\nThe function summarize() calculates summary information based on the functions you provide as arguments. This function creates a wholly new dataframe, providing one row for each grouping variable. If there is no grouping, the resulting dataframe will have one row.\nLet‚Äôs look at an example. We can use summarize() The syntax is new_column_name = function().\n\ndiamonds |&gt; \n  summarize(mean_price = mean(price))\n\n# A tibble: 1 √ó 1\n  mean_price\n       &lt;dbl&gt;\n1      3933.\n\n\nWe can also provide multiple items for summary.\n\ndiamonds |&gt; \n  summarize(mean_price = mean(price),\n            sd_price = sd(price),\n            count = n())\n\n# A tibble: 1 √ó 3\n  mean_price sd_price count\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1      3933.    3989. 53940\n\n\nHere are some examples of functions you can use within summarize():\n\nmean() and median(): calculate mean and median\nsd() and IQR(): calculate standard deviation and interquartile range\nmin() and max(): calculate min and max\nn() and n_distinct(): calculate how many observations there are, and how many distinct observations there are\n\nYou can also use the function across() combined with where() to calculate summary data ‚Äúacross‚Äù different columns.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nFor example, like we see in the illustration above, we might want to calculate the mean ‚Äúacross‚Äù all columns ‚Äúwhere‚Äù if we asked if that column contains numeric data, we would get TRUE.\n\ndiamonds |&gt; \n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 1 √ó 7\n  carat depth table price     x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.798  61.7  57.5 3933.  5.73  5.73  3.54\n\n\nI hope you can start to see now how combining lots of these different functions together will help you achieve what you want with your coding.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#operations-by-group-with-group_by",
    "href": "modules/module2/04_wrangling/04_wrangling.html#operations-by-group-with-group_by",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Operations by group with group_by()",
    "text": "Operations by group with group_by()\nSometimes you might want to group your data together to perform operations group-wise. You can do this with group_by(). The way to ungroup is to use ungroup().\nFor example, say we want to calculate the average price of a diamond for each cut type.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(mean_price = mean(price))\n\n# A tibble: 5 √ó 2\n  cut       mean_price\n  &lt;ord&gt;          &lt;dbl&gt;\n1 Fair           4359.\n2 Good           3929.\n3 Very Good      3982.\n4 Premium        4584.\n5 Ideal          3458.\n\n\nNow instead of getting one row for the mean price, we are getting a mean price for each cut.\nNote that when you use group_by(), the groupings are now embedded within your data. Let me show you what I mean.\n\ndiamonds_cut &lt;- diamonds |&gt; \n  group_by(cut)\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\nglimpse(diamonds_cut)\n\nRows: 53,940\nColumns: 10\nGroups: cut [5]\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.‚Ä¶\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver‚Ä¶\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,‚Ä¶\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, ‚Ä¶\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64‚Ä¶\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58‚Ä¶\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34‚Ä¶\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.‚Ä¶\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.‚Ä¶\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.‚Ä¶\n\n\nAgain we can combine these different functions together to summarize for the mean value across all columns that are numeric, but this time grouped by cut.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 5 √ó 8\n  cut       carat depth table price     x     y     z\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40\n\n\nWe can also use summarize() to add how many observations there are for each category.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(across(where(is.numeric), mean), n = n())\n\n# A tibble: 5 √ó 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\n\nHere is a helpful blogpost by Hadley Wickham for working across columns.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#pivoting-with-pivot_longer-and-pivot_wider",
    "href": "modules/module2/04_wrangling/04_wrangling.html#pivoting-with-pivot_longer-and-pivot_wider",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Pivoting with pivot_longer() and pivot_wider()",
    "text": "Pivoting with pivot_longer() and pivot_wider()\nThe function pivot_longer() will often let you make your data in ‚Äútidy‚Äù format, and pivot_wider() allow you to make it untidy (but often still useful) again. Let me explain more what I mean.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nThis is easier to ‚Äúsee‚Äù üëÄ than to explain. Here is an example of non-tidy data, where there is data embedded in column names, and one variable (the rank of a song) is spread across many columns:\n\nbillboard\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nHere is an example of the same exact data, in a tidy format, where those data that used to be column names, are now values coded for a particular variable.\n\nbillboard_long &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\nbillboard_long\n\n# A tibble: 24,092 √ó 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ‚Ñπ 24,082 more rows\n\n\nWe can go back from our new longer dataframe with pivot_wider().\n\nbillboard_long |&gt; \n  pivot_wider(names_from = week,\n              values_from = rank)\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nIn recap:\npivot_longer() pulls data that is embedded in column names, and reshapes your dataframe such this information is now embedded within the values. Or put differently, it collects variables that are spread across multiple columns into a single column. This makes your dataframes longer, i.e., increases the number of rows. Typically, we use pivot_longer() to make an untidy dataset tidy.\npivot_wider() takes data that is embedded in the values of your dataframe, and puts this information in variable names. Or put differently, it spreads a variable across multiple columns. This makes your dataframe ‚Äúwider‚Äù, i.e., increases the number of columns. Typically, pivot_wider() will make a dataset untidy. This can be useful for certain calculations, or if you want to use a for loop to do something iteratively across columns.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#joining-data-together",
    "href": "modules/module2/04_wrangling/04_wrangling.html#joining-data-together",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Joining data together",
    "text": "Joining data together\nOften you will have two separate dataframes that you want to join together. You can do this in two main ways:\n\nby matching something between them (i.e., using _join())\nby smushing them together in their existing order by columns bind_cols() or rows bind_rows().\n\n\n*_join()\nWe can join two dataframes, let‚Äôs call them x and y, together based on a key that we provide. This is one of the first things I did using R that I felt like wow this is really a lot easier than be doing this manually.\nThere are four types of joins:\n\ninner_join(): keeps observations in x that are also present in y\nleft_join(): keeps observations in x\nright_join()`: keeps observations in y\nfull_join(): keeps observations in both x and y\n\nWe will use the datasets band_members and band_instruments which are pre-loaded with the tidyverse to show how this works. You can also see these examples on the mutating joins documentation page.\n\nglimpse(band_members)\n\nRows: 3\nColumns: 2\n$ name &lt;chr&gt; \"Mick\", \"John\", \"Paul\"\n$ band &lt;chr&gt; \"Stones\", \"Beatles\", \"Beatles\"\n\nglimpse(band_instruments)\n\nRows: 3\nColumns: 2\n$ name  &lt;chr&gt; \"John\", \"Paul\", \"Keith\"\n$ plays &lt;chr&gt; \"guitar\", \"bass\", \"guitar\"\n\n\nR will make its best guess as to what you want to ‚Äújoin‚Äù based on, and that works a lot of the time, but I always like to be exclicit and indicate the column key for the join with by =.\nAn inner join: we will only get the observations that are present in both dataframes.\n\ninner_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 2 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nAn left join: we will only get the observations that are present in band_members. Note the appearance of NA for Mick.\n\nleft_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 3 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nAn right join: we will only get the observations that are present in band_instruments. Note the appearance of NA for band for Keith (Richards, who is in the Rolling Stones). You could also switch the order of the dataframes in your argument instead of using left vs right.\n\nright_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 3 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n3 Keith &lt;NA&gt;    guitar\n\n\nAn full join: we get all observations of what is present in band_members and band_instruments.\n\nfull_join(band_members, band_instruments, by = \"name\")\n\n# A tibble: 4 √ó 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nThere is nuances to what happens in different joining situations, so do this cautiously and always check that it went the way you expected it to.\n\n\nbind_rows() and bind_cols()\nGenerally it would be preferrable to use a _join() over bind_cols() or bind_rows() since in the latter, the binding happens in the order that observations appear. This might make your data not meaningful without you knowing.\nLet‚Äôs get to the examples.\n\ndata1 &lt;- tibble(x = 1:5)\ndata2 &lt;- tibble(y = 6:10)\n\nbind_cols(data1, data2)\n\n# A tibble: 5 √ó 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     6\n2     2     7\n3     3     8\n4     4     9\n5     5    10\n\n\n\nbind_rows(data1, data2)\n\n# A tibble: 10 √ó 2\n       x     y\n   &lt;int&gt; &lt;int&gt;\n 1     1    NA\n 2     2    NA\n 3     3    NA\n 4     4    NA\n 5     5    NA\n 6    NA     6\n 7    NA     7\n 8    NA     8\n 9    NA     9\n10    NA    10",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#handling-strings",
    "href": "modules/module2/04_wrangling/04_wrangling.html#handling-strings",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Handling strings",
    "text": "Handling strings\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nHandling strings (i.e., ‚Äústrings‚Äù of characters) could be multiple whole lessons, so my goal is to introduce you here to how to handle them. The tidyverse package to manage strings is called stringr. Sometimes you might want to automate extraction of only part of a value present in a column to use, remove some values, or split strings apart. This is valuable especially when the way that data is coded/recorded is different than the way you want it to be when you analyze it. Instead of manually recoding in excel, you can reproducibly and tracibly recode in R. You can read about all the functions within stringr here.\nYou can use regular expressions within stringr functions, but I‚Äôm not going to explicitly go over that (check out these code clubs regex1, regex2 if you want to learn more).\nI‚Äôm going to create some sample data to play with.\n\nstrings &lt;- tibble(\n  sample = c(rep.int(\"Treatment_Level1\", 3), \n             rep.int(\"Treatment_Level2\", 3),\n             rep.int(\"Treatment_Level3\", 3),\n             rep.int(\"Control_Level1\", 3),\n             rep.int(\"Control_Level2\", 3),\n             rep.int(\"Control_Level3\", 3)))\n\nLet‚Äôs look at it quickly.\n\nstrings\n\n# A tibble: 18 √ó 1\n   sample          \n   &lt;chr&gt;           \n 1 Treatment_Level1\n 2 Treatment_Level1\n 3 Treatment_Level1\n 4 Treatment_Level2\n 5 Treatment_Level2\n 6 Treatment_Level2\n 7 Treatment_Level3\n 8 Treatment_Level3\n 9 Treatment_Level3\n10 Control_Level1  \n11 Control_Level1  \n12 Control_Level1  \n13 Control_Level2  \n14 Control_Level2  \n15 Control_Level2  \n16 Control_Level3  \n17 Control_Level3  \n18 Control_Level3  \n\n\nLet‚Äôs first ask how many times do we have the string ‚Äú3‚Äù in our dataframe? Note that these functions accept a vector, so you need to provide data in that form. The function str_detect() gives a logical vector as the output, the same length as the vector provided, and indicates FALSE when the pattern is not met, and TRUE when it is.\n\nstr_detect(strings$sample, pattern = \"3\")\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n# using sum will count how many times the logical is evaluated to be TRUE\nsum(str_detect(strings$sample, pattern = \"3\"))\n\n[1] 6\n\n\nYou might want to re-code your data so that Level1 becomes that actual level used. Let‚Äôs say that Level1 is 100mg, Level2 is 300 mg, and Level3 is 500mg. We can do this with str_replace() to replace the first match only or str_replace_all() to replace all matches (which is what we want here).\n\n(strings$sample &lt;- strings |&gt; \n  select(sample) |&gt; \n  pull(sample) |&gt; # make a vector so can pass to next fxn\n  str_replace_all(pattern = \"Level1\", replacement = \"100mg\") |&gt; \n  str_replace_all(pattern = \"Level2\", replacement = \"300mg\") |&gt; \n  str_replace_all(pattern = \"Level3\", replacement = \"500mg\"))   \n\n [1] \"Treatment_100mg\" \"Treatment_100mg\" \"Treatment_100mg\" \"Treatment_300mg\"\n [5] \"Treatment_300mg\" \"Treatment_300mg\" \"Treatment_500mg\" \"Treatment_500mg\"\n [9] \"Treatment_500mg\" \"Control_100mg\"   \"Control_100mg\"   \"Control_100mg\"  \n[13] \"Control_300mg\"   \"Control_300mg\"   \"Control_300mg\"   \"Control_500mg\"  \n[17] \"Control_500mg\"   \"Control_500mg\"  \n\n\nWe might not want to have both Treatment/Control and Level nested in the same cell, we can split them apart using separate_*() functions. Here we are using separate_wider_delim() to seprate the column ‚Äúsample‚Äù into two new columns called ‚Äútreatment‚Äù and ‚Äúdose.\n\n(strings_separated &lt;- strings |&gt; \n  separate_wider_delim(cols = sample,\n                       delim = \"_\", # what is the delimiter\n                       names = c(\"treatment\", \"dose\")))\n\n# A tibble: 18 √ó 2\n   treatment dose \n   &lt;chr&gt;     &lt;chr&gt;\n 1 Treatment 100mg\n 2 Treatment 100mg\n 3 Treatment 100mg\n 4 Treatment 300mg\n 5 Treatment 300mg\n 6 Treatment 300mg\n 7 Treatment 500mg\n 8 Treatment 500mg\n 9 Treatment 500mg\n10 Control   100mg\n11 Control   100mg\n12 Control   100mg\n13 Control   300mg\n14 Control   300mg\n15 Control   300mg\n16 Control   500mg\n17 Control   500mg\n18 Control   500mg\n\n\nThe opposite function for separate_() is unite().\nIf we wanted to extract just the number part out of ‚Äúdose‚Äù we could use readr::parse_number() to do that. Note I‚Äôve embedded parse_number() within a mutate() function to change the values in the dataset.\n\nstrings_separated |&gt; \n  mutate(dose = parse_number(dose))\n\n# A tibble: 18 √ó 2\n   treatment  dose\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Treatment   100\n 2 Treatment   100\n 3 Treatment   100\n 4 Treatment   300\n 5 Treatment   300\n 6 Treatment   300\n 7 Treatment   500\n 8 Treatment   500\n 9 Treatment   500\n10 Control     100\n11 Control     100\n12 Control     100\n13 Control     300\n14 Control     300\n15 Control     300\n16 Control     500\n17 Control     500\n18 Control     500",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling.html#cleaning-up-column-names-with-clean_names",
    "href": "modules/module2/04_wrangling/04_wrangling.html#cleaning-up-column-names-with-clean_names",
    "title": "Wrangling your data ü§†, the basics",
    "section": "Cleaning up column names with clean_names()",
    "text": "Cleaning up column names with clean_names()\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\nI really like the package janitor which has some nice functions for cleaning up üßπ ‚Äúmessy‚Äù data. I use clean_names() a lot which converts untidy column names into only characters (default all in lower case) and connects words or terms with underscores.\nI am making up some messy names so you can see how this works.\n\n# make messy data\nmessy_data &lt;- tibble(\n  \"Sample Name\" = 1:5,\n  \"THE NEXT VARIABLE\" = 6:10,\n  \"ThisIsChaos\" = 11:15\n)\n\n# print column names\ncolnames(messy_data)\n\n[1] \"Sample Name\"       \"THE NEXT VARIABLE\" \"ThisIsChaos\"      \n\n\n\n# install and load janitor\n# install.packages(\"janitor)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# clean up column names\nclean_names(messy_data)\n\n# A tibble: 5 √ó 3\n  sample_name the_next_variable this_is_chaos\n        &lt;int&gt;             &lt;int&gt;         &lt;int&gt;\n1           1                 6            11\n2           2                 7            12\n3           3                 8            13\n4           4                 9            14\n5           5                10            15",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html",
    "title": "Wrangling your data ü§† Recitation",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class. When you go to the links below, click on the Download Raw File icon (the down arrow over a turned open bracket) at the top right of the file\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#introduction",
    "title": "Wrangling your data ü§† Recitation",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class. When you go to the links below, click on the Download Raw File icon (the down arrow over a turned open bracket) at the top right of the file\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#explore-your-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#explore-your-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Explore your data",
    "text": "Explore your data\nWrite some code that lets you explore that is in these two datasets.\nHow many observations there in each dataset?\nWhat years do the data contain information for?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#modifying-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#modifying-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Modifying data",
    "text": "Modifying data\nCreate a new dataset for life_expectancy that only includes observed data (i.e., remove the projected data after 2022).",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#calculating-summaries",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#calculating-summaries",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Calculating summaries",
    "text": "Calculating summaries\nWhat country has the highest average happiness index in 2022?\nWhat about overall average highest index?\nHow many countries had an average life expectancy over 80 years in 2022?\nWhat countries are in the top 10 percentile for happiness? What about the bottom? What about for life expectancy? You can calculate this for the most recent data, for the mean, or really for whatever you want. Remember there are lots of ways to do this.\nClick the button Show on the right if you need a hint\n\n# Hint - try using the functions in the `slice_()` family.\n\nWhich country has had their happiness index increase the most from 2012 to 2022? Which dropped the most?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation.html#joining-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation.html#joining-data",
    "title": "Wrangling your data ü§† Recitation",
    "section": "Joining data",
    "text": "Joining data\nTry joining the happiness and life_expectancy datasets together and use the different *_join() functions so you can see how they differ. Check their dimensions and look at them. Think about how you might want to do different joins in different situations.\nIf you wanted to create a plot that allowed you to see the correlation between happiness score and life expectancy in 2022, which joined dataset would you use and why?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 4 - Wrangling"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html",
    "href": "modules/module3/08_correlations/08_correlations.html",
    "title": "Visualizing Correlations",
    "section": "",
    "text": "Figure from XKCD",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations.html#introduction",
    "title": "Visualizing Correlations",
    "section": "Introduction",
    "text": "Introduction\nWe will will building on our lessons on ggplot101 and ggplot102 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, geoms, facets, scales, labels, and themes. Today we are going to apply what we learned towards trying to better understanding and visualize correlations within our data. To do this we will also use some ggplot extension packages.\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\n\nlibrary(tidyverse)\n\nToday we are going to continue to use the same real research data from my group from last week. We will be reading in the supplementary data from a paper written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 1. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloids &lt;- readxl::read_excel(\"tpg220192-sup-0002-supmat.xlsx\",\n                                sheet = \"S1 Raw Data Diversity Panel\")\n\n\nknitr::kable(head(alkaloids))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nYear\nEnvironment\nBlock\nGenotype\nPlot_Source\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nLatitude\nLongitude\nDehydrotomatidine\nTomatidine\nDehydrotomatine1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\n\n\n\n\n7805\n2018\nFreEarly18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.000000\n5.726010\n0.350331\n6.076341\n172.66244\n1.079190\n86.72742\n17.831892\n9.142607\n114.78111\n18.902399\n56.307182\n1.890053\n77.099634\n5.125904\n10.277325\n336.8893\n347.1666\n3.787979\n0.924195\n3.943230\n8.655404\n731.5675\n\n\n7898\n2017\nFre17\n2\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.169068\n0.000000\n0.000000\n0.000000\n55.47329\n0.000000\n53.32292\n13.630697\n4.841762\n71.79538\n3.557348\n4.107289\n0.000000\n7.664637\n2.905500\n5.548102\n199.6694\n205.2175\n8.978931\n1.897850\n6.794690\n17.671471\n360.8969\n\n\n7523\n2018\nFreLate18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.135675\n0.680554\n5.073552\n0.000000\n5.073552\n123.85835\n0.000000\n50.90989\n6.503939\n1.368847\n58.78268\n3.931461\n4.123222\n0.623340\n8.678023\n2.185082\n5.104115\n259.0177\n264.1218\n4.049145\n0.000000\n6.749386\n10.798531\n474.3143\n\n\n7724\n2017\nFre17\n1\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.054300\n0.497261\n19.419087\n0.000000\n19.419087\n239.01264\n0.000000\n36.02318\n8.557673\n7.483933\n52.06478\n3.341048\n16.415426\n1.057100\n20.813574\n0.000000\n0.000000\n203.0061\n203.0061\n1.678210\n0.000000\n2.349633\n4.027843\n538.8955\n\n\n7427\n2018\nFreLate18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.139454\n0.553801\n0.000000\n0.000000\n0.000000\n64.31783\n0.879435\n39.91027\n7.228388\n3.015298\n51.03339\n0.000000\n3.131685\n0.000000\n3.131685\n0.000000\n4.054211\n299.5687\n303.6229\n10.146857\n0.000000\n4.882339\n15.029197\n437.8283\n\n\n7854\n2018\nFreEarly18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.049700\n0.262174\n3.737579\n0.000000\n3.737579\n68.44913\n0.000000\n23.86864\n13.506299\n1.456982\n38.83192\n4.657902\n4.259007\n0.605729\n9.522638\n9.832149\n11.595595\n459.5205\n471.1161\n6.839930\n0.486236\n5.595751\n12.921917\n614.7233\n\n\n\n\n\nThis dataset has 605 observations, with data about different steroidal alkaloids in the fruits of different tomato germplasm grown in 3 locations across 2 years. There is also some other metadata too.\nFor those who are chemistry minded, here is a little pathway context for the compounds we are investigating today.\n\n\n\nFigure from Syzma≈Ñsky et al., Nature Genetics 2020",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#scatterplots",
    "href": "modules/module3/08_correlations/08_correlations.html#scatterplots",
    "title": "Visualizing Correlations",
    "section": "Scatterplots",
    "text": "Scatterplots\nA very simple first pass way to understand if you have relationships within your data is to make scatterplots of the variables you think might be correlated. Let‚Äôs start by investigating how the different alkaloid concentrations are correlated to each other. First we will see how alpha-tomatine content (Tomatine) is related to total steroidal alkaloid content (Total).\n\nalkaloids |&gt;\n  ggplot(aes(x = Total, y = Tomatine)) +\n  geom_point() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g)\",\n       y = \"Alpha-Tomatine (¬µg/100 g)\")\n\n\n\n\n\n\n\n\nIt seems like there are two separate groups here - the points with a steeper slope, and the points with a less steep slope. We can color our points based on what Class of tomato the data comes from, maybe that will reveal something. In the meanwhile let‚Äôs make this plot look a bit nicer. The package scales has some nice functions that help you control the scaling of your plots, in this case, making each of the axes have numbers in comma_format(). I also am using the hex codes for a color-blind friendly qualitative color scheme developed by Paul Tol.\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nalkaloids |&gt;\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point(alpha = 0.8) +\n  scale_x_continuous(labels =) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")\n\n\n\n\n\n\n\n\nAll of the tomatoes in the two extremes of this plot are from the Class Wild Cherry. What would this look like if we removed these fruits? Note, I adjusted the color scale to remove the hex code associated with Wild Cherry but keeping the other colors the same.\n\nalkaloids |&gt;\n  filter(Class != \"Wild Cherry\") |&gt;\n  ggplot(aes(x = Total, y = Tomatine, color = Class)) +\n  geom_point() +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#adding-geom_smooth",
    "href": "modules/module3/08_correlations/08_correlations.html#adding-geom_smooth",
    "title": "Visualizing Correlations",
    "section": "Adding geom_smooth()",
    "text": "Adding geom_smooth()\n\nalkaloids |&gt;\n  ggplot(aes(x = Total, y = Tomatine,)) +\n  geom_point(aes(color = Class), alpha = 0.8) +\n  geom_smooth() +\n  scale_x_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_y_continuous(labels = comma_format(big.mark = \",\")) + # requires the package scales\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  theme_minimal() +\n  labs(x = \"Total Steroidal Alkaloids (¬µg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (¬µg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFaceted scatterplots\nWe may be able to see trends by tomato class more easily if we facet our scatterplots. I also am demonstrating here how within the ggplot function you can make alter the aesthetics you plot - here I am turning data that is present as ¬µg/100 g to mg/100 g by dividing by 1000 and changing the axis labels accordingly.\n\nalkaloids |&gt;\n  ggplot(aes(x = Total/1000, y = Tomatine/1000, color = Class)) +\n  geom_point(alpha = 0.8) +\n  scale_color_manual(values = c(\"#4477AA\", \"#EE6677\", \"#228833\", \"#CCBB44\", \"#66CCEE\")) +\n  facet_wrap(vars(Class), scales = \"free\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total Steroidal Alkaloids (mg/100 g fresh weight)\",\n       y = \"Alpha-Tomatine (mg/100 g fresh weight)\",\n       title = \"Relationship between Alpha-Tomatine and Total Steroidal Alkaloids \\nAcross Different Germplasm in the Red Tomato Clade\")",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#correlation-matrix-with-cor",
    "href": "modules/module3/08_correlations/08_correlations.html#correlation-matrix-with-cor",
    "title": "Visualizing Correlations",
    "section": "Correlation matrix with cor()",
    "text": "Correlation matrix with cor()\ncor() is a function from base R that will allow you to create a correlation matrix.\nBefore we use cor() we will clean up our dataset to include only the variables we want to correlate.\n\ncolnames(alkaloids)\n\n [1] \"ID\"                                      \n [2] \"Year\"                                    \n [3] \"Environment\"                             \n [4] \"Block\"                                   \n [5] \"Genotype\"                                \n [6] \"Plot_Source\"                             \n [7] \"Class\"                                   \n [8] \"Origin\"                                  \n [9] \"Provence\"                                \n[10] \"Blanca_Cluster1\"                         \n[11] \"Blanca_Cluster2\"                         \n[12] \"Passport_Species\"                        \n[13] \"Passport_Classification\"                 \n[14] \"Sim_Grouping\"                            \n[15] \"Latitude\"                                \n[16] \"Longitude\"                               \n[17] \"Dehydrotomatidine\"                       \n[18] \"Tomatidine\"                              \n[19] \"Dehydrotomatine1\"                        \n[20] \"Dehydrotomatine2\"                        \n[21] \"TotalDehydrotomatine\"                    \n[22] \"Tomatine\"                                \n[23] \"Hydroxytomatine1\"                        \n[24] \"Hydroxytomatine2\"                        \n[25] \"Hydroxytomatine3\"                        \n[26] \"Hydroxytomatine4\"                        \n[27] \"TotalHydroxytomatine\"                    \n[28] \"Acetoxytomatine1\"                        \n[29] \"Acetoxytomatine2\"                        \n[30] \"Acetoxytomatine3\"                        \n[31] \"TotalAcetoxytomatine\"                    \n[32] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[33] \"LycoperosideFGEsculeosideA1\"             \n[34] \"LycoperosideFGEsculeosideA2\"             \n[35] \"TotalLycoperosideFGEsculeosideA\"         \n[36] \"EsculeosideB1\"                           \n[37] \"EsculeosideB2\"                           \n[38] \"EsculeosideB3\"                           \n[39] \"TotalEsculeosideB\"                       \n[40] \"Total\"                                   \n\n\nFrom looking at the colnames and reading the supplemental information, we can see that some columns are composites of others. For example, the column TotalAcetoxytomatine = Acetoxytomatine1 + Acetoxytomatine2 + Acetoxytomatine3. So we want to pull only the columns that represent the total for any given alkaloids. There should be 10 columns.\n\n# create a vector of the names we want to keep\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\n# make a new df including some metadata and the alkaloid_total_names\nalkaloids_totals &lt;- alkaloids |&gt;\n  select(ID, Year, Environment, Block, Genotype, Class, all_of(alkaloid_total_names))\n\n# did it work? look at colnames()\ncolnames(alkaloids_totals)\n\n [1] \"ID\"                                      \n [2] \"Year\"                                    \n [3] \"Environment\"                             \n [4] \"Block\"                                   \n [5] \"Genotype\"                                \n [6] \"Class\"                                   \n [7] \"Dehydrotomatidine\"                       \n [8] \"Tomatidine\"                              \n [9] \"TotalDehydrotomatine\"                    \n[10] \"Tomatine\"                                \n[11] \"TotalHydroxytomatine\"                    \n[12] \"TotalAcetoxytomatine\"                    \n[13] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[14] \"TotalLycoperosideFGEsculeosideA\"         \n[15] \"TotalEsculeosideB\"                       \n[16] \"Total\"                                   \n\n\nNow we can create a correlation matrix to see how each of our 10 alkaloids is correlated to the concentration of each other alkaloid (including the compile metric of Total which sums all the alkaloids). The default for cor() is to use Pearson‚Äôs correlation coefficient, but you can set to use Spearman method = \"spearman\" or Kendall method = \"kendall\" if you prefer. Check the documentation for cor() for more information.\n\nalkaloids_cor &lt;- alkaloids_totals |&gt;\n  select(all_of(alkaloid_total_names)) |&gt;\n  cor()\n\n# look at our correlation matrix\nknitr::kable(alkaloids_cor) # kable makes a nicely formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDehydrotomatidine\nTomatidine\nTotalDehydrotomatine\nTomatine\nTotalHydroxytomatine\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nTotalLycoperosideFGEsculeosideA\nTotalEsculeosideB\nTotal\n\n\n\n\nDehydrotomatidine\n1.0000000\n0.2974462\n0.0324918\n0.0238230\n0.0099126\n0.0322029\n0.0305049\n0.0761907\n0.0282219\n0.0708252\n\n\nTomatidine\n0.2974462\n1.0000000\n0.3744672\n0.3736949\n0.1003558\n0.0382981\n-0.0059964\n0.0373649\n0.0126724\n0.2044979\n\n\nTotalDehydrotomatine\n0.0324918\n0.3744672\n1.0000000\n0.9214859\n0.2290192\n0.4011257\n-0.0820469\n-0.1149682\n-0.1217560\n0.5636969\n\n\nTomatine\n0.0238230\n0.3736949\n0.9214859\n1.0000000\n0.0995212\n0.1220596\n-0.1140360\n-0.1357819\n-0.1260377\n0.3756155\n\n\nTotalHydroxytomatine\n0.0099126\n0.1003558\n0.2290192\n0.0995212\n1.0000000\n0.3563506\n0.0330078\n0.0284887\n0.0134806\n0.4774036\n\n\nTotalAcetoxytomatine\n0.0322029\n0.0382981\n0.4011257\n0.1220596\n0.3563506\n1.0000000\n-0.0865506\n-0.1106212\n-0.0947254\n0.6782337\n\n\nDehydrolycoperosideFGdehydroesculeosideA\n0.0305049\n-0.0059964\n-0.0820469\n-0.1140360\n0.0330078\n-0.0865506\n1.0000000\n0.8862982\n0.7401116\n0.4792108\n\n\nTotalLycoperosideFGEsculeosideA\n0.0761907\n0.0373649\n-0.1149682\n-0.1357819\n0.0284887\n-0.1106212\n0.8862982\n1.0000000\n0.7779405\n0.5222751\n\n\nTotalEsculeosideB\n0.0282219\n0.0126724\n-0.1217560\n-0.1260377\n0.0134806\n-0.0947254\n0.7401116\n0.7779405\n1.0000000\n0.4059824\n\n\nTotal\n0.0708252\n0.2044979\n0.5636969\n0.3756155\n0.4774036\n0.6782337\n0.4792108\n0.5222751\n0.4059824\n1.0000000\n\n\n\n\n\nNote the diagonal is all composed of 1s. This makes sense because the correlation of each alkaloid with itself is 1.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#using-ggcorrplot-from-ggcorrplot",
    "href": "modules/module3/08_correlations/08_correlations.html#using-ggcorrplot-from-ggcorrplot",
    "title": "Visualizing Correlations",
    "section": "Using ggcorrplot() from ggcorrplot",
    "text": "Using ggcorrplot() from ggcorrplot\nUse the function ggcorrplot() without any additional arguments besides the correlation matrix alkaloids_cor. In general, I think if you want to make a bunch of correlation plots quickly, and don‚Äôt intend to publish them, `ggcorrplot() works well, but the visuals of the plot are quite difficult to customize.\n\nlibrary(ggcorrplot)\n\nggcorrplot(alkaloids_cor)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the ggcorrplot package.\n  Please report the issue at &lt;https://github.com/kassambara/ggcorrplot/issues&gt;.\n\n\n\n\n\n\n\n\n\nThis is not a perfect plot but its a good starting point. Correlation matrices are inherently symmetric, meaning if we display only the top or bottom triangle, we do not lose any information. We will work on editing this plot in different ways to show more information and make it more beautiful.\nWe could also make the plot circles instead of squares at the same time.\n\nggcorrplot(alkaloids_cor, \n           method = \"circle\",\n           type = \"lower\")\n\n\n\n\n\n\n\n\nIn general, I think if you want to make a bunch of correlation plots quickly, and don‚Äôt intend to publish them, `ggcorrplot() works well, but the visuals of the plot are quite difficult to customize.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#using-corrplot-from-corrplot",
    "href": "modules/module3/08_correlations/08_correlations.html#using-corrplot-from-corrplot",
    "title": "Visualizing Correlations",
    "section": "Using corrplot() from corrplot",
    "text": "Using corrplot() from corrplot\nSimilarly, you can use a base R plotting based package corrplot() to make correlation plots. The customization syntax here is quite different from what we‚Äôve been working with in ggplot, but I wanted you to feel familiar with some base R tools.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrplot(alkaloids_cor, type = \"lower\")\n\n\n\n\n\n\n\n\nI have used corrplot() in publications before and felt like I couldn‚Äôt customize the plots as much as I wanted. In the process of putting together this content, I learned some news ways to customize these plots that are actually very nice. Here are some parameters you can modify in R. You can also order your variables by hierarchical clustering.\nFirst we will start (as we always do) by wrangling.\n\n# create matrix for correlation\nalkaloids_to_cor &lt;- alkaloids_totals |&gt;\n  select(all_of(alkaloid_total_names)) |&gt;\n  as.matrix() # rcorr() needs a matrix\n\nlibrary(Hmisc) # does cor() but also computes significance levels\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n# create a matrix of pvalues for the correlations\nalkaloids_rcorr = rcorr(alkaloids_to_cor, type = \"pearson\")\n\n# create a vector of the alkaloid names for labeling\nalkaloid_labels &lt;- c(\"Dehydrotomatidine\",\n                     \"Tomatidine\",\n                     \"Dehydrotomatine\",\n                     \"Alpha-Tomatine\",\n                     \"Hydroxytomatine\",\n                     \"Acetoxytomatine\",\n                     \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\",\n                     \"Lycoperoside F, G, \\nor Escueloside A\",\n                     \"Escueloside B\",\n                     \"Total Steroidal Alkaloids\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(alkaloids_rcorr$r) &lt;- alkaloid_labels\nrownames(alkaloids_rcorr$r) &lt;- alkaloid_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(alkaloids_rcorr$P) &lt;- alkaloid_labels\nrownames(alkaloids_rcorr$P) &lt;- alkaloid_labels\n\nNow we are ready to plot\n\ncorrplot(alkaloids_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = alkaloids_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"black\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 0.6) # size of correlation font",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#ggally",
    "href": "modules/module3/08_correlations/08_correlations.html#ggally",
    "title": "Visualizing Correlations",
    "section": "GGally",
    "text": "GGally\n\nggcorr()\nAnother ggplot extension package ggally has the function ggcorr() which also allows the creation of correlation plots, but ones that are more easily customizable. ggcorr() objects are moderately customizable. They make work for some of you so I‚Äôm sharing how to make them.\nNote, GGally::ggcorr() does not take a correlation matrix, but instead takes the data you want to make a correlation matrix for. You can specific the method of correlation in the arguments. The default is Pearson‚Äôs correlation.\n\nlibrary(GGally)\n\nto_corr &lt;- alkaloids_totals |&gt;\n  select(all_of(alkaloid_total_names)) \n\nggcorr(to_corr)\n\n\n\n\n\n\n\n\nThere is only one labeled axis - this is because there is no diagonal in these plots, like we saw with ggcorrplot() and corrplot().\nWe can now spend some time improving the aesthetics of our plot.\n\nggcorr(to_corr, # data for correlation\n       low = \"#f1a340\", # -1 correlation color\n       mid = \"#f7f7f7\", # 0 correlation color\n       high = \"#998ec3\") # 1 correlation color\n\n\n\n\n\n\n\nggcorr(to_corr,\n       low = \"#f1a340\", mid = \"#f7f7f7\", high = \"#998ec3\",\n       geom = \"circle\",\n       label = TRUE, \n       label_size = 2, \n       label_round = 2,\n       layout.exp = 3)\n\n\n\n\n\n\n\n\nFor this example, we have very long label names which are really difficult to wrap, but if your labels are more reasonable this may work well for you.\n\n\nggpairs()\nWe can also use the function GGally::ggpairs() to make a matrix of correlation related plots.\n\nalkaloids_totals |&gt;\n  ggpairs(columns = c(\"Tomatine\", \"TotalLycoperosideFGEsculeosideA\", \"Total\"), # pick variables\n          aes(color = Class))\n\n\n\n\n\n\n\n\nLet‚Äôs customized a bit.\n\n# remove zeroes since they don't log transform\n# make log transformed columns\nalkaloids_totals_log &lt;- alkaloids_totals |&gt;\n  filter(Tomatine != 0, \n         TotalLycoperosideFGEsculeosideA != 0,\n         Total != 0) |&gt;\n  mutate(log10_tomatine = log10(Tomatine),\n         log10_FGA = log10(TotalLycoperosideFGEsculeosideA),\n         log10_total = log10(Total))\n\nalkaloids_totals_log |&gt;\n    ggpairs(columns = c(\"log10_tomatine\", \"log10_FGA\", \"log10_total\"),\n          aes(color = Class, alpha = 0.5), # note alpha inside aes which is weird idk why\n          columnLabels = c(\"Alpha-Tomatine\", \"Lycoperoside F/G\\n Escueloside A\", \"Total Alkaloids\"))",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#manually-making-correlation-plots-with-reshapemelt-and-ggplot",
    "href": "modules/module3/08_correlations/08_correlations.html#manually-making-correlation-plots-with-reshapemelt-and-ggplot",
    "title": "Visualizing Correlations",
    "section": "Manually making correlation plots with reshape::melt() and ggplot",
    "text": "Manually making correlation plots with reshape::melt() and ggplot\nBecause some of the correlation specific packages are hard to customize, I am going to show you how to make your own plots by reshaping your data with reshape2::melt() and some base R functions, and plotting using the standard ggplot syntax.\n\nlibrary(reshape2) # contains melt()\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# take cor matrix and convert to df with 3 columns: Var1, Var2, and value\nmelted_alkaloids_cor &lt;- melt(alkaloids_cor)\n\n# what does it look like?\nhead(melted_alkaloids_cor)\n\n                  Var1              Var2       value\n1    Dehydrotomatidine Dehydrotomatidine 1.000000000\n2           Tomatidine Dehydrotomatidine 0.297446153\n3 TotalDehydrotomatine Dehydrotomatidine 0.032491778\n4             Tomatine Dehydrotomatidine 0.023823011\n5 TotalHydroxytomatine Dehydrotomatidine 0.009912624\n6 TotalAcetoxytomatine Dehydrotomatidine 0.032202892\n\n\nFirst pass minimalist plotting\n\nmelted_alkaloids_cor |&gt;\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile()\n\n\n\n\n\n\n\n\nLots to fix! What if we want only the upper or lower triangle, again since this plot is symmetric.\n\nUpper triangle\nKeep only the upper triangle.\n\n# \"save as\"\nalkaloids_upper &lt;- alkaloids_cor\n\n# use function lower.tri() and set the lower triangle all to NA\n# then we can keep only the upper triangle\nalkaloids_upper[lower.tri(alkaloids_upper)] &lt;- NA\n\n# melt to go back to long format\nmelted_alkaloids_upper &lt;- melt(alkaloids_upper, na.rm = TRUE)\n\n# did it work?\nhead(melted_alkaloids_upper) # yup\n\n                   Var1                 Var2      value\n1     Dehydrotomatidine    Dehydrotomatidine 1.00000000\n11    Dehydrotomatidine           Tomatidine 0.29744615\n12           Tomatidine           Tomatidine 1.00000000\n21    Dehydrotomatidine TotalDehydrotomatine 0.03249178\n22           Tomatidine TotalDehydrotomatine 0.37446722\n23 TotalDehydrotomatine TotalDehydrotomatine 1.00000000\n\n\n\n\nLower triangle\nCreate a lower triangle object to plot.\n\n# \"save as\"\nalkaloids_lower &lt;- alkaloids_cor\n\n# use function upper.tri() and set the upper triangle all to NA\n# then we can keep only the lower triangle\nalkaloids_lower[upper.tri(alkaloids_lower)] &lt;- NA\n\n# melt to go back to long format\nmelted_alkaloids_lower &lt;- melt(alkaloids_lower, na.rm = TRUE)\n\n# did it work?\nhead(melted_alkaloids_lower) # yup\n\n                  Var1              Var2       value\n1    Dehydrotomatidine Dehydrotomatidine 1.000000000\n2           Tomatidine Dehydrotomatidine 0.297446153\n3 TotalDehydrotomatine Dehydrotomatidine 0.032491778\n4             Tomatine Dehydrotomatidine 0.023823011\n5 TotalHydroxytomatine Dehydrotomatidine 0.009912624\n6 TotalAcetoxytomatine Dehydrotomatidine 0.032202892\n\n\nPlot\n\n# remember we made alkaloid_labels\nprint(alkaloid_labels)\n\n [1] \"Dehydrotomatidine\"                                 \n [2] \"Tomatidine\"                                        \n [3] \"Dehydrotomatine\"                                   \n [4] \"Alpha-Tomatine\"                                    \n [5] \"Hydroxytomatine\"                                   \n [6] \"Acetoxytomatine\"                                   \n [7] \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\"\n [8] \"Lycoperoside F, G, \\nor Escueloside A\"             \n [9] \"Escueloside B\"                                     \n[10] \"Total Steroidal Alkaloids\"                         \n\nmelted_alkaloids_lower |&gt;\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#f1a340\",\n                       mid = \"#f7f7f7\",\n                       high = \"#998ec3\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = alkaloid_labels) +\n  scale_y_discrete(labels = alkaloid_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y =\"\",\n       title = \"Correlation between steroidal alkaloids using \\nPearson's correlation coefficient\")",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations.html#useful-resources",
    "href": "modules/module3/08_correlations/08_correlations.html#useful-resources",
    "title": "Visualizing Correlations",
    "section": "Useful resources",
    "text": "Useful resources\n\ncor()\nHmisc::rcorr()\nggcorrplot::ggcorrplot()\ncorrplot::corrplot()\nGGally\nGGally:ggcorr()\nGGally:ggpairs()",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(???)",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#introduction",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(???)",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#total-cupping-score-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#total-cupping-score-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "Total cupping score in Arabica and Robusta",
    "text": "Total cupping score in Arabica and Robusta\nMake 3 different visualizations that shows the distribution of total cupping score (i.e.¬†total_cup_points) across Arabica and Robusta beans. Make the plots so you think they look good.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation ‚òï",
    "section": "Individual characteristic cupping scores in Arabica and Robusta",
    "text": "Individual characteristic cupping scores in Arabica and Robusta\nMake 3 different visualizations that show the distribution of all the individual contributors (i.e., aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points) to total cupping score across Arabica and Robusta in one plot.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html",
    "href": "modules/module3/07_distributions/07_distributions.html",
    "title": "Understanding Data Distributions",
    "section": "",
    "text": "Figure from Allison Horst",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions.html#introduction",
    "title": "Understanding Data Distributions",
    "section": "Introduction",
    "text": "Introduction\nWe will will building on our lesson on ggplot101 and ggplot102 which focused on an overall understanding of the grammar of graphics, basic syntax, adding data, aesthetic mappings, geoms, facets, scales, labels, and themes. Today we are going to apply what we learned towards trying to better understanding our underlying data distributions.\nOften, we think about figure generation as the last part of the scientific process, something you do as you prepare a manuscript for publication. I hope to convince you that exploring your data, and making exploratory plots is a critical part of the data analysis and interpretation process.\n\n\n\n\n\nFigure from Allison Horst\n\n\n\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load the packages we need.\n\nlibrary(tidyverse)\n\nToday we are using real research data from my group. We will be reading in the supplementary data from a paper written by Michael Dzakovich, and published in The Plant Genome.\n\n\n\n\n\n\nDownload today‚Äôs data\n\n\n\nYou can download the Excel file from the supporting information link, or you can access it directly here.\n\n\nThe data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 1. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloids &lt;- readxl::read_excel(\"tpg220192-sup-0002-supmat.xlsx\",\n                                sheet = \"S1 Raw Data Diversity Panel\")\n\n\nknitr::kable(head(alkaloids))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nYear\nEnvironment\nBlock\nGenotype\nPlot_Source\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nLatitude\nLongitude\nDehydrotomatidine\nTomatidine\nDehydrotomatine1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\n\n\n\n\n7805\n2018\nFreEarly18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.000000\n5.726010\n0.350331\n6.076341\n172.66244\n1.079190\n86.72742\n17.831892\n9.142607\n114.78111\n18.902399\n56.307182\n1.890053\n77.099634\n5.125904\n10.277325\n336.8893\n347.1666\n3.787979\n0.924195\n3.943230\n8.655404\n731.5675\n\n\n7898\n2017\nFre17\n2\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.000000\n0.169068\n0.000000\n0.000000\n0.000000\n55.47329\n0.000000\n53.32292\n13.630697\n4.841762\n71.79538\n3.557348\n4.107289\n0.000000\n7.664637\n2.905500\n5.548102\n199.6694\n205.2175\n8.978931\n1.897850\n6.794690\n17.671471\n360.8969\n\n\n7523\n2018\nFreLate18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.135675\n0.680554\n5.073552\n0.000000\n5.073552\n123.85835\n0.000000\n50.90989\n6.503939\n1.368847\n58.78268\n3.931461\n4.123222\n0.623340\n8.678023\n2.185082\n5.104115\n259.0177\n264.1218\n4.049145\n0.000000\n6.749386\n10.798531\n474.3143\n\n\n7724\n2017\nFre17\n1\nCULBPT_05_11\n2K9-8584\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.054300\n0.497261\n19.419087\n0.000000\n19.419087\n239.01264\n0.000000\n36.02318\n8.557673\n7.483933\n52.06478\n3.341048\n16.415426\n1.057100\n20.813574\n0.000000\n0.000000\n203.0061\n203.0061\n1.678210\n0.000000\n2.349633\n4.027843\n538.8955\n\n\n7427\n2018\nFreLate18\n1\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.139454\n0.553801\n0.000000\n0.000000\n0.000000\n64.31783\n0.879435\n39.91027\n7.228388\n3.015298\n51.03339\n0.000000\n3.131685\n0.000000\n3.131685\n0.000000\n4.054211\n299.5687\n303.6229\n10.146857\n0.000000\n4.882339\n15.029197\n437.8283\n\n\n7854\n2018\nFreEarly18\n2\nCULBPT_05_11\n2K17-7724\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n40.712800000000001\n-74.006\n0.049700\n0.262174\n3.737579\n0.000000\n3.737579\n68.44913\n0.000000\n23.86864\n13.506299\n1.456982\n38.83192\n4.657902\n4.259007\n0.605729\n9.522638\n9.832149\n11.595595\n459.5205\n471.1161\n6.839930\n0.486236\n5.595751\n12.921917\n614.7233\n\n\n\n\n\nThis dataset has 605 observations, with data about different steroidal alkaloids in the fruits of different tomato germplasm grown in 3 locations across 2 years. There is also some other metadata too.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#geoms-for-distributions",
    "href": "modules/module3/07_distributions/07_distributions.html#geoms-for-distributions",
    "title": "Understanding Data Distributions",
    "section": "Geoms for distributions",
    "text": "Geoms for distributions\n\ngeom_col()\nOften, people use bar charts, representing the height or the length of the bar as proportional to the average value that it represents. These charts are sometimes called dynamite plots because they resemble (when they have an error bar with whisker) those cartoon style dynamite sticks. Pow!\nHowever, these bar charts, even if you add a standard deviation/error, really can hide the true distribution of your data, and for this reason, I and others hope you don‚Äôt select to make them.\nAside: You may be thinking ‚ÄúJess you asked us to make one of these in Module 2 homework‚Äù and I did but also that was a little different. The plot I asked you to make shows the number of degrees awarded, a value for which there really is no distribution. So in that case we are using a bar plot to show something different than a bar plot which is meant to show somehow an average/median and distribution.\nI hope after today, you see that there is always a better chart type to make than a bar chart. But I will show you how to make them anyway.\nBefore we plot, let‚Äôs calculate some summary statistics so we know what we should expect.\n\nalkaloids |&gt;\n  group_by(Class) |&gt;\n  summarize(mean_tomatine = mean(Tomatine))\n\n# A tibble: 5 √ó 2\n  Class                 mean_tomatine\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 Cultivated Cherry              235.\n2 Cultivated Processing          330.\n3 S. pimpinellifolium            685.\n4 Wide Cross Hybrid              534.\n5 Wild Cherry                   4928.\n\n\n\n# this is wrong but an easy mistake to make\n# this is not what we want\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_col()\n\n\n\n\n\n\n\n\nJust calling geom_col() does not give us what we want. Look at the y-axis scale and how out of line this is with our summary statistics. The reason for this is that geom_col() defaults to position = \"stack\" which will just sum the alkaloid content across all the observations. Even changing to position = \"identity\" does not work. This is because we are plotting a transformation of the data (calculation of the mean) which these geoms are not doing.\nWe can calculate manually by generating the summary values and then piping that into our ggplot call.\n\nalkaloids |&gt;\n  group_by(Class) |&gt;\n  summarize(mean_tomatine = mean(Tomatine)) |&gt;\n  ggplot(aes(x = Class, y = mean_tomatine)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nstat_summary()\nAn easier way to do this would be just with stat_summary(), which does not require the calculation of summary statistic first.\n\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\n\n\n\n\n\nReordering x-variables\nNote in these plots the ordering of the x-axis categories ‚Äì they are alphabetical. This is the ggplot default. There are many reasons why this might not be the most compelling ordering for your data. You may want to order from lowest to highest mean, or in this case, I want to order the tomatoes from most cultivated on the left, to most wild on the right, since this is the prevailing theme of our paper.\nWe can do this in two ways:\nSimply reorder the plot.\n\n# set what the order is\nalkaloids_order &lt;- c(\"Cultivated Processing\",\n                     \"Cultivated Cherry\",\n                     \"Wide Cross Hybrid\",\n                     \"Wild Cherry\",\n                     \"S. pimpinellifolium\")\n\n# plot and re-level within aes()\nalkaloids |&gt;\n  ggplot(aes(x = factor(Class, levels = alkaloids_order), y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\n\n\n\n\nChange the levels of the data so the reordering happens to every plot in the future.\n\n# what type of variable is Class?\nclass(alkaloids$Class)\n\n[1] \"character\"\n\n# convert to factor, and set levels\nalkaloids$Class &lt;- factor(alkaloids$Class,\n                          levels = c(\"Cultivated Processing\",\n                                     \"Cultivated Cherry\",\n                                     \"Wide Cross Hybrid\",\n                                     \"Wild Cherry\",\n                                     \"S. pimpinellifolium\"))\n\n\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_summary(fun = \"mean\", geom = \"bar\")\n\n\n\n\n\n\n\n\nMy tendency would be to re-level the data if I always want to use the same order, and just re-level the plot if I only want to do this once or twice.\nYou can also re-order factors based on characteristics of your data using different functions that are part of the forcats package.\n\n\n\ngeom_boxplot()\nA boxplot has the benefit of showing you more than the median and the standard deviation, so you can better see the true distribution of your data. With geom_boxplot():\n\nLower whisker: smallest observation greater than or equal to lower hinge - 1.5 * IQR\nLower hinge/bottom line of box part of boxplot 25% quantile\nMiddle: median, 50% quantile\nUpper hinge/top line of box part of boxplot 75% quantile\nUpper whisker largest observation less than or equal to upper hinge + 1.5 * IQR\n\n\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nOne reason why this is really importantly different from the bar plot is look at the number of outliers we are seeing for Wild Cherry. You don‚Äôt capture this at all with the median/mean bar plots.\nBecause of the scale of this data, it might be beneficial to log transform the y-axis.\n\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot() +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning in scale_y_continuous(trans = \"log10\"): log-10 transformation\nintroduced infinite values.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\ngeom_jitter()\ngeom_jitter() is a shortcut for geom_point(position = \"jitter\"), but is common enough that the shortcut exists. It is often nice to jitter on top of a boxplot. Note, if you don‚Äôt want the outliers from geom_boxplot() to be plotted twice, you should indicate outlier.shape = NA.\n\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning in scale_y_continuous(trans = \"log10\"): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nJittering introduces a small amount of variation into your points so they‚Äôre easier to see.\n\nA width of 0 is no horizontal jitter\nA height of 0 is no vertical jitter\n\nTypically you don‚Äôt want veritcal jitter so that the points retain their fidelity on the y-axis (which is where their concentration is plotted). I basically always use geom_jitter(height = 0) for plots where I want to retain y-axis fidelity.\n\nalkaloids |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(height = 0) +\n  scale_y_continuous(trans = \"log10\") # or scale_y_log10()\n\nWarning in scale_y_continuous(trans = \"log10\"): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\ngeom_histogram()\nWe could also look at these distribution more like histograms and it provides to us some additional information. When coupled with faceting, this can be very powerful.\n\nalkaloids |&gt;\n  ggplot(aes(x = Tomatine)) +\n  geom_histogram(bins = 75) + # default is bins = 30\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(vars(Class))\n\n\n\n\n\n\n\n\n\nalkaloids |&gt;\n  ggplot(aes(x = Tomatine)) +\n  geom_density() +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(vars(Class))\n\n\n\n\n\n\n\n\n\n\nggridges::geom_density_ridges()\nI really like the function geom_density_ridges() which is a part of the ggplot add-on package ggridges. It allows you to create ridgeline plots to show distributes in a single non-faceted plot.\n\nlibrary(ggridges) # for ridgeline plots\nlibrary(scales) # for comma format\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Tomatine, y = Class)) +\n  geom_density_ridges(alpha = 0.5) +\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\")\n\n\n\n\n\n\n\n\nYou can also use the function geom_density_ridges() which will allow you to easily map quantiles or other functions on top of your ridges.\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Tomatine, y = Class)) +\n  stat_density_ridges(alpha = 0.5,\n                      quantile_lines = TRUE,\n                      quantiles = 2) + # break into 2 groups, therefore median\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\",\n       caption = \"Black line represents tomato class median concent\")\n\nPicking joint bandwidth of 0.198\n\n\n\n\n\n\n\n\n\n\nChanging class labels\nI am bothered by the fact that S. pimpinellifolium (a species of wild tomato) is not indicated in italics. We don‚Äôt want to italicize all of the labels, just S. pimpinellifolium. Let‚Äôs fix that.\nWe can start by creating a vector of the labels how we want them to appear in the plot.\n\nclass_labels &lt;- c(\"Cultivated Processing\", \n                  \"Cultivated Cherry\",\n                  \"Wide Cross Hybrid\", \n                  \"Wild Cherry\",\n                  expression(italic(\"S. pimpinellifolium\")))\n\nclass_labels\n\nexpression(\"Cultivated Processing\", \"Cultivated Cherry\", \"Wide Cross Hybrid\", \n    \"Wild Cherry\", italic(\"S. pimpinellifolium\"))\n\n\nThen we can use one of the scale_*() functions to change our y-axis scale labels to how we want them to be.\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Tomatine, y = Class)) +\n  geom_density_ridges(alpha = 0.5) +\n  scale_x_continuous(trans = \"log10\", labels = comma) +\n  scale_y_discrete(labels = class_labels) +\n  labs(x = \"Alpha-tomatine content, ¬µg/100g fresh weight\",\n       y = \"\",\n       title = \"Distribution of Alpha-Tomatine Content Across 107 Accessions \\nof Tomato Grown Across 3 Environments\")\n\nPicking joint bandwidth of 0.198\n\n\n\n\n\n\n\n\n\nIf for example your variables were mapped to color or fill, you could do this using scale_color_manual() or scale_fill_manual(), respectively.\n\n\n\nggdist functions\nAnother ggplot extension package ggdist has cool geoms you can integrate into ggplots to visualize distributions. I think these work better than geom_dotplot().\nSometimes using geom_jitter() when you have a lot of data points can look a bit messy. I think in this case, using geom_dots() works very well. The default orientation is is layout = \"bin\"\n\nlibrary(ggdist)\n\n\nAttaching package: 'ggdist'\n\n\nThe following objects are masked from 'package:ggridges':\n\n    scale_point_color_continuous, scale_point_color_discrete,\n    scale_point_colour_continuous, scale_point_colour_discrete,\n    scale_point_fill_continuous, scale_point_fill_discrete,\n    scale_point_size_continuous\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Tomatine, y = Class)) +  \n  geom_dots() +\n  scale_x_continuous(trans = \"log10\", labels = comma) + \n  scale_y_discrete(labels = class_labels) +\n  labs(x = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       y = \"\",\n       title = \"Distribution of alkaloid content found among \\ntomatoes of different classes\")\n\n\n\n\n\n\n\n\nYou can really change the feel of the plot by changing the orientation between horizontal and vertical. If you want to use the orientation layout = \"swarm\" you need the package ggbeeswarm. This is also a nice package that performs similarly to ggdist but has less functionality which is why I‚Äôm covering ggdist here.\n\nlibrary(ggbeeswarm) # required for layout = \"swarm\"\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  geom_dots(side = \"both\", layout = \"swarm\") + # requires ggbeeswarm\n  scale_x_discrete(labels = class_labels) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  labs(x = \"\",\n       y = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       title = \"Distribution of alkaloid content found among tomatoes of different classes\")\n\n\n\n\n\n\n\n\nYou can also use stat_dotsinterval() which will by default add the median and the interquartile range (though you can change exactly what you want to be displayed).\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Class, y = Tomatine)) +\n  stat_dotsinterval(side = \"both\") +\n  scale_x_discrete(labels = class_labels) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  labs(x = \"\",\n       y = \"Alpha-tomatine, ¬µg/100 g fresh weight\",\n       title = \"Distribution of alkaloid content found among tomatoes of different classes\")\n\n\n\n\n\n\n\n\nDon‚Äôt forget we can keep layering. We can always map other aethetics to our plot (e.g.¬†shape = as.factor(Year), and we include as.factor() because Year is a character datatype).\n\nalkaloids |&gt;\n  filter(Tomatine != 0) |&gt;\n  ggplot(aes(x = Class, y = Tomatine, shape = as.factor(Year))) +\n  scale_y_continuous(trans = \"log10\", labels = comma) + \n  geom_dots(side = \"both\") +\n  theme_ggdist() +\n  theme(legend.position = c(.18, .99),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(5, 5, 5, 5)) +\n  labs(shape = \"Year\",\n       y = \"Alpha-tomatine (¬µg/100 g fresh weight)\")",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions.html#useful-resources",
    "href": "modules/module3/07_distributions/07_distributions.html#useful-resources",
    "title": "Understanding Data Distributions",
    "section": "Useful resources",
    "text": "Useful resources\n\nggplot2 cheatsheet\nggplot2 documentation\nggplot2: elegant graphics for data analysis by Hadley Wickham\nA really compehensive list of resources compiled by Erik Gahner Larsen\nggridges\nggdist\nggbeeswarm\nPast ggplot Code Clubs:\n\nVisualizing Data by Michael Broe\nggplot round 2 by me\nFaceting, multi-plots, and animating\nVisualizing Data by Michael Broe a second one\nggplot round 2 a second one by me",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 7 - Data distributions"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\nBelwo are the packages that I used to make my plot.\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(rstatix)\nlibrary(ggpubr)\nlibrary(glue)\nlibrary(rcompanion)",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html#introduction",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\nBelwo are the packages that I used to make my plot.\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(rstatix)\nlibrary(ggpubr)\nlibrary(glue)\nlibrary(rcompanion)",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "title": "Annotating Statistics onto Plots Recitation",
    "section": "Is total cholesterol (TotChol) different by age (AgeDecade)?",
    "text": "Is total cholesterol (TotChol) different by age (AgeDecade)?\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint - you want to test your assumptions to see what tests to do. You might need to use different posthoc comparison methods than we did in class.\n\n\n\n\n\n\n\n\n\nNeed another hint? (Click to expand)\n\n\n\n\n\nAnother hint - the function rcompanion::cldList() will convert the resulting comparison table from a posthoc Dunn test to create a column with the letters indicating which groups are significantly different from each other.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html",
    "href": "modules/module4/10_pca/10_pca.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Today we are going to start Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\nPCA is a data reduction approach, and useful if you have many variables, for example, thousands of genes or metabolites. PCA creates summary variables (the principal components) which maximize the variation in the dataset. It can be categorized as an unsupervised approach, as PCA doesn‚Äôt know which samples belong to your different groups. When you look at a scores plot, points that are closer together are more similar based on your input data, and those further apart are more different. The location of the loadings helps you understand what is driving those differences in your scores plot.\nIf you are unfamiliar with PCA, I‚Äôd recommend these two Youtube videos by Josh Starmer of StatQuest which explain PCA in 5 mins, or with more detail in 20 min. Bam üí•!\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\n\n\n\nToday we are going to continue to use the same real research data from my group from the lessons on distributions and correlations. We will be reading in the supplementary data from a paper from my group written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 3. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloid_blups &lt;- read_excel(\"data/tpg220192-sup-0002-supmat.xlsx\",\n                             sheet = \"S3 BLUP Diversity Panel\")\n\nLet‚Äôs take a look at this new data sheet.\n\nknitr::kable(head(alkaloid_blups))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenotype\nPlot_Source\nSpecies\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nDehydrotomatidine\nTomatidine\nDehydrotomatineA1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\nLatitude\nLongitude\n\n\n\n\nCULBPT_05_11\n2K9-8584\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n0.0001504\n0.0043495\n0.0072241\n-0.0010738\n0.0061503\n0.1929106\n0.0102521\n0.1596566\n0.0352666\n0.0864042\n0.2917962\n-0.0680508\n-0.1242448\n0.0117481\n-0.1805475\n-0.0369856\n0.0056573\n-0.1693034\n-0.1636462\n-0.0340685\n-0.0061713\n-0.0371543\n-0.0773942\n0.0365648\n40.712800000000001\n-74.006\n\n\nCULBPT_05_15\n2K9-8622\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n-0.0001716\n0.0086547\n0.0000000\n0.0086547\n0.2824926\n0.0006910\n0.1118818\n0.0243950\n0.0189345\n0.1559021\n0.0038232\n0.0467380\n0.0024737\n0.0530349\n0.0066251\n0.0125334\n0.3417664\n0.3542998\n0.0138429\n0.0021833\n0.0139309\n0.0299571\n0.8907677\n40.712800000000001\n-74.006\n\n\nCULBPT_05_22\n2K17-7708-1\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000649\n-0.0002516\n0.0085205\n0.0000000\n0.0085205\n0.1755250\n-0.0001804\n0.0505547\n0.0040900\n0.0023229\n0.0576202\n0.0078240\n0.0069610\n-0.0001145\n0.0146705\n0.0031468\n0.0150674\n0.3840689\n0.3991363\n0.0007128\n0.0008227\n0.0028133\n0.0043489\n0.6618186\n40.712800000000001\n-74.006\n\n\nCULBPT04_1\n2K9-8566\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n0.0001259\n-0.0038737\n0.0000000\n-0.0038737\n-0.0061446\n0.0015930\n0.1059454\n0.0195829\n0.0083566\n0.1354779\n0.0098585\n0.0095344\n0.0007941\n0.0201870\n0.0049377\n0.0100416\n0.3466282\n0.3566699\n0.0100830\n0.0006432\n0.0089015\n0.0196277\n0.5269806\n40.712800000000001\n-74.006\n\n\nE6203\n2K9-8600\nProcessing\nCultivated Processing\nUSA\nCA\nSLL_processing_1\nSLL_processing_1_1\nSLL\nSLL_processing_CA\nArid\n-0.0000272\n0.0000159\n0.0099538\n0.0000000\n0.0099538\n0.2929606\n0.0000000\n0.0227075\n0.0039160\n0.0088279\n0.0354514\n0.0002365\n0.0395374\n0.0041794\n0.0439532\n0.0027439\n0.0133225\n0.3018732\n0.3151958\n0.0080894\n-0.0003574\n0.0100045\n0.0177365\n0.7179839\n36.778300000000002\n-119.4179\n\n\nF06-2041\n2K16-9843\nProcessing\nCultivated Processing\nUSA\nOH\nSLL_processing_1\nSLL_processing_1_3\nSLL\nSLL_processing_OH\nHumid\n-0.0000272\n-0.0001648\n-0.0011236\n0.0000000\n-0.0011236\n0.0169681\n0.0011237\n0.0195573\n-0.0016308\n-0.0016069\n0.0188454\n-0.0004237\n0.0271524\n-0.0005286\n0.0262002\n0.0053084\n0.0049033\n0.1647694\n0.1696728\n-0.0006298\n-0.0003074\n0.0019315\n0.0009943\n0.2352713\n40.417299999999997\n-82.9071\n\n\n\n\n\nWhat are the dimensions of this dataframe?\n\ndim(alkaloid_blups)\n\n[1] 107  37\n\n\n\n\n\nHere we have the best linear unbiased predictors (BLUPs) representing the alkaloid content of 107 genotypes of tomatoes. There is extra meta-data here we won‚Äôt use, so like we did in correlations, we are going to create a vector to indicate which column name reprents the alkaloids we want to include in our principal components analysis. Then we can create a new trimmed dataframe.\n\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\nalkaloid_blups_trim &lt;- alkaloid_blups |&gt;\n  select(Genotype, Species, Class, all_of(alkaloid_total_names))\n\n# did it work?\ncolnames(alkaloid_blups_trim) # yes\n\n [1] \"Genotype\"                                \n [2] \"Species\"                                 \n [3] \"Class\"                                   \n [4] \"Dehydrotomatidine\"                       \n [5] \"Tomatidine\"                              \n [6] \"TotalDehydrotomatine\"                    \n [7] \"Tomatine\"                                \n [8] \"TotalHydroxytomatine\"                    \n [9] \"TotalAcetoxytomatine\"                    \n[10] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[11] \"TotalLycoperosideFGEsculeosideA\"         \n[12] \"TotalEsculeosideB\"                       \n[13] \"Total\"",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#introduction",
    "href": "modules/module4/10_pca/10_pca.html#introduction",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Today we are going to start Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\nPCA is a data reduction approach, and useful if you have many variables, for example, thousands of genes or metabolites. PCA creates summary variables (the principal components) which maximize the variation in the dataset. It can be categorized as an unsupervised approach, as PCA doesn‚Äôt know which samples belong to your different groups. When you look at a scores plot, points that are closer together are more similar based on your input data, and those further apart are more different. The location of the loadings helps you understand what is driving those differences in your scores plot.\nIf you are unfamiliar with PCA, I‚Äôd recommend these two Youtube videos by Josh Starmer of StatQuest which explain PCA in 5 mins, or with more detail in 20 min. Bam üí•!\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\n\n\n\nToday we are going to continue to use the same real research data from my group from the lessons on distributions and correlations. We will be reading in the supplementary data from a paper from my group written by Michael Dzakovich, and published in The Plant Genome. The data is present in a Excel worksheet, so we will use the function read_excel() from the tidyverse (but not core tidyverse) package readxl. We want to import Supplemental Table 3. You can indicate which sheet you want to import in the arguments to read_excel().\n\nalkaloid_blups &lt;- read_excel(\"data/tpg220192-sup-0002-supmat.xlsx\",\n                             sheet = \"S3 BLUP Diversity Panel\")\n\nLet‚Äôs take a look at this new data sheet.\n\nknitr::kable(head(alkaloid_blups))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenotype\nPlot_Source\nSpecies\nClass\nOrigin\nProvence\nBlanca_Cluster1\nBlanca_Cluster2\nPassport_Species\nPassport_Classification\nSim_Grouping\nDehydrotomatidine\nTomatidine\nDehydrotomatineA1\nDehydrotomatine2\nTotalDehydrotomatine\nTomatine\nHydroxytomatine1\nHydroxytomatine2\nHydroxytomatine3\nHydroxytomatine4\nTotalHydroxytomatine\nAcetoxytomatine1\nAcetoxytomatine2\nAcetoxytomatine3\nTotalAcetoxytomatine\nDehydrolycoperosideFGdehydroesculeosideA\nLycoperosideFGEsculeosideA1\nLycoperosideFGEsculeosideA2\nTotalLycoperosideFGEsculeosideA\nEsculeosideB1\nEsculeosideB2\nEsculeosideB3\nTotalEsculeosideB\nTotal\nLatitude\nLongitude\n\n\n\n\nCULBPT_05_11\n2K9-8584\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n0.0001504\n0.0043495\n0.0072241\n-0.0010738\n0.0061503\n0.1929106\n0.0102521\n0.1596566\n0.0352666\n0.0864042\n0.2917962\n-0.0680508\n-0.1242448\n0.0117481\n-0.1805475\n-0.0369856\n0.0056573\n-0.1693034\n-0.1636462\n-0.0340685\n-0.0061713\n-0.0371543\n-0.0773942\n0.0365648\n40.712800000000001\n-74.006\n\n\nCULBPT_05_15\n2K9-8622\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n-0.0001716\n0.0086547\n0.0000000\n0.0086547\n0.2824926\n0.0006910\n0.1118818\n0.0243950\n0.0189345\n0.1559021\n0.0038232\n0.0467380\n0.0024737\n0.0530349\n0.0066251\n0.0125334\n0.3417664\n0.3542998\n0.0138429\n0.0021833\n0.0139309\n0.0299571\n0.8907677\n40.712800000000001\n-74.006\n\n\nCULBPT_05_22\n2K17-7708-1\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000649\n-0.0002516\n0.0085205\n0.0000000\n0.0085205\n0.1755250\n-0.0001804\n0.0505547\n0.0040900\n0.0023229\n0.0576202\n0.0078240\n0.0069610\n-0.0001145\n0.0146705\n0.0031468\n0.0150674\n0.3840689\n0.3991363\n0.0007128\n0.0008227\n0.0028133\n0.0043489\n0.6618186\n40.712800000000001\n-74.006\n\n\nCULBPT04_1\n2K9-8566\nProcessing\nCultivated Processing\nUSA\nNY\nSLL_processing_2\nSLL_processing_2\nSLL\nSLL_processing_NY\nArid\n-0.0000272\n0.0001259\n-0.0038737\n0.0000000\n-0.0038737\n-0.0061446\n0.0015930\n0.1059454\n0.0195829\n0.0083566\n0.1354779\n0.0098585\n0.0095344\n0.0007941\n0.0201870\n0.0049377\n0.0100416\n0.3466282\n0.3566699\n0.0100830\n0.0006432\n0.0089015\n0.0196277\n0.5269806\n40.712800000000001\n-74.006\n\n\nE6203\n2K9-8600\nProcessing\nCultivated Processing\nUSA\nCA\nSLL_processing_1\nSLL_processing_1_1\nSLL\nSLL_processing_CA\nArid\n-0.0000272\n0.0000159\n0.0099538\n0.0000000\n0.0099538\n0.2929606\n0.0000000\n0.0227075\n0.0039160\n0.0088279\n0.0354514\n0.0002365\n0.0395374\n0.0041794\n0.0439532\n0.0027439\n0.0133225\n0.3018732\n0.3151958\n0.0080894\n-0.0003574\n0.0100045\n0.0177365\n0.7179839\n36.778300000000002\n-119.4179\n\n\nF06-2041\n2K16-9843\nProcessing\nCultivated Processing\nUSA\nOH\nSLL_processing_1\nSLL_processing_1_3\nSLL\nSLL_processing_OH\nHumid\n-0.0000272\n-0.0001648\n-0.0011236\n0.0000000\n-0.0011236\n0.0169681\n0.0011237\n0.0195573\n-0.0016308\n-0.0016069\n0.0188454\n-0.0004237\n0.0271524\n-0.0005286\n0.0262002\n0.0053084\n0.0049033\n0.1647694\n0.1696728\n-0.0006298\n-0.0003074\n0.0019315\n0.0009943\n0.2352713\n40.417299999999997\n-82.9071\n\n\n\n\n\nWhat are the dimensions of this dataframe?\n\ndim(alkaloid_blups)\n\n[1] 107  37\n\n\n\n\n\nHere we have the best linear unbiased predictors (BLUPs) representing the alkaloid content of 107 genotypes of tomatoes. There is extra meta-data here we won‚Äôt use, so like we did in correlations, we are going to create a vector to indicate which column name reprents the alkaloids we want to include in our principal components analysis. Then we can create a new trimmed dataframe.\n\nalkaloid_total_names &lt;- c(\"Dehydrotomatidine\",\n                          \"Tomatidine\",\n                          \"TotalDehydrotomatine\",\n                          \"Tomatine\",\n                          \"TotalHydroxytomatine\",\n                          \"TotalAcetoxytomatine\",\n                          \"DehydrolycoperosideFGdehydroesculeosideA\",\n                          \"TotalLycoperosideFGEsculeosideA\",\n                          \"TotalEsculeosideB\",\n                          \"Total\")\n\nalkaloid_blups_trim &lt;- alkaloid_blups |&gt;\n  select(Genotype, Species, Class, all_of(alkaloid_total_names))\n\n# did it work?\ncolnames(alkaloid_blups_trim) # yes\n\n [1] \"Genotype\"                                \n [2] \"Species\"                                 \n [3] \"Class\"                                   \n [4] \"Dehydrotomatidine\"                       \n [5] \"Tomatidine\"                              \n [6] \"TotalDehydrotomatine\"                    \n [7] \"Tomatine\"                                \n [8] \"TotalHydroxytomatine\"                    \n [9] \"TotalAcetoxytomatine\"                    \n[10] \"DehydrolycoperosideFGdehydroesculeosideA\"\n[11] \"TotalLycoperosideFGEsculeosideA\"         \n[12] \"TotalEsculeosideB\"                       \n[13] \"Total\"",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#run-pca",
    "href": "modules/module4/10_pca/10_pca.html#run-pca",
    "title": "Principal Components Analysis",
    "section": "Run PCA",
    "text": "Run PCA\nThere are many packages that have functions that run PCA (including ) but I think the most common function used is a part of base R, and is called prcomp().\n\n\n\n\n\n\nWarning\n\n\n\nNote, PCA will allow zeroes, but will throw an error if you feed it NAs.\n\n\n\nalkaloids_pca &lt;- prcomp(alkaloid_blups_trim |&gt; select(all_of(alkaloid_total_names)),\n                        scale = TRUE, # default is false\n                        center = TRUE) # default is true, just being explicit\n\nLet‚Äôs investigate alkaloids_pca.\n\nglimpse(alkaloids_pca)\n\nList of 5\n $ sdev    : num [1:10] 1.794 1.732 1.215 0.99 0.776 ...\n $ rotation: num [1:10, 1:10] 0.162 0.309 0.422 0.326 0.311 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n  .. ..$ : chr [1:10] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:10] 0.000169 0.00721 0.142798 1.865975 1.323755 ...\n  ..- attr(*, \"names\")= chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n $ scale   : Named num [1:10] 0.000505 0.024096 0.339379 5.889986 2.91824 ...\n  ..- attr(*, \"names\")= chr [1:10] \"Dehydrotomatidine\" \"Tomatidine\" \"TotalDehydrotomatine\" \"Tomatine\" ...\n $ x       : num [1:107, 1:10] -1.51 -1.5 -1.55 -1.55 -1.52 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:10] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\n\nprint(alkaloids_pca)\n\nStandard deviations (1, .., p=10):\n [1] 1.7944141686 1.7318715307 1.2151400892 0.9904494488 0.7763914595\n [6] 0.6695313752 0.4394142511 0.2041300753 0.1932182462 0.0001503975\n\nRotation (n x k) = (10 x 10):\n                                               PC1         PC2         PC3\nDehydrotomatidine                        0.1621938  0.05363402 -0.06746652\nTomatidine                               0.3088504 -0.13094437 -0.44243776\nTotalDehydrotomatine                     0.4222731 -0.30216776 -0.22116400\nTomatine                                 0.3263804 -0.28919346 -0.43424991\nTotalHydroxytomatine                     0.3111090 -0.07515005  0.42780515\nTotalAcetoxytomatine                     0.3190534 -0.17396967  0.54287286\nDehydrolycoperosideFGdehydroesculeosideA 0.2125680  0.49447329 -0.07346836\nTotalLycoperosideFGEsculeosideA          0.2130280  0.52056383 -0.08463631\nTotalEsculeosideB                        0.1864604  0.50165801 -0.10122926\nTotal                                    0.5191805  0.04443498  0.24834278\n                                                 PC4          PC5         PC6\nDehydrotomatidine                         0.92897283 -0.275272461 -0.09845544\nTomatidine                                0.13150651  0.482582095  0.65879800\nTotalDehydrotomatine                     -0.16555038 -0.170452131 -0.15777601\nTomatine                                 -0.13638091 -0.163532836 -0.41927007\nTotalHydroxytomatine                      0.15203796  0.706002906 -0.41591986\nTotalAcetoxytomatine                     -0.05462895 -0.326082234  0.41704857\nDehydrolycoperosideFGdehydroesculeosideA -0.16278360  0.013643330 -0.06919125\nTotalLycoperosideFGEsculeosideA          -0.09078436 -0.009476836 -0.02310538\nTotalEsculeosideB                         0.02495982 -0.037371129  0.01529154\nTotal                                    -0.11066018 -0.170588385  0.05600981\n                                                 PC7         PC8          PC9\nDehydrotomatidine                         0.13097525 -0.02201625 -0.012909093\nTomatidine                                0.03505366  0.05778857  0.054315300\nTotalDehydrotomatine                      0.03480557 -0.61023340 -0.475709677\nTomatine                                 -0.09303946  0.40353582  0.403874861\nTotalHydroxytomatine                     -0.04702814 -0.04023463  0.009165434\nTotalAcetoxytomatine                     -0.01377758 -0.02258828  0.182475550\nDehydrolycoperosideFGdehydroesculeosideA  0.61727735 -0.31823996  0.437114636\nTotalLycoperosideFGEsculeosideA           0.10012665  0.42335087 -0.583833541\nTotalEsculeosideB                        -0.76029219 -0.28461575  0.203734859\nTotal                                    -0.01570365  0.31194871 -0.025467058\n                                                  PC10\nDehydrotomatidine                        -0.0001604947\nTomatidine                                0.0010655927\nTotalDehydrotomatine                      0.0147128823\nTomatine                                  0.2559833151\nTotalHydroxytomatine                      0.1268345182\nTotalAcetoxytomatine                      0.5059522400\nDehydrolycoperosideFGdehydroesculeosideA  0.0080094872\nTotalLycoperosideFGEsculeosideA           0.3707973486\nTotalEsculeosideB                         0.0226436777\nTotal                                    -0.7239562737\n\n\n\nclass(alkaloids_pca)\n\n[1] \"prcomp\"\n\n\nWe can see that the resulting PCA object is a prcomp object, and is a list of 5 lists and vectors.\nThis includes:\n\nsdev: the standard deviations (square roots of the eigenvalues of the covariance matrix) of the principal components\nrotation: the PCs for the variables (i.e., the variable loadings)\nx: the PCs for samples (i.e., the scores)\ncenter: the centering used\nscale: the scaling used\n\nWe can also look at the output of our PCA in a different way using the function summary().\n\nsummary(alkaloids_pca) \n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     1.794 1.7319 1.2151 0.9904 0.77639 0.66953 0.43941\nProportion of Variance 0.322 0.2999 0.1477 0.0981 0.06028 0.04483 0.01931\nCumulative Proportion  0.322 0.6219 0.7696 0.8677 0.92796 0.97279 0.99210\n                           PC8     PC9      PC10\nStandard deviation     0.20413 0.19322 0.0001504\nProportion of Variance 0.00417 0.00373 0.0000000\nCumulative Proportion  0.99627 1.00000 1.0000000\n\n\nWe can convert this summary into something later usable by extraction the element importance from summary(alkaloids_pca) and converting it to a dataframe.\n\nimportance &lt;- summary(alkaloids_pca)$importance |&gt;\n  as.data.frame()\n\nknitr::kable(head(importance))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\n\n\n\n\nStandard deviation\n1.794414\n1.731872\n1.21514\n0.9904494\n0.7763915\n0.6695314\n0.4394143\n0.2041301\n0.1932182\n0.0001504\n\n\nProportion of Variance\n0.321990\n0.299940\n0.14766\n0.0981000\n0.0602800\n0.0448300\n0.0193100\n0.0041700\n0.0037300\n0.0000000\n\n\nCumulative Proportion\n0.321990\n0.621930\n0.76959\n0.8676900\n0.9279600\n0.9727900\n0.9921000\n0.9962700\n1.0000000\n1.0000000\n\n\n\n\n\nBy looking at the summary we can see, for example, that the first two PCs explain 62.19% of variance.\nWe are going to go over making scree, scores and loadings plots using helper functions (here, they start fviz_() and come from the package factoextra, and manually via ggplot. The helper functions allow you look at each plot type simply. This is an important step because when you make your plots with ggplot, you want to be sure they look how they should.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#scree-plot",
    "href": "modules/module4/10_pca/10_pca.html#scree-plot",
    "title": "Principal Components Analysis",
    "section": "Scree plot",
    "text": "Scree plot\nA scree plot shows what percentage of total variance is explained by each principal component.\n\nUsing fviz_eig()\nWe can do this quickly using fviz_eig().\n\nfviz_eig(alkaloids_pca)\n\nWarning in geom_bar(stat = \"identity\", fill = barfill, color = barcolor, :\nIgnoring empty aesthetic: `width`.\n\n\n\n\n\n\n\n\n\nYou can actually do this very easily with base R plotting as well. If you weren‚Äôt planning to publish this type of plot, it might not be important it look beautiful, and then both of these options would be great and quick. Note though that the base R plot is plotting at a different scale.\n\nplot(alkaloids_pca)\n\n\n\n\n\n\n\n\n\n\nManually\nIf you wanted to make a scree plot manually, you could by plotting using a wrangled version of the importance dataframe we made earlier.\n\nimportance_tidy &lt;- importance |&gt;\n  rownames_to_column(var = \"measure\") |&gt;\n  pivot_longer(cols = PC1:PC10,\n               names_to = \"PC\",\n               values_to = \"value\")\n\nimportance_tidy |&gt;\n  filter(measure == \"Proportion of Variance\") |&gt;\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col()\n\n\n\n\n\n\n\n\nAlmost! PC10 is displaying right after PC1 because alphabetically, this is the order. Let‚Äôs fix it.\n\n# create a vector with the order we want\nmy_order &lt;- colnames(importance)\n\n# relevel according to my_order\nimportance_tidy$PC &lt;- factor(importance_tidy$PC, levels = my_order)\n\n# check to see if it worked\nlevels(importance_tidy$PC)\n\n [1] \"PC1\"  \"PC2\"  \"PC3\"  \"PC4\"  \"PC5\"  \"PC6\"  \"PC7\"  \"PC8\"  \"PC9\"  \"PC10\"\n\n\nLet‚Äôs plot again.\n\nimportance_tidy |&gt;\n  filter(measure == \"Proportion of Variance\") |&gt;\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col()\n\n\n\n\n\n\n\n\nSuccess!\nIf we want to tighten up this plot we can.\n\nimportance_tidy |&gt;\n  filter(measure == \"Proportion of Variance\") |&gt;\n  ggplot(aes(x = PC, y  = value)) +\n  geom_col(alpha = 0.1, color = \"black\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  labs(x = \"Principal component\",\n       y = \"Percent variance explained\",\n       title = \"Scree plot of 10 alkaloids analyzed across 107 tomato accessions\")\n\n\n\n\n\n\n\n\nThis is a perfectly ready scree plot for the supplementary materials of a publication.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#scores-plot",
    "href": "modules/module4/10_pca/10_pca.html#scores-plot",
    "title": "Principal Components Analysis",
    "section": "Scores plot",
    "text": "Scores plot\nWhen people talk about PCA plots, what they most often mean is PCA scores plots. Here, each point represents a sample, and we are plotting their coordinates typically for the first 2 PCs. Sometimes people make 3D PCA plots with the first 3 PCs but I think these are not easy to look in 2D and I wouldn‚Äôt recommend you to put them in your papers.\n\nUsing fviz_pca_ind()\nWe can also look at a scores plot using fviz_pca_ind() where ind means individuals. Here, each point is a sample.\n\nfviz_pca_ind(alkaloids_pca)\n\n\n\n\n\n\n\n\nBecause our alkaloids_pca doesn‚Äôt have any meta-data, this is a hard to interpret plot, where each number indicates the rownumber of that sample. Making the scores plot this way is useful because it shows us the shape of the plot which we can use to confirm that we have made a ggplot that looks like its been created correctly.\n\n\nManually\nWe want to plot the scores, which are in provided in alkaloids_pca$x.\nWe can convert the list into a dataframe of scores values by using as.data.frame(). Then we can bind back our relevant metadata so they‚Äôre all together. Note, to use bind_cols() both datasets need to be in the same order. In this case they are so we are good.\n\n# create a df of alkaloids_pca$x\nscores_raw &lt;- as.data.frame(alkaloids_pca$x)\n\n# bind meta-data\nscores &lt;- bind_cols(alkaloid_blups |&gt; select(Genotype, Plot_Source, Species), # metadata\n                    scores_raw)\n\n# how does our new df look?\nscores[1:6, 1:6]\n\n# A tibble: 6 √ó 6\n  Genotype     Plot_Source Species      PC1    PC2     PC3\n  &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 CULBPT_05_11 2K9-8584    Processing -1.51 -1.17  0.00632\n2 CULBPT_05_15 2K9-8622    Processing -1.50 -0.915 0.0649 \n3 CULBPT_05_22 2K17-7708-1 Processing -1.55 -0.942 0.0658 \n4 CULBPT04_1   2K9-8566    Processing -1.55 -0.906 0.0817 \n5 E6203        2K9-8600    Processing -1.52 -0.940 0.0436 \n6 F06-2041     2K16-9843   Processing -1.58 -0.934 0.0677 \n\n\nNow we can plot.\n\nscores |&gt;\n  ggplot(aes(x = PC1, y = PC2, color = Species)) +\n  geom_point() \n\n\n\n\n\n\n\n\nOur shapes are looking the same, this is good. Let‚Äôs pretty up our plot.\nFirst let‚Äôs wrangle.\n\n# create objects indicating percent variance explained by PC1 and PC2\nPC1_percent &lt;- round((importance[2,1])*100, # index 2nd row, 1st column, times 100\n                     1) # round to 1 decimal\nPC2_percent &lt;- round((importance[2,2])*100, 1) \n\n# make Species a factor and set levels\n# from least wild to most wild\n# we did this back in data distributions.\nscores$Species &lt;- factor(scores$Species,\n                         levels = c(\"Processing\",\n                                    \"Cultivated Cherry\",\n                                    \"Hybrid\",\n                                    \"Wild Cherry\",\n                                    \"Wild\"))\n\nThen we can plot\n\n# plot\n(scores_plot &lt;- scores |&gt;\n  ggplot(aes(x = PC1, y = PC2, fill = Species)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(shape = 21, color = \"black\", size = 2.5, alpha = 0.7) +\n   # sequential color selecting hex codes from YlOrRd color by wildness\n  scale_fill_manual(values = c(\"#ffffcc\", \"#fed976\", \"#fd8d3c\", \"#e31a1c\", \"#800026\")) + \n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Scores Plot of 10 Alkaloids Present in 107 Tomato Accessions\"))\n\n\n\n\n\n\n\n\nThis looks nice.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#loadings-plot",
    "href": "modules/module4/10_pca/10_pca.html#loadings-plot",
    "title": "Principal Components Analysis",
    "section": "Loadings plot",
    "text": "Loadings plot\n\nUsing fviz_pca_var()\nWe can also look at a loadings plot using fviz_pca_var() where var means variables. Here, each point is a variable.\n\nfviz_pca_var(alkaloids_pca)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n‚Ñπ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\nManually\nWe can also make a more customized loadings plot manually using ggplot and using the dataframe alkaloids_pca$rotation.\n\n# grab raw loadings, without any metadata\nloadings_raw &lt;- as.data.frame(alkaloids_pca$rotation)\n\nloadings &lt;- loadings_raw |&gt;\n  rownames_to_column(var = \"alkaloid\")\n\nWe can then plot with ggplot like normal.\n\nloadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_text() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot\") \n\n\n\n\n\n\n\n\nWe have two problems with this plot.\n\nThe names are abbreviated and not how we want them to appear\nThe label names are on top of the points/each other\n\nWe can fix both of these problems.\nWe can create a vector of the labels as we want them to appear, as we have done previously.\n\nalkaloid_labels &lt;- c(\"Dehydrotomatidine\",\n                     \"Tomatidine\",\n                     \"Dehydrotomatine\",\n                     \"Alpha-Tomatine\",\n                     \"Hydroxytomatine\",\n                     \"Acetoxytomatine\",\n                     \"Dehydrlycoperoside F, G, \\nor Dehydroescueloside A\",\n                     \"Lycoperoside F, G, \\nor Escueloside A\",\n                     \"Escueloside B\",\n                     \"Total Steroidal Alkaloids\")\n\nThen we can re-plot with these labels.\n\nloadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +  \n  geom_text() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot\") \n\n\n\n\n\n\n\n\nOk the label names are better but they‚Äôre still smushed. The package ggrepel has some good functions to help us. You can try using geom_text_repel() and geom_label_repel().\n\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\nWith geom_text_repel()\n\nlibrary(ggrepel)\n\n(loadings_plot &lt;- loadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +  \n  geom_point() +\n  geom_text_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot using geom_text_repel()\"))\n\n\n\n\n\n\n\n\nWith geom_label_repel()\n\nloadings |&gt;\n  ggplot(aes(x = PC1, y = PC2, label = alkaloid_labels)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_label_repel() +\n  scale_fill_brewer() +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Loadings Plot using geom_label_repel()\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#patchwork",
    "href": "modules/module4/10_pca/10_pca.html#patchwork",
    "title": "Principal Components Analysis",
    "section": "patchwork",
    "text": "patchwork\nYou can pop these two plots side by side easing using the package patchwork.\n\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\n\nlibrary(patchwork)\n\nscores_plot + loadings_plot",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#biplot",
    "href": "modules/module4/10_pca/10_pca.html#biplot",
    "title": "Principal Components Analysis",
    "section": "Biplot",
    "text": "Biplot\n\nUsing fviz_pca().\nYou can make a biplot quickly with fviz_pca(). Note, fviz_pca_biplot() and fviz_pca() are the same.\n\nfviz_pca(alkaloids_pca)\n\n\n\n\n\n\n\n\nInstead of making this plot manually, let‚Äôs go through how to alter the existing plot made with fviz_pca(). We can do this because factoextra creates ggplot objects. To start off, we need to be using a dataframe that includes our metadata.\n\nfviz_pca(alkaloids_pca, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = \"text\") +\n  geom_point(aes(fill = alkaloid_blups$Species), shape = 21) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot Plot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       fill = \"Species\")\n\n\n\n\n\n\n\n\nThis is almost what we want - except we have only the abbreviated names for the alkaloids. Since in a biplot, we are really plotting two different sets of data (the scores and the loadings)there isn‚Äôt the ability to use labeller or similar with fviz_pca for the loadings only. There is a workaround though, we can go into our PCA object, change the rownames of alkaloids_pca$rotation to be our longer labels, and that should inherit to our new plot.\n\n# save as a new df\nalkaloids_pca_labelled &lt;- alkaloids_pca\n\n# assign alkaloid_labels to rownames\nrownames(alkaloids_pca_labelled$rotation) &lt;- alkaloid_labels\n\n# plot\nfviz_pca(alkaloids_pca_labelled, # pca object\n         label = \"var\",\n         repel = TRUE,\n         geom.var = c(\"text\", \"point\"),\n         col.var = \"black\") +\n  geom_point(aes(fill = alkaloid_blups$Species), shape = 21) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot Plot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       fill = \"Species\")\n\n\n\n\n\n\n\n\n\n\nManually\nWe can‚Äôt add the loadings right on top of the scores because they are on different scales.\n\n# plot\nscores |&gt;\n  ggplot() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(aes(x = PC1, y = PC2, fill = Species), # from piped data\n             shape = 21, color = \"black\", size = 2.5, alpha = 0.7) +\n  geom_point(data = loadings,\n             aes(x = PC1, y = PC2)) + # set locally\n   # sequential color selecting hex codes from YlOrRd color by wildness\n  scale_fill_manual(values = c(\"#ffffcc\", \"#fed976\", \"#fd8d3c\", \"#e31a1c\", \"#800026\")) + \n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Scores Plot of 10 Alkaloids Present in 107 Tomato Accessions\")\n\n\n\n\n\n\n\n\nThe issue here is that the scores and loadings are on different scales - let‚Äôs make them on the same scale.\nI can write a quick function to allow normalization.\n\nnormalize &lt;- function(x) return((x - min(x))/(max(x) - min(x)))\n\nThen I can nornalize the scores using the scale function, since the loadings are already normalized.\n\nscores_normalized &lt;- scores |&gt;\n  mutate(PC1_norm = scale(normalize(PC1), center = TRUE, scale = FALSE)) |&gt;\n  mutate(PC2_norm = scale(normalize(PC2), center = TRUE, scale = FALSE)) |&gt;\n  select(Genotype, Plot_Source, Species, \n         PC1_norm, PC2_norm, everything()) # pick metadata and reorder\n\nHow did it go? PC1_norm and PC2_norm should all now be between -1 and 1\n\n# look at first six rows\nhead(scores_normalized)\n\n# A tibble: 6 √ó 15\n  Genotype    Plot_Source Species PC1_norm[,1] PC2_norm[,1]   PC1    PC2     PC3\n  &lt;chr&gt;       &lt;chr&gt;       &lt;fct&gt;          &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 CULBPT_05_‚Ä¶ 2K9-8584    Proces‚Ä¶       -0.201      -0.114  -1.51 -1.17  0.00632\n2 CULBPT_05_‚Ä¶ 2K9-8622    Proces‚Ä¶       -0.200      -0.0892 -1.50 -0.915 0.0649 \n3 CULBPT_05_‚Ä¶ 2K17-7708-1 Proces‚Ä¶       -0.206      -0.0918 -1.55 -0.942 0.0658 \n4 CULBPT04_1  2K9-8566    Proces‚Ä¶       -0.206      -0.0883 -1.55 -0.906 0.0817 \n5 E6203       2K9-8600    Proces‚Ä¶       -0.203      -0.0916 -1.52 -0.940 0.0436 \n6 F06-2041    2K16-9843   Proces‚Ä¶       -0.210      -0.0910 -1.58 -0.934 0.0677 \n# ‚Ñπ 7 more variables: PC4 &lt;dbl&gt;, PC5 &lt;dbl&gt;, PC6 &lt;dbl&gt;, PC7 &lt;dbl&gt;, PC8 &lt;dbl&gt;,\n#   PC9 &lt;dbl&gt;, PC10 &lt;dbl&gt;\n\n# range of PC1\nrange(scores_normalized$PC1_norm)\n\n[1] -0.2128768  0.7871232\n\n# range of PC2\nrange(scores_normalized$PC2_norm)\n\n[1] -0.4608813  0.5391187\n\n\nWe can set the scores_normalized data for the overall plot and add the loadings within another geom_point().\n\nscores_normalized |&gt;\n  ggplot() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(aes(x = PC1_norm, y = PC2_norm, fill = Species), # scores\n             shape = 21, color = \"black\", size = 2.5, alpha = 0.7) + \n  geom_point(data = loadings, # loadings points\n             aes(x = PC1, y = PC2), color = \"gray\") +\n  geom_label_repel(data = loadings, # loadings labels\n                   aes(x = PC1, y = PC2, label = alkaloid_labels),\n                   size = 2.5, # make labels smaller \n                   fill = alpha(c(\"white\"),0.5)) + # make label background transparent\n  scale_fill_manual(values = c(\"#ffffcc\", \"#fed976\", \"#fd8d3c\", \"#e31a1c\", \"#800026\")) +\n  theme_minimal() +\n  labs(x = glue(\"PC1: {PC1_percent}%\"), \n       y = glue(\"PC2: {PC2_percent}%\"), \n       title = \"PCA Biplot of 10 Alkaloids Present in 107 Tomato Accessions\",\n       subtitle = \"Samples are colored points and loadings are grey with labels\")\n\n\n\n\n\n\n\n\nVoila.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca.html#useful-resources",
    "href": "modules/module4/10_pca/10_pca.html#useful-resources",
    "title": "Principal Components Analysis",
    "section": "Useful resources",
    "text": "Useful resources\n\nCode club about PCA by Jelmer Poelstra\nfactoextra\nPCA from DataCamp",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "",
    "text": "Today we are going to work with whole genome sequencing of the pig gut microbiome.\n\nPig microbiome study: paper, data.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-rows-and-columns-do-the-data-have",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-rows-and-columns-do-the-data-have",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "How many rows and columns do the data have?",
    "text": "How many rows and columns do the data have?",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-phyla-do-the-data-contains-and-how-many-columns-represents-metadata-of-the-experiment",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots_recitation.html#how-many-phyla-do-the-data-contains-and-how-many-columns-represents-metadata-of-the-experiment",
    "title": "Interactive plots with plotly and ggplotly recitation",
    "section": "How many phyla do the data contains and how many columns represents metadata of the experiment?",
    "text": "How many phyla do the data contains and how many columns represents metadata of the experiment?\n\nCreate a new column with a new phyla assignation\nKeep the phyla when they are Firmicutes or Bacteroidetes, otherwise assign Phyla to ‚ÄúOther phyla‚Äù.\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint: You may need to pivot the data to evaluate the column names as observations\n\n\n\n\n\nCompute the cumulative abundance by the new Phyla levels that you created\n\n\nCreate the bar plot in ggplot\n\n\nMake your plot interactive\n\nggplotly(your_awesome_plot)",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html",
    "href": "modules/module4/13_leftovers/13_leftovers.html",
    "title": "Leftover tidbits",
    "section": "",
    "text": "Today we are going to go over a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#introduction",
    "href": "modules/module4/13_leftovers/13_leftovers.html#introduction",
    "title": "Leftover tidbits",
    "section": "",
    "text": "Today we are going to go over a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#really-start-using-an-rproject",
    "href": "modules/module4/13_leftovers/13_leftovers.html#really-start-using-an-rproject",
    "title": "Leftover tidbits",
    "section": "Really start using an Rproject üìΩÔ∏è",
    "text": "Really start using an Rproject üìΩÔ∏è\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nI have noticed that many of you are still not using RProjects. I would really recommend that for easy file management that you do. Here is an a chapter in R for Data Science on how to set one up. If you want to start using Git in the future, you will need to set up a project.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#gghighlight",
    "href": "modules/module4/13_leftovers/13_leftovers.html#gghighlight",
    "title": "Leftover tidbits",
    "section": "gghighlight üî¶",
    "text": "gghighlight üî¶\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nThe package gghighlight allows you to highlight certain geoms in ggplot. Doing this helps your reader focus on the thing you want them to, and helps prevent plot spaghetti. To practice with gghighlight we are going to use some data from the R package gapminder\n\nInstall\n\ninstalll.packages(\"gghighlight\")\ninstall.packages(\"gapminder\")\n\n\n\nLoad libraries\nFirst let‚Äôs load our libraries.\n\nlibrary(gghighlight) # for highlighting\nlibrary(gapminder) # where data is\n\n\n\nWrangle\nWe can create a dataframe that includes only the data for the countries in the continent Americas.\n\ngapminder_americas &lt;- gapminder %&gt;%\n  filter(continent == \"Americas\")\n\n\n\nPlot\nIf we look at all the countries at once, we get plot spaghetti üçù.\n\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country, color = country)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")\n\n\n\n\n\n\n\n\nCreate a lineplot showing the life expectacy over 1952 to 2007 for all countries, highlighting the United States.\n\n# highlight just the US\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country, color = country)) +\n  geom_line() +\n  gghighlight(country == \"United States\") +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")\n\n\n\n\n\n\n\n\nFacet our plot, and highlight the country for each facet.\n\n# facet and highlight each country\ngapminder_americas %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_line(aes(color = country)) +\n  gghighlight() +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        strip.text.x = element_text(size = 8),\n        axis.text.x = element_text(angle = 90)) +\n  facet_wrap(vars(country)) +\n  labs(x = \"Year\",\n       y = \"Life Expectancy (years)\",\n       title = \"Life Expectancy in Countries in the Americas\",\n       subtitle = \"From 1952 to 2007\",\n       caption = \"Data from gapminder.org\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#patchwork-a-little-more",
    "href": "modules/module4/13_leftovers/13_leftovers.html#patchwork-a-little-more",
    "title": "Leftover tidbits",
    "section": "patchwork, a little more üìàüìäüìâ",
    "text": "patchwork, a little more üìàüìäüìâ\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nWe have talked a bit about patchwork in the lecture on PCA but its such a useful package I wanted to go over it a bit more. The goal of patchwork is to make it very simple to combine plots together.\n\nLoad libraries\n\nlibrary(patchwork)\nlibrary(palmerpenguins) # for making some plots to assemble\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\n\n\n\nMake some plots\n\nplot1 &lt;- penguins %&gt;%\n  ggplot(aes(x = species, y = body_mass_g, color = species)) +\n  geom_boxplot()\n\nplot2 &lt;- penguins %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()\n\nplot3 &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = island, y = flipper_length_mm, color = species)) +\n  geom_boxplot() +\n  facet_wrap(vars(sex))\n\n\n\nCombine plots\nThe simplest ways to combine plots is with the plus sign operator +. The forward slash / stacks plots. The pipe | puts plots next to each other. You can learn more about using patchwork here.\n\n(plot1 + plot2) / plot3 \n\n\n\n\n\n\n\n\nYou can also add annotation and style to your plots. Learn more here.\n\n(plot1 + plot2) / plot3 + plot_annotation(tag_levels = c(\"1\"),\n                  title = \"Here is some information about penguins\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#gganimate",
    "href": "modules/module4/13_leftovers/13_leftovers.html#gganimate",
    "title": "Leftover tidbits",
    "section": "gganimate üíÉ",
    "text": "gganimate üíÉ\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nhttps://gganimate.com/reference/transition_states.html\n\nInstall\n\ninstall.packages(\"gganimate\") # gganimate\ninstall.packages(\"gapminder\") # gapminder data for example\ninstall.packages(\"magick\") # for gif rendering\n\n\n\nLoad libraries\n\nlibrary(gganimate)\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\n\n\nPlot\nFirst let‚Äôs make a base plot. Note that this measure of population isn‚Äôt actually correct as its summing all of the populations across all the years.\n\n(base_plot &lt;- gapminder %&gt;%\n  filter(continent == \"Africa\") %&gt;%\n  ggplot(aes(x = pop, y = reorder(country, pop))) +\n  geom_col() +\n  scale_x_continuous(labels = scales::unit_format(unit = \"M\", scale = 1e-6)) +\n  theme_classic() +\n  labs(title = \"Population from 1952 to 2007 in Africa\", \n       x = \"Population\", \n       y = \"Country\"))\n\n\n\n\n\n\n\n\n\n(plot_to_animate &lt;- base_plot +\n  labs(subtitle = \"Year: {frame_time}\") + # label subtitle with year\n  transition_time(year) + # gif over year\n  ease_aes()) # makes the transitions smoother\n\n\n# set parameters for your animation\nanimated_plot &lt;- animate(plot = plot_to_animate, \n                         duration = 10, # number of seconds for whole animation\n                         fps = 10, # framerate, frames/sec\n                         start_pause = 20, # show first time for 20 frames\n                         end_pause = 20, # show end for 20 frames\n                         width = 700, # width in pixels\n                         height = 700, # height in pixels\n                         renderer = magick_renderer()) # program for rendering\n\n\n\nPrint\nPrint your animation.\n\nanimated_plot\n\n\n\n\n\n\n\n\n\n\n\n\nSave\nSave your animation.\n\n# save it\nanim_save(filename = \"gapminder_gif.gif\",\n          animation = last_animation())",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#ggradar",
    "href": "modules/module4/13_leftovers/13_leftovers.html#ggradar",
    "title": "Leftover tidbits",
    "section": "ggradar üì°",
    "text": "ggradar üì°\nThe package ggradar allows you to create radar plots, which allow the plotting of multidimensional data on a two dimension chart. Typically with these plots, the goal is to compare the variables on the plot across different groups. We are going to try this out with the coffee tasting data from the distributions recitation.\nInstall ggradar if you don‚Äôt already have it. This package is not available on CRAN for the newest version of R, so we can use devtools and install_github() to install it. You could also try using install.packages() and see if that works for you.\n\ndevtools::install_github(\"ricardo-bion/ggradar\",\n                         dependencies = TRUE)\n\n\nlibrary(ggradar)\nlibrary(scales) # for scaling data\n\n# load coffee data from distributions recitation\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n# extract out df on coffee_ratings\ncoffee &lt;- tuesdata$coffee_ratings\n\n# what are the column names again?\ncolnames(coffee)\n\n [1] \"total_cup_points\"      \"species\"               \"owner\"                \n [4] \"country_of_origin\"     \"farm_name\"             \"lot_number\"           \n [7] \"mill\"                  \"ico_number\"            \"company\"              \n[10] \"altitude\"              \"region\"                \"producer\"             \n[13] \"number_of_bags\"        \"bag_weight\"            \"in_country_partner\"   \n[16] \"harvest_year\"          \"grading_date\"          \"owner_1\"              \n[19] \"variety\"               \"processing_method\"     \"aroma\"                \n[22] \"flavor\"                \"aftertaste\"            \"acidity\"              \n[25] \"body\"                  \"balance\"               \"uniformity\"           \n[28] \"clean_cup\"             \"sweetness\"             \"cupper_points\"        \n[31] \"moisture\"              \"category_one_defects\"  \"quakers\"              \n[34] \"color\"                 \"category_two_defects\"  \"expiration\"           \n[37] \"certification_body\"    \"certification_address\" \"certification_contact\"\n[40] \"unit_of_measurement\"   \"altitude_low_meters\"   \"altitude_high_meters\" \n[43] \"altitude_mean_meters\" \n\n\nWe are going to wrangle the data to facilitate plotting. We are using rescale as we need the data for each attribute to be between 0 and 1.\n\n# tidy data to summarize easily\n(coffee_summary_long &lt;- coffee %&gt;%\n  select(species, aroma:cupper_points) %&gt;% # first column is the groups\n  pivot_longer(cols = aroma:cupper_points, # our favorite - tidy data to faciliate summarizing\n               names_to = \"attribute\",\n               values_to = \"score\") %&gt;% \n  group_by(attribute) %&gt;% # perform operations by species and attribute pairs\n  mutate(across(where(is.numeric), rescale)) %&gt;% # rescale data that is numeric\n  ungroup() |&gt; # get rid of grouping by attribute\n  group_by(species, attribute) |&gt; # group now by species and attribute\n  summarize(mean_score = mean(score)))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 √ó 3\n# Groups:   species [2]\n   species attribute     mean_score\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n 1 Arabica acidity            0.861\n 2 Arabica aftertaste         0.853\n 3 Arabica aroma              0.864\n 4 Arabica balance            0.859\n 5 Arabica body               0.876\n 6 Arabica clean_cup          0.983\n 7 Arabica cupper_points      0.750\n 8 Arabica flavor             0.851\n 9 Arabica sweetness          0.990\n10 Arabica uniformity         0.983\n11 Robusta acidity            0.875\n12 Robusta aftertaste         0.872\n13 Robusta aroma              0.880\n14 Robusta balance            0.862\n15 Robusta body               0.875\n16 Robusta clean_cup          0.993\n17 Robusta cupper_points      0.776\n18 Robusta flavor             0.864\n19 Robusta sweetness          0.768\n20 Robusta uniformity         0.990\n\n\nggradar takes wide data though, so we are going to pivot back to wide data.\n\n# go back to wide\ncoffee_summary_wide &lt;- coffee_summary_long %&gt;%\n  pivot_wider(names_from = \"attribute\",\n              values_from = \"mean_score\")\n\n\nggradar(coffee_summary_wide)\n\nIgnoring unknown labels:\n‚Ä¢ size : \"14\"\n\n\n\n\n\n\n\n\n\nWe are going to fix our labels and chanage some parameters on the plot to make it look nicer. You can also do this with coding (instead of manually) using the function to_any_case() in the package snakecase.\n\n# write code to get nicer looking label names\n# create a vector of our variable names\n\n# create a vector of the column names\nvariables &lt;- colnames(coffee_summary_wide)\n\n# remove the first observation (species) since we don't want that one\nvariables &lt;- variables[-1]\n\n# use the function to_any_case() from the package snakecase\n# to convert to \"sentence\" case\n# install.packages(\"snakecase\")\ncoffee_labels &lt;- snakecase::to_any_case(variables, case = \"sentence\")\n\n# how do they look?\ncoffee_labels\n\n [1] \"Acidity\"       \"Aftertaste\"    \"Aroma\"         \"Balance\"      \n [5] \"Body\"          \"Clean cup\"     \"Cupper points\" \"Flavor\"       \n [9] \"Sweetness\"     \"Uniformity\"   \n\n\nOr to do this manually\n\n# set our pretty coffee labels\n# ggradar plots in alphabetical order so that is how we will label here\ncoffee_labels &lt;- c(\"Acidity\",\n                   \"Aftertaste\",\n                   \"Aroma\",\n                   \"Balance\",\n                   \"Body\",\n                   \"Clean cup\",\n                   \"Cupper points\",\n                   \"Flavor\",\n                   \"Sweetness\",\n                   \"Uniformity\")\n\n\nggradar(coffee_summary_wide,\n        axis.labels = coffee_labels,\n        legend.position = \"bottom\",\n        axis.label.size = 3,\n        grid.label.size = 5) +\n  theme(legend.key = element_rect(fill = NA, color = NA),\n        plot.title = element_text(size = 16),\n        legend.text = element_text(size = 12)) +\n  labs(title = \"Difference in average coffee cupper score \\nin Arabica and Robusta beans\")\n\nIgnoring unknown labels:\n‚Ä¢ size : \"14\"",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#heatmaps",
    "href": "modules/module4/13_leftovers/13_leftovers.html#heatmaps",
    "title": "Leftover tidbits",
    "section": "Heatmaps üü•‚¨úÔ∏èüü¶",
    "text": "Heatmaps üü•‚¨úÔ∏èüü¶\n\nInstall\n\ninstall.packages(\"pheatmap\")\n\n\n\nLoad libraries\n\nlibrary(pheatmap)\n\n\n\nPlot\n\npheatmap(mtcars)\n\n\n\n\n\n\n\n\n\npheatmap(mtcars, \n         scale = \"column\",\n         cluster_rows = TRUE) # cluster rows based on similarity\n\n\n\n\n\n\n\n\n\n\nConplexHeatmap\nThe package ComplexHeatmap allows more customized and complicated heatmaps to be produced. If you are interested in making heatmaps, this package is worth to check out.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers.html#useful-resources",
    "href": "modules/module4/13_leftovers/13_leftovers.html#useful-resources",
    "title": "Leftover tidbits",
    "section": "Useful resources",
    "text": "Useful resources\n\ngghighlight\npatchwork\ngganimate\nggradar\npheatmap\nComplexHeatmap",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html",
    "href": "modules/module4/12_manhattan/12_manhattan.html",
    "title": "Manhattan Plots",
    "section": "",
    "text": "Today we are going to continue putting it together in Module 4. Today‚Äôs material is on making Manhattan plots, which is a commonly used plot type for visualizing the result of genome wide association studies (GWAS). The name comes from its resemblance to the skyscrapers in Manhattan, poking above the background of the rest of the buildings.\n\n\n\n\n\nFigure from Wikipedia\n\n\n\n\nThe plot visualizes the relationship between a trait and genetic markers. The x-axis shows the position on each chromosome, and the y-axis shows the negative log (usually log10) p-value of the quantitative response of a trait to that specific marker. Negative log10 p-value is used because a significant p-value is always small, and this transformation converts low p-value to a number that can be seen easily among the background of non-significant associations.\nIf you work in genetics/genomics, it is likely you will create Manhattan plots. Even if you think you‚Äôll never make one of these types of plots, its a useful activity to see additional ways of customizing your plots.\n\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels\nlibrary(tidyverse) # everything\n\n\n\nToday we are going to continue to use some different real research data collected by Emma Bilbrey from my team where we conducted many GWAS in apple. This work was published in 2021 in New Phytologist and can be found here. Click on Sections and go to Supporting Information and you‚Äôll find Table S16. This data is more complex than a typical GWAS so we are only going to use a small portion of it.\nWe will be reading in Table S16 which includes the -log10 p-values for the GWAS conducted across all apples for all features found in the LC-MS negative ionization mode metabolomics dataset.\nThe data is present in a .csv file, so we will use the function read_csv() from the tidyverse. We want to import Supplemental Table 16.\nThis will take a second, its a big file.\n\ngwas &lt;- read_csv(\"data/nph17693-sup-0007-tables16.csv\") # be patient\n\n\n\nRows: 11165 Columns: 4704\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4704): Index, Linkage_Group, Genetic_Distance, X885.2037_2.98177, X525....\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat are the dimensions of this dataframe? What kind of object is it?\n\ndim(gwas)\n\n[1] 11165  4704\n\nclass(gwas)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nBecause this dataframe is so big, if we use head(gwas) we will get a print out of the first 6 rows, and all the columns. In thi case there are 4704 columns so that will be unwieldy.\nEmma came up with a simple way to approach this when she was writing her code, she wrote herself a little function that she could use regularly to extract out the first 5 rows, and the first 5 columns, without having to index each time.\nIf we wanted to just see the first 5 rows, the first 5 columns we could do this:\n\ngwas[1:5,1:5]\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\nhead_short &lt;- function(x){\n  x[1:5,1:5] # print first 5 rows and columns of an object\n  } \n\nNow instead of indexing all the time, we can just run head_short() which I think is easier. We will talk a little bit more about writing functions later today.\n\nhead_short(gwas)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\n\n\nLet me provide you a little bit of info about this data:\n\nIndex provides a unique number identifier for each SNP\nLinkage_Group indicates the chromosome\nGenetic_Distance gives you the physical genetic distance on each chromosome\nThe rest of the columns represent a metabolomic feature, and the data in each cell represents the -log10 p-values for the relationship between that SNP and that feature in the GWAS.\n\nWe will write a little code to see this better.\nHow many markers are included here?\n\nnrow(gwas)\n\n[1] 11165\n\n\nHow many linkage groups do we have? (Each linkage group is a chromosome.)\n\nunique(gwas$Linkage_Group)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n\n\nWhat is the range of Genetic_Distance and Index for each chromosome?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  dplyr::summarize(min_genetic_distance = min(Genetic_Distance),\n            max_genetic_distance = max(Genetic_Distance),\n            min_index = min(Index),\n            max_index = max(Index))\n\n# A tibble: 17 √ó 5\n   Linkage_Group min_genetic_distance max_genetic_distance min_index max_index\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1             1                0                     63.1         1       663\n 2             2                0                     78.4       664      1687\n 3             3                0                     74.0      1688      2630\n 4             4                0.004                 65.5      2635      3432\n 5             5                0                     77.8      3433      4530\n 6             6                0                     75.3      4533      5266\n 7             7                0.001                 82.4      5270      5934\n 8             8                0                     68.5      5936      6792\n 9             9                0.292                 67.0      6793      7623\n10            10                0                     81.3      7624      8648\n11            11                0                     80.9      8652      9728\n12            12                0.382                 65.4      9731     10719\n13            13                0                     71.4     10721     11558\n14            14                0                     64.4     11560     12365\n15            15                0                    112.      12366     13605\n16            16                0.001                 67.5     13610     14428\n17            17                0                     71.8     14431     15260\n\n\nOk here we can see Index does not repeat, but Genetic_Distance restarts with each chromosome.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#introduction",
    "href": "modules/module4/12_manhattan/12_manhattan.html#introduction",
    "title": "Manhattan Plots",
    "section": "",
    "text": "Today we are going to continue putting it together in Module 4. Today‚Äôs material is on making Manhattan plots, which is a commonly used plot type for visualizing the result of genome wide association studies (GWAS). The name comes from its resemblance to the skyscrapers in Manhattan, poking above the background of the rest of the buildings.\n\n\n\n\n\nFigure from Wikipedia\n\n\n\n\nThe plot visualizes the relationship between a trait and genetic markers. The x-axis shows the position on each chromosome, and the y-axis shows the negative log (usually log10) p-value of the quantitative response of a trait to that specific marker. Negative log10 p-value is used because a significant p-value is always small, and this transformation converts low p-value to a number that can be seen easily among the background of non-significant associations.\nIf you work in genetics/genomics, it is likely you will create Manhattan plots. Even if you think you‚Äôll never make one of these types of plots, its a useful activity to see additional ways of customizing your plots.\n\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels\nlibrary(tidyverse) # everything\n\n\n\nToday we are going to continue to use some different real research data collected by Emma Bilbrey from my team where we conducted many GWAS in apple. This work was published in 2021 in New Phytologist and can be found here. Click on Sections and go to Supporting Information and you‚Äôll find Table S16. This data is more complex than a typical GWAS so we are only going to use a small portion of it.\nWe will be reading in Table S16 which includes the -log10 p-values for the GWAS conducted across all apples for all features found in the LC-MS negative ionization mode metabolomics dataset.\nThe data is present in a .csv file, so we will use the function read_csv() from the tidyverse. We want to import Supplemental Table 16.\nThis will take a second, its a big file.\n\ngwas &lt;- read_csv(\"data/nph17693-sup-0007-tables16.csv\") # be patient\n\n\n\nRows: 11165 Columns: 4704\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4704): Index, Linkage_Group, Genetic_Distance, X885.2037_2.98177, X525....\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat are the dimensions of this dataframe? What kind of object is it?\n\ndim(gwas)\n\n[1] 11165  4704\n\nclass(gwas)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nBecause this dataframe is so big, if we use head(gwas) we will get a print out of the first 6 rows, and all the columns. In thi case there are 4704 columns so that will be unwieldy.\nEmma came up with a simple way to approach this when she was writing her code, she wrote herself a little function that she could use regularly to extract out the first 5 rows, and the first 5 columns, without having to index each time.\nIf we wanted to just see the first 5 rows, the first 5 columns we could do this:\n\ngwas[1:5,1:5]\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\nhead_short &lt;- function(x){\n  x[1:5,1:5] # print first 5 rows and columns of an object\n  } \n\nNow instead of indexing all the time, we can just run head_short() which I think is easier. We will talk a little bit more about writing functions later today.\n\nhead_short(gwas)\n\n# A tibble: 5 √ó 5\n  Index Linkage_Group Genetic_Distance X885.2037_2.98177 X525.1583_3.24969\n  &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1             1            0                0.176             0.117 \n2     3             1            0.002            0.0978            0.0166\n3     4             1            0.003            0.169             0.0519\n4     5             1            0.004            0.217             0.309 \n5     6             1            0.005            0.0548            0.110 \n\n\n\n\n\nLet me provide you a little bit of info about this data:\n\nIndex provides a unique number identifier for each SNP\nLinkage_Group indicates the chromosome\nGenetic_Distance gives you the physical genetic distance on each chromosome\nThe rest of the columns represent a metabolomic feature, and the data in each cell represents the -log10 p-values for the relationship between that SNP and that feature in the GWAS.\n\nWe will write a little code to see this better.\nHow many markers are included here?\n\nnrow(gwas)\n\n[1] 11165\n\n\nHow many linkage groups do we have? (Each linkage group is a chromosome.)\n\nunique(gwas$Linkage_Group)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n\n\nWhat is the range of Genetic_Distance and Index for each chromosome?\n\ngwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  dplyr::summarize(min_genetic_distance = min(Genetic_Distance),\n            max_genetic_distance = max(Genetic_Distance),\n            min_index = min(Index),\n            max_index = max(Index))\n\n# A tibble: 17 √ó 5\n   Linkage_Group min_genetic_distance max_genetic_distance min_index max_index\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1             1                0                     63.1         1       663\n 2             2                0                     78.4       664      1687\n 3             3                0                     74.0      1688      2630\n 4             4                0.004                 65.5      2635      3432\n 5             5                0                     77.8      3433      4530\n 6             6                0                     75.3      4533      5266\n 7             7                0.001                 82.4      5270      5934\n 8             8                0                     68.5      5936      6792\n 9             9                0.292                 67.0      6793      7623\n10            10                0                     81.3      7624      8648\n11            11                0                     80.9      8652      9728\n12            12                0.382                 65.4      9731     10719\n13            13                0                     71.4     10721     11558\n14            14                0                     64.4     11560     12365\n15            15                0                    112.      12366     13605\n16            16                0.001                 67.5     13610     14428\n17            17                0                     71.8     14431     15260\n\n\nOk here we can see Index does not repeat, but Genetic_Distance restarts with each chromosome.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#manhattan-plot-chlorogenic-acid",
    "href": "modules/module4/12_manhattan/12_manhattan.html#manhattan-plot-chlorogenic-acid",
    "title": "Manhattan Plots",
    "section": "Manhattan plot: chlorogenic acid",
    "text": "Manhattan plot: chlorogenic acid\nAt its core, a Manhattan plot is a scatter plot. The data we are working with has 4701 traits, which here are relative metabolite abundance. We are going to pick one metabolite to start working with.\nWe will start with the feature that represents chlorogenic acid, a caffeoyl-quinic acid you find in apples. The column we want is X353.09194_2.23795. The data is already present as the -log10 p-value for the relationship between allelic variation at that marker, and relative abundance of chlorogenic acid.\n\n# rename X353.09194_2.23795 to chlorogenic_acid\ngwas &lt;- gwas %&gt;%\n  rename(chlorogenic_acid = `X353.09194_2.23795`)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point()\n\n\n\n\n\n\n\n\nSee how color is plotted on a continuous scale? This is because Linkage_Group is a continuous, numeric variable. Since each chromosome is actually discrete, let‚Äôs convert Linkage_Group to a factor and then plot again.\n\nLinkage_Group as a factor\n\n# make Linkage_Group a factor\ngwas$Linkage_Group &lt;- as.factor(gwas$Linkage_Group)\n\n# first pass manhattan plot\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point()\n\n\n\n\n\n\n\n\nBetter but this really isn‚Äôt what we want. We want our x-axis to indicate the chromosome number in the middle of the block of that chromosome, not label by Index which just is a key for linking back to each specific marker.\n\n\nSet axis\nIf we want to label the x-axis with breaks for each chromosome, we have to do some wrangling first. Just like we did some calculations in the lesson on adding statistics, we will calculate some min, center, and max for each chromosome so we know where to put the labels.\n\n(set_axis &lt;- gwas %&gt;%\n  group_by(Linkage_Group) %&gt;%\n  dplyr::summarize(min = min(Index),\n            max = max(Index),\n            center = (max - min)/2))\n\n# A tibble: 17 √ó 4\n   Linkage_Group   min   max center\n   &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 1                 1   663   331 \n 2 2               664  1687   512.\n 3 3              1688  2630   471 \n 4 4              2635  3432   398.\n 5 5              3433  4530   548.\n 6 6              4533  5266   366.\n 7 7              5270  5934   332 \n 8 8              5936  6792   428 \n 9 9              6793  7623   415 \n10 10             7624  8648   512 \n11 11             8652  9728   538 \n12 12             9731 10719   494 \n13 13            10721 11558   418.\n14 14            11560 12365   402.\n15 15            12366 13605   620.\n16 16            13610 14428   409 \n17 17            14431 15260   414.\n\n\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n# set breaks and labels using set_axis  \n  scale_x_continuous(breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  theme_classic() +\n  theme(legend.position = \"none\") + # legend not really necessary\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"GWAS of chlorogenic acid in apple\")\n\n\n\n\n\n\n\n\n\n\nAlternate colors\nHaving a rainbow of colors is not really necessary here, and in fact telling exactly where chromosome 15 ends and 16 begins is difficult because the colors are so similar.\nWhat you will see in a lot of papers is people simply alternate the colors of their points by chromosome so you can easily tell which points belong to which chromosome.\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n# alternating colors by chromosome, black and darkgray\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\n\n\n\n\nRemoving that annoying front gap\nThe gap between chromosome 1 and the y-axis of the plot sort of bothers me. Let‚Äôs remove it.\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  scale_x_continuous(expand = c(0,0), # remove gap between y-axis and chr1\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"grey52\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\n\n\n\n\nAdd p-value hline\nBecause we have so many SNPs, we are making a lot of comparisons that require a multiple-testing correction or we run the risk of an enormous number of false-positives. We can use the conservative Bonferroni correction, which takes our p-value, and the total number of comparisons we are making, creating a new adjusted value that our p-values need to be less than to be considered significant. In this case, the number of comparisons are our number of rows, in this case 11165\n\n# what would the pvalue cut off with a bonferroni correction be?\nbonferroni_pval &lt;- -log10(0.05/nrow(gwas))\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\n\n\n\n\nColor sig points\nWe might want to better see the points (i.e., SNPs) that are significantly related to chlorogenic acid content by coloring them differently from the other points. Think about how you could also use this in volcano plots, or any other time you want to highlight some points on a plot.\n\n# select all SNPs with -log10 pvalue &gt; bonferroni cutoff for chlorogenic acid\nchlorogenic_acid_sig &lt;- gwas %&gt;%\n  filter(chlorogenic_acid &gt; bonferroni_pval) %&gt;%\n  select(Index, Linkage_Group, Genetic_Distance, chlorogenic_acid)\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n# another geom_point layer with only the sig points and make them red\n  geom_point(data = chlorogenic_acid_sig, \n             aes(x = Index, y = chlorogenic_acid), color = \"red\") +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")\n\n\n\n\n\n\n\n\n\n\nLabel most sig marker\nWe might be interested to know the marker that has the most significant association with chlorogenic acid content, and label it on our plot.\n\n# select which SNP has the smallest pvalue.\nsmallest_pval &lt;- chlorogenic_acid_sig %&gt;% \n  filter(chlorogenic_acid == max(chlorogenic_acid))\n\ngwas %&gt;%\n  ggplot(aes(x = Index, y = chlorogenic_acid, color = Linkage_Group)) +\n  geom_point() +\n  geom_point(data = chlorogenic_acid_sig, \n             aes(x = Index, y = chlorogenic_acid), color = \"red\") +\n  geom_label_repel(data = smallest_pval,\n                   aes(x = Index, y = chlorogenic_acid, label = glue(\"Index: {Index}\"))) +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for Chlorogenic Acid in Apple\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#investigating-other-traits",
    "href": "modules/module4/12_manhattan/12_manhattan.html#investigating-other-traits",
    "title": "Manhattan Plots",
    "section": "Investigating other traits",
    "text": "Investigating other traits\nIn this study, we conducted a series of GWAS on thousands of metabolomic features in apple. What if we wanted to see Manhattan plots for certain features based on how important we could predict they would be? For example, what if we want to see the Manhattan plot for the feature with biggest -log10p-value? Or the feature that has a significant association with the largest number of markers?\nTo make this wrangling easier, we will convert our data, as we have many times before, from wide to long with pivot_longer().\n\nWide to long (again)\n\n# pivoting, our favorite\ngwas_tidy &lt;- gwas %&gt;%\n  pivot_longer(cols = starts_with(\"X\"), # all the metabolomic features\n               names_to = \"feature\",\n               values_to = \"neg_log10_p\")\n\n# how did it go?\nhead(gwas_tidy)\n\n# A tibble: 6 √ó 6\n  Index Linkage_Group Genetic_Distance chlorogenic_acid feature      neg_log10_p\n  &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n1     1 1                            0            0.361 X885.2037_2‚Ä¶       0.176\n2     1 1                            0            0.361 X525.1583_3‚Ä¶       0.117\n3     1 1                            0            0.361 X569.17408_‚Ä¶       0.252\n4     1 1                            0            0.361 X739.17477_‚Ä¶       0.250\n5     1 1                            0            0.361 X600.12641_‚Ä¶       0.323\n6     1 1                            0            0.361 X349.0664_1‚Ä¶       0.283\n\n\n\n\nSet p-value cutoff\nWe can make another df that includes only the features that have at least one marker where there is a significant p-value.\n\n# make df of associations that pass bonferroni correction\ngwas_tidy_bonferroni &lt;- gwas_tidy %&gt;%\n  filter(neg_log10_p &gt; bonferroni_pval)\n\n# how many unique features are there?\nlength(unique(gwas_tidy_bonferroni$feature))\n\n[1] 962\n\n# how many unique markers are there?\nlength(unique(gwas_tidy_bonferroni$Index))\n\n[1] 544\n\n\nThere are 962 unique features/metabolite that have a Bonferroni adjusted significant p-value with at least one marker. There are 544 unique markers that have a Bonferroni adjusted significant p-value with at least one feature/metabolite.\n\n\nData investigating\nWhat features are associated with the largest number of markers?\n\ngwas_tidy_bonferroni %&gt;%\n  group_by(feature) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 962 √ó 2\n# Groups:   feature [962]\n   feature                n\n   &lt;chr&gt;              &lt;int&gt;\n 1 X417.13237_1.82968    46\n 2 X349.15073_1.79191    44\n 3 X601.13217_2.40546    34\n 4 X593.12835_2.53465    31\n 5 X291.0768_2.44657     30\n 6 X591.1485_2.86273     30\n 7 X637.09169_2.78692    30\n 8 X661.08791_2.10005    30\n 9 X137.02484_2.44808    29\n10 X561.13983_2.53357    29\n# ‚Ñπ 952 more rows\n\n\nWow, the marker X417.13237_1.82968 has significant associations with 46 markers. What would that Manhattan plot look like?\n\ngwas_tidy %&gt;%\n  filter(feature == \"X417.13237_1.82968\") %&gt;%\n  ggplot(aes(x = Index, y = neg_log10_p, color = Linkage_Group)) +\n  geom_point() +\n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"darkgray\"), 17)) +\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5)) + \n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = \"Manhattan Plot after GWAS for 417.13237 m/z at retention time 1.82968 in Apple\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#making-many-plots-at-once",
    "href": "modules/module4/12_manhattan/12_manhattan.html#making-many-plots-at-once",
    "title": "Manhattan Plots",
    "section": "Making many plots at once",
    "text": "Making many plots at once\nWhat if we want to make Manhattan plots for the 50 features/metabolites that are associated with the most markers? This is probably too many plots to facet, so we can do some calculations, write a function to make plots, and apply it over our dataframe.\nFirst, how many significant associations with a Bonferroni multiple testing correction are there?\n\n# make df of associations that pass bonferroni correction\ngwas_tidy_bonferroni &lt;- gwas_tidy %&gt;%\n  filter(neg_log10_p &gt; bonferroni_pval)\n\n# how many unique features are this?\ngwas_tidy_bonferroni %&gt;%\n  count(feature) \n\n# A tibble: 962 √ó 2\n   feature                 n\n   &lt;chr&gt;               &lt;int&gt;\n 1 X1000.22158_2.71331    12\n 2 X1000.72569_2.72017     2\n 3 X1001.23392_2.70506     5\n 4 X1008.71962_2.91507     3\n 5 X1008.72084_2.64352     2\n 6 X1009.22592_2.91262     5\n 7 X1009.72353_2.64479     2\n 8 X1010.22369_2.64604     2\n 9 X1010.72865_2.64538     1\n10 X1014.70219_2.12784     2\n# ‚Ñπ 952 more rows\n\n# how many unique markers are there?\ngwas_tidy_bonferroni %&gt;%\n  count(Index) \n\n# A tibble: 544 √ó 2\n   Index     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   170     1\n 2   217     1\n 3   218     1\n 4   233     2\n 5   294     1\n 6   311     1\n 7   341     2\n 8   368     1\n 9   386     4\n10   520     6\n# ‚Ñπ 534 more rows\n\n\nWhich features are associated with the largest number of markers?\n\ngwas_tidy_bonferroni %&gt;%\n  count(feature) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 962 √ó 2\n   feature                n\n   &lt;chr&gt;              &lt;int&gt;\n 1 X417.13237_1.82968    46\n 2 X349.15073_1.79191    44\n 3 X601.13217_2.40546    34\n 4 X593.12835_2.53465    31\n 5 X291.0768_2.44657     30\n 6 X591.1485_2.86273     30\n 7 X637.09169_2.78692    30\n 8 X661.08791_2.10005    30\n 9 X137.02484_2.44808    29\n10 X561.13983_2.53357    29\n# ‚Ñπ 952 more rows\n\n\nWhich markers are associated with the largest number of features?\n\ngwas_tidy_bonferroni %&gt;%\n  count(Index) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 544 √ó 2\n   Index     n\n   &lt;dbl&gt; &lt;int&gt;\n 1 13684   320\n 2 13685   318\n 3 13681   317\n 4 13715   298\n 5 13657   223\n 6 13660   223\n 7 13675   221\n 8 13630   219\n 9 13623   199\n10 13617   198\n# ‚Ñπ 534 more rows\n\n\nWe will make a new df that includes only the 50 features with the most makers associated with them.\n\n# create a df with only the top 50 features with the most marker associations\ntop50 &lt;- gwas_tidy_bonferroni %&gt;%\n  count(feature) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 50)\n\n# what does that look like?\nhead(top50)\n\n# A tibble: 6 √ó 2\n  feature                n\n  &lt;chr&gt;              &lt;int&gt;\n1 X417.13237_1.82968    46\n2 X349.15073_1.79191    44\n3 X601.13217_2.40546    34\n4 X593.12835_2.53465    31\n5 X291.0768_2.44657     30\n6 X591.1485_2.86273     30\n\n\nNow we can filter our dataset to only include our top 50 features\n\n# filter the whole dataset to include only top 50 features\ngwas_top50_long &lt;- gwas_tidy %&gt;%\n  filter(feature %in% top50$feature)\n\n\nWriting a function to plot\nThen we can write a function to plot, where we will iterate across feature_of_interest. Here, feature_of_interest is just the name I‚Äôve assigned here, but you could easily call it x or i or whatever.\n\n# write a function to make your plots across the features of interest\nmanhattan_plot &lt;- function(feature_of_interest){\n  gwas_top50_long %&gt;% # our df with only the top 50, but long\n  filter(feature == feature_of_interest) %&gt;% # pick the feature_of_interest only\n  ggplot(aes(x = Index, y = neg_log10_p, color = Linkage_Group)) +\n  geom_point() + \n  geom_hline(yintercept = bonferroni_pval, color = \"grey\", linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = (set_axis$center + set_axis$min), \n                     labels = set_axis$Linkage_Group) +\n  scale_color_manual(values = rep(c(\"black\", \"gray\"),17)) +\n  labs(x = \"Chromosome\",\n       y = expression(\"-log\"[10]*\"P-Value\"),\n       title = glue(\"{feature_of_interest}\")) + # here we glue the feature name in the title\n  theme_classic() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5))\n  }\n\nBefore trying to use our new function manhattan_plot on 50 features, let‚Äôs try it out on one. We can provide our feature of interest as a string.\n\nmanhattan_plot(\"X599.12186_2.10421\")\n\n\n\n\n\n\n\n\nNow that our function is working for one feature, let‚Äôs create a vector of features to iterate over, and plot. I am calling that vector features_to_plot. It contains the unique feature names from gwas_top50_long.\n\nfeatures_to_plot &lt;- unique(gwas_top50_long$feature)\n\n\n\nApplying the function with map().\nOnce we have our function written, we can apply that function over a vector using purrr:map(). What map() does is take a function and apply it to a vector. In this case, the function is manhattan_plot (the function that makes a Manhattan plot), and it is applied over features_to_plot, the vector of features we want a plot for.\n\n# create an object called my_plots which is a list \n# the list contains our 50 plots\nmy_plots &lt;- map(features_to_plot, # vector to apply over\n            manhattan_plot) # what function to use\n\n# print the first 6\nmy_plots[1:6]\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n\nSaving out plots\nBut you can print them all, save particular ones using ggsave(), or do what we are going to do here, which is save each of them to a new folder, each as their own .svg because why use raster when you can vectorize.\nFirst we will create a vector of what we want our file names to look like, and apply them to be the names of our plots.\n\n# use str_c to combine two character vectors\n# here, features_to_plot and adding .svg so the file name \n# includes the extension type\n# then set that as the names for my_plots\nnames(my_plots) &lt;- str_c(features_to_plot, \".svg\")\n\nThen, we will save.\n\n# use pwalk to \"walk\" across the different plots and save them\n# to use the path subfolder \"img\" you need to create that in your working dir\npwalk(list(names(my_plots), my_plots), # what to iterate over and output\n      ggsave, # what the function is\n      path = \"img/\") # where they should go\n\nNow all of your plots are in your working directory. Remember, you need to add the directory img if you want to save with the code I‚Äôm using here.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan.html#useful-resources",
    "href": "modules/module4/12_manhattan/12_manhattan.html#useful-resources",
    "title": "Manhattan Plots",
    "section": "Useful resources",
    "text": "Useful resources\n\npurrr:map()",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "rmds/03_Rmd_recitation.html",
    "href": "rmds/03_Rmd_recitation.html",
    "title": "RMarkdown Recitation",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation.html#r-markdown-recitation",
    "href": "rmds/03_Rmd_recitation.html#r-markdown-recitation",
    "title": "RMarkdown Recitation",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HCS 7100: Data Visualization in R, Autumn 2025",
    "section": "",
    "text": "Instructor: Jessica Cooperstone  Teaching assistant: Daniel Quiroz Moreno\n\nThis course aims to introduce students to the principles and practice of data visualization. Students will learn fundamental principles of data visualization and create figures that appropriately and ethically represent their data. Data visualizations will be created in the R programming environment, using tools including the grammar of graphics implemented in ggplot2. In the process of creating visualizations, students will also become familiar with data handling and wrangling in R.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html",
    "href": "assignments/modules/module2/module_2_solutions.html",
    "title": "Module 2 Assignment Solutions",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on RMarkdown, wrangling, ggplot101, and ggplot102.\nYou will submit this assignment by uploading a knitted .html to Carmen. Make sure you include the Code Download button, and please show your code within your knitted .html as well. Customize the YAML and the document so you like how it looks.\nRemember there are often many ways to reach the same end product.\n\nThis assignment will be due on Tuesday, October 1, 2024, at 11:59pm.\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded.",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#introduction",
    "href": "assignments/modules/module2/module_2_solutions.html#introduction",
    "title": "Module 2 Assignment Solutions",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on RMarkdown, wrangling, ggplot101, and ggplot102.\nYou will submit this assignment by uploading a knitted .html to Carmen. Make sure you include the Code Download button, and please show your code within your knitted .html as well. Customize the YAML and the document so you like how it looks.\nRemember there are often many ways to reach the same end product.\n\nThis assignment will be due on Tuesday, October 1, 2024, at 11:59pm.\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded.",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#including-the-code-download-button",
    "href": "assignments/modules/module2/module_2_solutions.html#including-the-code-download-button",
    "title": "Module 2 Assignment Solutions",
    "section": "Including the code download button",
    "text": "Including the code download button\nTo get a code download button, you need to indicate that you want one in your YAML. You do this by setting code_download: true in your YAML. I‚Äôm clipping a YAML below which could be copied and edited.\n\n---\ntitle: \"Module 2 assignment\"\nauthor: \"Jessica Cooperstone\"\ndate: \"October 1, 2024\"\noutput: \n  html_document: # knit to a .html doc\n    toc: true # creates a table of contents\n    toc_float: true # has that TOC float so you can see it even when you scroll\n    number_sections: true # numbers your sections\n    theme: flatly # set a global theme, this is what i use for this site\n    code_download: true # insert the code download button\n---",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#writing-in-markdown-1",
    "href": "assignments/modules/module2/module_2_solutions.html#writing-in-markdown-1",
    "title": "Module 2 Assignment Solutions",
    "section": "Writing in Markdown 1",
    "text": "Writing in Markdown 1\nUsing coding in text, write a sentence in markdown that pulls from this data how many total PhDs were awarded in 2017. If you want to make some calculations in a code chunk first that is ok.\n\nlibrary(tidyverse)\nlibrary(scales) # for using comma format \n\nSetting as an object the number for the total Ph.D.s earned in 2017.\n\nphds_2017 &lt;- phd_field |&gt;\n  filter(year == 2017) |&gt;\n  select(n_phds) |&gt;\n  colSums(na.rm = TRUE) # calculate a column sum\n\nYou could also do it this way:\n\nphd_field |&gt;\n  filter(year == 2017) |&gt;\n  summarize(total_phds = sum(n_phds, # sum n_phds\n                             na.rm = TRUE)) # remove missing values\n\n# A tibble: 1 √ó 1\n  total_phds\n       &lt;dbl&gt;\n1      54110\n\n\nHow to write in Markdown:\nIn 2017, there were were `r format(phds_2017, scientific = F, big.mark = \",\")` Ph.D.¬†degrees awarded in the United States.\nIn 2017, there were were `r phd_field |&gt; filter(year == 2017) |&gt; select(n_phds) |&gt; colSums(na.rm = TRUE) |&gt; format(scientific = F, big.mark = \",\")` Ph.D.¬†degrees awarded in the United States.\nHow this will look when rendered to html:\nIn 2017, there were 54,110 Ph.D.¬†degrees awarded in the United States.\nIn 2017, there were 54,110 Ph.D.¬†degrees awarded in the United States.",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#visualization-1",
    "href": "assignments/modules/module2/module_2_solutions.html#visualization-1",
    "title": "Module 2 Assignment Solutions",
    "section": "Visualization 1",
    "text": "Visualization 1\nMake a chart to visualize of the total number of PhDs awarded for each broad_field across the total time period of this data. You pick the type of chart that you think is appropriate, and make sure your plot is appropriately labelled and you are happy with how it looks. Hint, to do this you‚Äôll probably have to do some data wrangling first.\nFirst I will calculate the total number of Ph.D.s awarded across each broad_field for the whole time period.\n\nbroad_field_sum &lt;- phd_field |&gt;\n  group_by(broad_field) |&gt;\n  summarize(broad_field_sum = sum(n_phds, na.rm = TRUE)) |&gt;\n  arrange(-broad_field_sum)\n\nbroad_field_sum\n\n# A tibble: 7 √ó 2\n  broad_field                       broad_field_sum\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 Life sciences                              205703\n2 Psychology and social sciences             119116\n3 Humanities and arts                         53045\n4 Education                                   52639\n5 Mathematics and computer sciences           35481\n6 Other                                       28855\n7 Engineering                                 18139\n\n\nThis summary helps me know what to expect to see in my plots. Now I can create some plots.\nThe first one will just show the total number of Ph.D.s across each broad_field. I decided to put the broad_field on the y-axis so that the labels are easier to read. I‚Äôve also re-ordered broad_field based on the total number of PhDs for each category (i.e., broad_field_sum). I chose to use no colors because I didn‚Äôt feel like it was adding much here.\n\nbroad_field_sum |&gt;\n  ggplot(aes(x = broad_field_sum, y = fct_reorder(broad_field, broad_field_sum))) +\n  geom_col(color = \"black\", fill = \"grey\") +\n  scale_x_continuous(labels = comma) + # add a comma to the x-axis breaks\n  theme_minimal() +\n  labs(x = \"Total number of Ph.D.s\",\n       y = \"\", # no label on the y\n       title = \"Number of PhDs awarded across different \\nbroad disciplines from 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\n\n\n\n\nThe following few examples weren‚Äôt exactly the question (which was to show total PhDs across each broad_field across the total time period of this data), but I‚Äôll show you here how to make some other stuff.\n\nbroad_field_sumonly_eachyear &lt;- phd_field |&gt;\n  group_by(year) |&gt;\n  summarize(all_the_phds = sum(n_phds, na.rm = TRUE)) \n\nbroad_field_sumonly_eachyear\n\n# A tibble: 10 √ó 2\n    year all_the_phds\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1  2008        48026\n 2  2009        49141\n 3  2010        47628\n 4  2011        48546\n 5  2012        50777\n 6  2013        52370\n 7  2014        53364\n 8  2015        54154\n 9  2016        54862\n10  2017        54110\n\n\nThen we can plot.\n\nbroad_field_sumonly_eachyear |&gt;\n  ggplot(aes(x = year, y = all_the_phds)) +\n  geom_col(color = \"black\", fill = \"grey\") +\n  scale_y_continuous(labels = comma) + # add a comma to the x-axis breaks\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Number of Ph.Ds\", \n       title = \"Total number of Ph.D.s awarded in the United States per year from 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\n\n\n\n\nThis wasn‚Äôt exactly the question, but if we want to see a little bit better this over the time period, first we need to create a df that has total PhDs per broad field per year. We can do this by adding year to our group_by() statement.\n\nbroad_field_sum_byyear &lt;- phd_field |&gt;\n  group_by(broad_field, year) |&gt;\n  summarize(total_phds = sum(n_phds, na.rm = TRUE)) \n\n`summarise()` has grouped output by 'broad_field'. You can override using the\n`.groups` argument.\n\n# what does that look like\nhead(broad_field_sum_byyear)\n\n# A tibble: 6 √ó 3\n# Groups:   broad_field [1]\n  broad_field  year total_phds\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Education    2008       6561\n2 Education    2009       6528\n3 Education    2010       5287\n4 Education    2011       4670\n5 Education    2012       4803\n6 Education    2013       4934\n\n\nNow we can plot. We can now make different types of plots. Let‚Äôs start simpler. Here I‚Äôve re-ordered the legend to be in the same order as the chart so you can see the disciplines that award the most Ph.D.s just by looking at the order of the legend.\n\nbroad_field_sum_byyear |&gt;\n  ggplot(aes(x = year, y = total_phds, \n    # reorder broad_field by descending total_phds         \n             color = reorder(broad_field, -total_phds))) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2008, 2017, 2)) +\n  scale_y_continuous(labels = comma) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Total number of Ph.D.s\",\n       color = \"Broad Field\",\n       title = \"Number of Ph.D.s awarded by discipline in the United States\",\n       subtitle = \"From 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\n\n\n\n\nCould also try facets.\n\n# re-setting factors of broad_field so that it is re-levelled by the discipline with\n# the most phds\n# you could also do this manually\nbroad_field_sum_byyear$broad_field &lt;- fct_reorder(broad_field_sum_byyear$broad_field,\n                                                  -broad_field_sum_byyear$total_phds)\n\nbroad_field_sum_byyear |&gt;\n  ggplot(aes(x = year, y = total_phds, color = broad_field)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2008, 2017, 2)) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(vars(broad_field)) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\",\n       y = \"Total number of Ph.D.s\",\n       color = \"Broad Field\",\n       title = \"Number of Ph.D.s awarded by discipline in the United States\",\n       subtitle = \"From 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")\n\n\n\n\n\n\n\n\nCan also try with ‚Äúfree_y‚Äù axes. You can see the number better but can‚Äôt as easily compare between the disciplines.\n\nbroad_field_sum_byyear |&gt;\n  ggplot(aes(x = year, y = total_phds, color = broad_field)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(2008, 2017, 2)) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(vars(broad_field), scales = \"free_y\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\",\n       y = \"Total number of Ph.D.s\",\n       color = \"Broad Field\",\n       title = \"Number of Ph.D.s awarded by discipline in the United States\",\n       subtitle = \"From 2008-2017\",\n       caption = \"Data collected by the National Science Foundation\")",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#visualization-2",
    "href": "assignments/modules/module2/module_2_solutions.html#visualization-2",
    "title": "Module 2 Assignment Solutions",
    "section": "Visualization 2",
    "text": "Visualization 2\nPick the field that most closely matches the area of your degree. Make a line graph (with points for each datapoint) that shows how the number of PhDs awarded in your field has changed from 2008 to 2017. Make sure your x-axis indicates each year for which you have data, your graph is appropriately labelled, and you think it is aesthetically pleasing.\n\n# what are the options for broad_field?\nphd_field |&gt;\n  select(broad_field) |&gt;\n  unique()\n\n# A tibble: 7 √ó 1\n  broad_field                      \n  &lt;chr&gt;                            \n1 Life sciences                    \n2 Mathematics and computer sciences\n3 Psychology and social sciences   \n4 Engineering                      \n5 Education                        \n6 Humanities and arts              \n7 Other                            \n\n# what are the options for life sciences?\nphd_field |&gt;\n  filter(broad_field == \"Life sciences\") |&gt;\n  select(major_field) |&gt;\n  unique()\n\n# A tibble: 6 √ó 1\n  major_field                                          \n  &lt;chr&gt;                                                \n1 Agricultural sciences and natural resources          \n2 Biological and biomedical sciences                   \n3 Health sciences                                      \n4 Chemistry                                            \n5 Geosciences, atmospheric sciences, and ocean sciences\n6 Physics and astronomy                                \n\n# what are the options for Agricultural sciences and natural resources?\nphd_field |&gt;\n  filter(major_field == \"Agricultural sciences and natural resources\") |&gt;\n  select(field) |&gt;\n  unique()\n\n# A tibble: 26 √ó 1\n   field                                        \n   &lt;chr&gt;                                        \n 1 Agricultural economics                       \n 2 Agricultural and horticultural plant breeding\n 3 Agricultural animal breeding                 \n 4 Agronomy and crop science                    \n 5 Animal nutrition                             \n 6 Animal science, poultry or avian             \n 7 Animal sciences, other                       \n 8 Environmental science                        \n 9 Fishing and fisheries sciences and management\n10 Food science                                 \n# ‚Ñπ 16 more rows\n\n\nI‚Äôm picking ‚ÄúHorticulture science‚Äù as my field.\n\nphd_field |&gt;\n  filter(field == \"Horticulture science\") |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Number of Ph.D. Degrees Awarded\",\n       title = \"Number of Ph.D. Degrees Awarded in Horticulture Science in the United States\",\n       subtitle = \"Data collected by the National Science Foundation\")",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_solutions.html#visualization-3",
    "href": "assignments/modules/module2/module_2_solutions.html#visualization-3",
    "title": "Module 2 Assignment Solutions",
    "section": "Visualization 3",
    "text": "Visualization 3\nPick at least 3 additional fields (you can use more if you like) that are adjacent to your Ph.D.¬†field. Make a faceted plot to show the number of degrees awarded in each of these disciplines across the same time period. Make sure you label your plot appropriately and you think it is aesthetic (i.e., if you have squished strip text you want to fix that).\nFirst I am making a vector of the fields in plant science.\n\nplant_sci &lt;- c(\"Agricultural and horticultural plant breeding\",\n               \"Agronomy and crop science\",\n               \"Horticulture science\",\n               \"Plant pathology and phytopathology, agricultural\",\n               \"Plant sciences, other\")\n\nThen I can plot. \\n adds an automatic line break.\n\nphd_field |&gt;\n  filter(field %in% plant_sci) |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  facet_wrap(vars(field), labeller = label_wrap_gen()) + # wraps labels\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Year\",\n       y = \"Number of Ph.D. Degrees Awarded\",\n       title = \"Number of Ph.D. Degrees Awarded in Plant Science Subdisciplines \\nin the United States\",\n       subtitle = \"Data collected by the National Science Foundation\")\n\n\n\n\n\n\n\n\nOr I can make the same style plot but with bars instead of a lines/points.\n\nphd_field |&gt;\n  filter(field %in% plant_sci) |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_col() +\n  scale_x_continuous(breaks = seq(2008, 2017, 1)) +\n  facet_wrap(vars(field), labeller = label_wrap_gen()) + # wraps labels\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Year\",\n       y = \"Number of Ph.D. Degrees Awarded\",\n       title = \"Number of Ph.D. Degrees Awarded in Plant Science Subdisciplines \\nin the United States\",\n       subtitle = \"Data collected by the National Science Foundation\")",
    "crumbs": [
      "Module assignment solutions",
      "Module 2 solutions"
    ]
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html",
    "href": "assignments/modules/module3/module_3_assignment.html",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "This is your assignment for Module 3, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nInclude information about both what you are doing and why you are doing it.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, November 4, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)",
    "crumbs": [
      "Module assignments",
      "Module 3"
    ]
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#introduction",
    "href": "assignments/modules/module3/module_3_assignment.html#introduction",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "This is your assignment for Module 3, focused on the material you learned in the lectures and recitation activities on data distributions, correlations, and annotating statistics.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nInclude information about both what you are doing and why you are doing it.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, November 4, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using was collected by the US Department of Education and collated by tuitiontracker.org. You can learn more about the data by going through the readme here.\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')\n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/salary_potential.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(corrplot)\nlibrary(Hmisc)",
    "crumbs": [
      "Module assignments",
      "Module 3"
    ]
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#data-distributions-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#data-distributions-visualization-3-pts",
    "title": "Module 3 Assignment",
    "section": "1. Data distributions visualization (3 pts)",
    "text": "1. Data distributions visualization (3 pts)\nCreate a visualization that shows the distribution of tuition costs (both in_state_tuition and out_of_state_tuition) across public, private, and for-profit universities and colleges. You can use whatever type of plot you think is appropriate to show this distribution across different types of universities. Your plot should be publication ready quality.",
    "crumbs": [
      "Module assignments",
      "Module 3"
    ]
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#adding-statistics-visualization-3-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#adding-statistics-visualization-3-pts",
    "title": "Module 3 Assignment",
    "section": "2. Adding statistics visualization (3 pts)",
    "text": "2. Adding statistics visualization (3 pts)\nMake a plot that shows the difference in early_career_pay across private and public universities/colleges. Is there any statistical difference in pay across these two categories of institutions? Is the same true for mid_career_pay? This can be either one or two plots, its up to you. Make sure you are doing the right statistical test appropriate for your data.",
    "crumbs": [
      "Module assignments",
      "Module 3"
    ]
  },
  {
    "objectID": "assignments/modules/module3/module_3_assignment.html#understanding-correlations-visualization-2-pts",
    "href": "assignments/modules/module3/module_3_assignment.html#understanding-correlations-visualization-2-pts",
    "title": "Module 3 Assignment",
    "section": "3. Understanding correlations visualization (2 pts)",
    "text": "3. Understanding correlations visualization (2 pts)\nMake a visualization that investigates and then visualizes correlation between early_career_pay, mid_career_pay and university tuition (both in state and out of state) showing correlation coefficients. Show how this is different across public and private universities. If you feel like you want to make a couple plots to display this relationship, that is fine.",
    "crumbs": [
      "Module assignments",
      "Module 3"
    ]
  },
  {
    "objectID": "assignments/recitations.html",
    "href": "assignments/recitations.html",
    "title": "Reflections",
    "section": "",
    "text": "I ask you submit 8 of 11 recitations to Carmen to show you have made a good faith effort to engage with the course material. I will mark these at 0 or 1 points, with 1 point given for completion of at least 70% of the assignment.\nDue Date: Recitations are due the Sunday after each class. For example, if class is on Tuesday September 9, the recitation for that class is due on Sunday September 14 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Recitations",
      "Recitation instructions"
    ]
  },
  {
    "objectID": "assignments/recitations.html#reflections",
    "href": "assignments/recitations.html#reflections",
    "title": "Reflections",
    "section": "",
    "text": "I ask you submit 8 of 11 recitations to Carmen to show you have made a good faith effort to engage with the course material. I will mark these at 0 or 1 points, with 1 point given for completion of at least 70% of the assignment.\nDue Date: Recitations are due the Sunday after each class. For example, if class is on Tuesday September 9, the recitation for that class is due on Sunday September 14 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Recitations",
      "Recitation instructions"
    ]
  },
  {
    "objectID": "assignments/capstone.html",
    "href": "assignments/capstone.html",
    "title": "Capstone assignment",
    "section": "",
    "text": "At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nThere will be 1 capstone assignment.\nDue Dates:\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nCapstone plan\nTuesday, October 28, 2025\n\n\nCapstone\nFriday, December 12, 2025\n\n\n\n\n\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Capstone assignment",
      "Capstone instructions"
    ]
  },
  {
    "objectID": "assignments/capstone.html#capstone-assignment",
    "href": "assignments/capstone.html#capstone-assignment",
    "title": "Capstone assignment",
    "section": "",
    "text": "At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nThere will be 1 capstone assignment.\nDue Dates:\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nCapstone plan\nTuesday, October 28, 2025\n\n\nCapstone\nFriday, December 12, 2025\n\n\n\n\n\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Capstone assignment",
      "Capstone instructions"
    ]
  },
  {
    "objectID": "assignments/reflections.html",
    "href": "assignments/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "After each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday August 20, the reflection for that class is due on Tuesday August 28 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Reflections",
      "Reflection instructions"
    ]
  },
  {
    "objectID": "assignments/reflections.html#reflections",
    "href": "assignments/reflections.html#reflections",
    "title": "Reflections",
    "section": "",
    "text": "After each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday August 20, the reflection for that class is due on Tuesday August 28 by 11:59pm.\n\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Reflections",
      "Reflection instructions"
    ]
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html",
    "href": "assignments/modules/module4/module_4_assignment.html",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "This is your assignment for Module 4 Putting It All Together, focused on the material you learned in the lectures and recitation activities on PCA, Manhattan plots, interactive plots, and the leftovers.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing. Starting on this assignment, I will be considering for overall format and readability of your assignment as part of your grade. I am doing this because the format of your report will be considered for your final capstone assignment. This means you should have reasonable headers and header levels, understandable flow between plots and code, and use Markdown language when appropriate.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, December 9, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is the same we used in the ggplot102 recitation that includes information about dog breed trait information from the American Kennel Club.\nDownload the data using the code below. Don‚Äôt use the code from week 5 recitation.\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_traits_fixed.csv')\n\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/trait_description.csv')\n\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_rank_all.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(gghighlight)",
    "crumbs": [
      "Module assignments",
      "Module 4"
    ]
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#introduction",
    "href": "assignments/modules/module4/module_4_assignment.html#introduction",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "This is your assignment for Module 4 Putting It All Together, focused on the material you learned in the lectures and recitation activities on PCA, Manhattan plots, interactive plots, and the leftovers.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\nYour headers should be logical and your report and code annotated with descriptions of what you‚Äôre doing. Starting on this assignment, I will be considering for overall format and readability of your assignment as part of your grade. I am doing this because the format of your report will be considered for your final capstone assignment. This means you should have reasonable headers and header levels, understandable flow between plots and code, and use Markdown language when appropriate.\n\nRemember there are often many ways to reach the same end product. I have showed you many ways in class to achieve a similar end product, you only need to show me one of them. As long as your answer is reasonable, you will get full credit even if its different than what I intended.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, December 9, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is the same we used in the ggplot102 recitation that includes information about dog breed trait information from the American Kennel Club.\nDownload the data using the code below. Don‚Äôt use the code from week 5 recitation.\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_traits_fixed.csv')\n\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/trait_description.csv')\n\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/jcooperstone/jcooperstone.github.io/main/assignments/modules/module4/data/breed_rank_all.csv')\n\nFor a little hint, here are the packages I used to complete this task. Yours might not be exactly the same.\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(gghighlight)",
    "crumbs": [
      "Module assignments",
      "Module 4"
    ]
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#principal-components-analysis-pca-of-american-kennel-club-dog-bred-trait-data-4-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#principal-components-analysis-pca-of-american-kennel-club-dog-bred-trait-data-4-pts",
    "title": "Module 4 Assignment",
    "section": "1. Principal components analysis (PCA) of American Kennel Club dog bred trait data (4 pts)",
    "text": "1. Principal components analysis (PCA) of American Kennel Club dog bred trait data (4 pts)\nRun a PCA on breed_traits for all of the numeric data present in that dataset. Create the following plots and make them of publication quality:\n\nA scree plot\nA scores plot\nA loadings plot\nA two panel plot that has the scores plot and the scree plot together",
    "crumbs": [
      "Module assignments",
      "Module 4"
    ]
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#make-your-pca-plot-interactive-2-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#make-your-pca-plot-interactive-2-pts",
    "title": "Module 4 Assignment",
    "section": "2. Make your PCA plot interactive (2 pts)",
    "text": "2. Make your PCA plot interactive (2 pts)\nMake your PCA scores plot interactive, and so that when you hover each point, you can see what the name of that dog breed is (and only the breed of that dog).",
    "crumbs": [
      "Module assignments",
      "Module 4"
    ]
  },
  {
    "objectID": "assignments/modules/module4/module_4_assignment.html#see-how-your-pca-related-to-breed-popularity-2-pts",
    "href": "assignments/modules/module4/module_4_assignment.html#see-how-your-pca-related-to-breed-popularity-2-pts",
    "title": "Module 4 Assignment",
    "section": "3. See how your PCA related to breed popularity (2 pts)",
    "text": "3. See how your PCA related to breed popularity (2 pts)\nUsing breed_traits and breed_rank_all, label the points that show data for the top 10 dog breeds in 2020 and color them different from the rest of the points. Your plot does not need to be interactive.",
    "crumbs": [
      "Module assignments",
      "Module 4"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html",
    "href": "assignments/modules/module2/module_2_assignment.html",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on R Markdown, wrangling, ggplot101, and ggplot102.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, October 7, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded.",
    "crumbs": [
      "Module assignments",
      "Module 2"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#introduction",
    "href": "assignments/modules/module2/module_2_assignment.html#introduction",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This is your assignment for Module 2, focused on the material you learned in the lectures and recitation activities on R Markdown, wrangling, ggplot101, and ggplot102.\nSubmission info: you will submit this assignment by uploading a knitted .html to Carmen.\n\nMake sure you include the Code Download button\nShow your code within your knitted .html\nCustomize the YAML and the document so you like how it looks\n\nRemember there are often many ways to reach the same end product.\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Tuesday, October 7, 2025 at 11:59pm.\n\n\n\n\nThe data we will be using is collected by the National Science Foundation about the fields and number of Ph.D.¬†degrees awarded each year.\n\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nTake a look at the data collected by NSF on how which fields give PhDs each year, and how many are awarded.",
    "crumbs": [
      "Module assignments",
      "Module 2"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#writing-in-markdown-1-1-pt",
    "href": "assignments/modules/module2/module_2_assignment.html#writing-in-markdown-1-1-pt",
    "title": "Module 2 Assignment",
    "section": "Writing in Markdown 1 (1 pt)",
    "text": "Writing in Markdown 1 (1 pt)\nUsing coding in text, write a sentence in markdown that pulls from this data how many total PhDs were awarded in 2017. If you want to make some calculations in a code chunk first that is ok.",
    "crumbs": [
      "Module assignments",
      "Module 2"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-1-3-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-1-3-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 1 (3 pts)",
    "text": "Visualization 1 (3 pts)\nMake a chart to visualize of the total number of PhDs awarded for each broad_field across the total time period of this data. You pick the type of chart that you think is appropriate, and make sure your plot is appropriately labelled and you are happy with how it looks. Hint, to do this you‚Äôll probably have to do some data wrangling first.",
    "crumbs": [
      "Module assignments",
      "Module 2"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-2-2-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-2-2-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 2 (2 pts)",
    "text": "Visualization 2 (2 pts)\nPick the field that most closely matches the area of your degree. Make a line graph (with points for each datapoint) that shows how the number of PhDs awarded in your field has changed from 2008 to 2017. Make sure your x-axis indicates each year for which you have data, your graph is appropriately labelled, and you think it is aesthetically pleasing.",
    "crumbs": [
      "Module assignments",
      "Module 2"
    ]
  },
  {
    "objectID": "assignments/modules/module2/module_2_assignment.html#visualization-3-2-pts",
    "href": "assignments/modules/module2/module_2_assignment.html#visualization-3-2-pts",
    "title": "Module 2 Assignment",
    "section": "Visualization 3 (2 pts)",
    "text": "Visualization 3 (2 pts)\nPick at least 3 additional fields (you can use more if you like) that are adjacent to your Ph.D.¬†field. Make a faceted plot to show the number of degrees awarded in each of these disciplines across the same time period. Make sure you label your plot appropriately and you think it is aesthetic (i.e., if you have squished strip text you want to fix that).",
    "crumbs": [
      "Module assignments",
      "Module 2"
    ]
  },
  {
    "objectID": "assignments/modules/module1/module_1_assignment.html",
    "href": "assignments/modules/module1/module_1_assignment.html",
    "title": "Module 1 Assignment",
    "section": "",
    "text": "This is your assignment for Module 1.\nPlease upload to Carmen:\n\n1 good visualization, along with 1 paragraph about why it is good\n1 bad visualization, along with 1 paragraph about why it is bad.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Monday, September 1, 2025 at 11:59pm so we can talk about them in class the next day.",
    "crumbs": [
      "Module assignments",
      "Module 1"
    ]
  },
  {
    "objectID": "assignments/modules/module1/module_1_assignment.html#introduction",
    "href": "assignments/modules/module1/module_1_assignment.html#introduction",
    "title": "Module 1 Assignment",
    "section": "",
    "text": "This is your assignment for Module 1.\nPlease upload to Carmen:\n\n1 good visualization, along with 1 paragraph about why it is good\n1 bad visualization, along with 1 paragraph about why it is bad.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment will be due on Monday, September 1, 2025 at 11:59pm so we can talk about them in class the next day.",
    "crumbs": [
      "Module assignments",
      "Module 1"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus, Autumn 2025",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D.\nEmail address: cooperstone dot 1 at osu dot edu (preferred contact method)\nPhone number: 614-292-2843 (non-preferred contact method)\nTA: Daniel Quiroz Moreno, quirozmoreno dot 1 at osu dot edu\nOffice hours: TBD",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#instructor",
    "href": "syllabus/syllabus.html#instructor",
    "title": "Syllabus, Autumn 2025",
    "section": "",
    "text": "Instructor: Jessica Cooperstone, Ph.D.\nEmail address: cooperstone dot 1 at osu dot edu (preferred contact method)\nPhone number: 614-292-2843 (non-preferred contact method)\nTA: Daniel Quiroz Moreno, quirozmoreno dot 1 at osu dot edu\nOffice hours: TBD",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-meeting-time-and-place",
    "href": "syllabus/syllabus.html#course-meeting-time-and-place",
    "title": "Syllabus, Autumn 2025",
    "section": "Course meeting time and place",
    "text": "Course meeting time and place\nTuesdays synchronously from 4:10-5:55 pm in:\n\nHowlett Hall 116 (Columbus)\nWilliams Hall 123 (Wooster)\nVirtually via Zoom https://go.osu.edu/dataviz-zoom",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Syllabus, Autumn 2025",
    "section": "Course description",
    "text": "Course description\nThis course aims to introduce students to the principles and practice of data visualization. Students will learn fundamental principles of data visualization and create figures that appropriately and ethically represent their data. Data visualizations will be created in the R programming environment, using tools including the grammar of graphics implemented in ggplot2. In the process of creating visualizations, students will also become familiar with data handling and wrangling in R.\nCourse learning outcomes\nBy the end of this course, students should successfully be able to:\n\nRecall and describe the fundamental goals and principles of data visualization.\nDistinguish between good and bad visualizations, and understand how to make those are ineffective more effective.\nLearn to use R, R Markdown, and ggplot to make clear, descriptive, and aesthetic visualization.\nApply principles learned in class, both theoretical and technical, to create effective visualizations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-delivery",
    "href": "syllabus/syllabus.html#course-delivery",
    "title": "Syllabus, Autumn 2025",
    "section": "Course delivery",
    "text": "Course delivery\nMode of delivery: This course is a hybrid-delivered course where I provide ~ 50 min of lecture material, followed by ~1 hour of recitation activities to do in-class (with assistance from the instructor and the TA). You can attend class in person or virtually via Zoom. You are also welcome to come to class with questions about the weekly videos, or problems you are currently encountering in creating your visualizations.\nAttendance and participation requirements: This is a hybrid course, but is taught synchronously, meaning it is expected that you attend class, either in person or virtually (via Zoom) during its meeting time. I will not take attendance. If circumstances require you to miss class, it will be expected you watch the recorded sessions, and catch up on material on your own. I have found that students who attend classes more quickly and completely master course content.\nClass recordings: To help you master material, and to better accommodate students, classes will be recorded, and recordings uploaded directly after class to Carmen. You can find a link to a OneDrive folder with the recorded lectures on the Syllabus page on Carmen.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-schedule",
    "href": "syllabus/syllabus.html#course-schedule",
    "title": "Syllabus, Autumn 2025",
    "section": "Course schedule",
    "text": "Course schedule\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-08-26\n1: Principles\nPrinciples of data visualization\n\n\n2025-09-02\n1: Principles\nGood and bad visualizations\n\n\n2025-09-09\n2: Coding fundamentals\nR Markdown for reproducible research\n\n\n2025-09-16\n2: Coding fundamentals\nWrangling, the basics\n\n\n2025-09-23\n2: Coding fundamentals\nggplot 101\n\n\n2025-09-30\n2: Coding fundamentals\nThemes, labels, facets (ggplot 102)\n\n\n2025-10-07\n3: Data exploration\nData distributions\n\n\n2025-10-14\n3: Data exploration\nCorrelations\n\n\n2025-10-21\nCapstone prep\nCapstone plan prep, open session\n\n\n2025-10-28\n3: Data exploration\nAnnotating statistics\n\n\n2025-11-04\n4: Putting it together\nPrincipal components analysis\n\n\n2025-11-11\nNo class, Veterans Day\n‚Äì\n\n\n2025-11-18\n4: Putting it together\nInteractive plots\n\n\n2025-11-25\n4: Putting it together\nManhattan plots and making lots of plots at once (asynchronous)\n\n\n2025-12-02\n4: Putting it together\nggplot extension packages\n\n\n2025-12-09\nCapstone prep\nCapstone assignment, open session\n\n\n\n\n\n\n\n\n\n\n\nNon-standard days\n\n\n\nThere is no class on November 11 for Veterans Day. On November 25 (the short week of Thanksgiving), I will provide to you the lecture material asynchronously and you can go through the recitation material on your own. Daniel and I will be available to answer questions during office hours, or at the beginning of the next class period.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-resources",
    "href": "syllabus/syllabus.html#course-resources",
    "title": "Syllabus, Autumn 2025",
    "section": "Course resources",
    "text": "Course resources\nThere are no required textbooks for this course, though you will find many of the recommend texts and resources belowvery useful.\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nIntroduction to Data Science by Rafael Irizarry\nData Visualization with R by Rob Kabacoff\nData Visualization, A practical introduction by Kieran Healy\nHands-On Data Visualization by Jack Dougherty and Ilya Ilyankou\nFundamentals of Data Visualization by Claus O. Wilke\nggplot2, Elegant Graphics for Data Analysis by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nModern Data Science with R by Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton\nR Markdown Cookbook by Yihui Xie, Christophe Dervieux, Emily Riederer\nThe tidyverse style guide\nA ggplot2 tutorial for beautiful plotting in R by C√©dric Scherer\nRStudio cheatsheets, including ggplot2, dplyr, tidyr, readr and other data import packages, stringr for managing character strings, forcats for managing factors, and R Markdown",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#required-software",
    "href": "syllabus/syllabus.html#required-software",
    "title": "Syllabus, Autumn 2025",
    "section": "Required software",
    "text": "Required software\n\nR: We will use the R programming environment for this class https://www.r-project.org/ (free). You can do so many things in R (including building this course website).\nRStudio Desktop: This IDE (integrated development environment) allows a user-friendly interface with the R programming environment, which we will use in class as well. You must have R before you download RStudio (free).\nMicrosoft Office 365: All Ohio State students are now eligible for free Microsoft Office 365. Full instructions for downloading and installation can be found at go.osu.edu/office365help.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#prior-r-experience",
    "href": "syllabus/syllabus.html#prior-r-experience",
    "title": "Syllabus, Autumn 2025",
    "section": "Prior R experience",
    "text": "Prior R experience\nYou do not need to be an R expert for this class, but I will assume working-level knowledge of R programming. If you have no experience with R, but would still like to take this class, you can. I ask then you get yourself up to speed by taking this free online class https://www.edx.org/course/data-science-r-basics (audit only) before the start of the 3rd week of class. The course will take 8-16 hours to complete so please leave yourself enough time to do so before week 3. Tips and tricks in R will be scattered throughout the course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#grading",
    "href": "syllabus/syllabus.html#grading",
    "title": "Syllabus, Autumn 2025",
    "section": "Grading",
    "text": "Grading\n\nHow your grade is calculated\n\n\n\n\n\n\n\nAssignment category (how many assignments)\nPoints\n\n\n\n\nModule assignments (4)\n32 (8 points per assignment)\n\n\nRecitations (8, you pick which ones)\n8 (1 point per recitation submitted)\n\n\nClass reflections (10, you pick which ones)\n20 (2 points per reflection)\n\n\nCapstone assignment (1 plan, 1 assignment)\n10 for the capstone plan, 30 for the assignment\n\n\n\nSee the Assignments tab for additional information.\n\n\nAssignment descriptions\n\nModule assignments\nDescription: After each module, there will be an assignment to provide practice for the techniques learned in class. Assignments will be posted at least one week prior to their due date, and due dates can be found on Carmen.\nGrading: Each part of the assignment will have a certain number of points associated with it, provided along with the assignment.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nRecitation submissions\nDescription: I ask you submit 8 of 11 recitations to Carmen to show you have made a good faith effort to engage with the course material.\nGrading: I will mark these at 0 or 1 points, with 1 point given for completion of at least 70% of the assignment.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own. Generative AI is not to be used.\n\n\nClass reflections\nAfter each week, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nThere will be 10 class reflections (you can select which classes you want to reflect upon).\nDue Date: Reflections are due 1 week after each class. For example, if class is on Tuesday September 1, the reflection for that class is due on Tuesday September 8 by 11:59pm.\nGrading: Reflections will be graded as follows:\n\nFull credit: Reflections thoughtfully engage with course content, demonstrate the student thought about material and how it would (or wouldn‚Äôt) be relevant to their work and development. This is the level of engagement I expect for this course.\nHalf credit: Reflections are superficial and demonstrate minimal engagement with the course content. A half credit grade indicates better engagement is required for the next reflection. I do not expect to assign these grades often.\n\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nCapstone assignment\nDescription: At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why. You will be required to submit a capstone assignment ‚Äúplan‚Äù by the beginning of November.\nGrading: Guidance will be provided for the grading of the capstone assignment. Assignments completed outside R Markdown, or not knitted to a .html are not acceptable.\nAcademic Integrity: Students may use class notes and class resource materials. Students are not permitted to collaborate on assignments. Students must complete work on their own.\n\n\nDue dates\n\n\n\n\n\nAssignment\nDue Date\n\n\n\n\nReflections\n1 week after each class\n\n\nRecitations\nSunday after each class\n\n\nModule 1: Good and bad visualizations\nMonday, September 1, 2025\n\n\nModule 2: Coding Fundamentals\nTuesday, October 7, 2025\n\n\nModule 3: Data Exploration\nTuesday, November 4, 2025\n\n\nModule 4: Putting it together\nTuesday, December 9, 2025\n\n\nCapstone plan\nTuesday, October 28, 2025\n\n\nCapstone\nFriday, December 12, 2025\n\n\n\n\n\n\n\n\nLate assignments\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.\n\n\nGrading scale\n\n\n\n\n\nScore\nGrade\n\n\n\n\n93‚Äì100\nA\n\n\n90‚Äì92.9\nA-\n\n\n87‚Äì89.9\nB+\n\n\n83‚Äì86.9\nB+\n\n\n80‚Äì82.9\nB-\n\n\n77‚Äì79.9\nC+\n\n\n73‚Äì76.9\nC\n\n\n70‚Äì72.9\nC-\n\n\n67‚Äì69.9\nD+\n\n\n60‚Äì66.9\nD\n\n\nBelow 60\nE\n\n\n\n\n\n\n\nInstructor feedback and response time\nYour instructional team is here to help you proceed through this course. We will match but not exceed the enthusiasm that you give to learn the course content.\nI am providing the following list to give you an idea of my intended availability throughout the course. (Remember that you can call 614-688-4357(HELP) at any time if you have a technical problem.)\n\nGrading and feedback: For assignments, you can generally expect feedback within 7 days.\nEmail: We will reply to emails within 48 hours on days when class is in session at the university.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#other-course-policies",
    "href": "syllabus/syllabus.html#other-course-policies",
    "title": "Syllabus, Autumn 2025",
    "section": "Other course policies",
    "text": "Other course policies\n\nDiscussion and communication guidelines\nI expect all communication will be respectful and thoughtful.\n\n\nAcademic Misconduct/Academic Integrity\nSee Descriptions of major course assignments, above, for my specific guidelines about collaboration and academic integrity in the context of this online class. Academic integrity is essential to maintaining an environment that fosters excellence in teaching, research, and other educational and scholarly activities. Thus, The Ohio State University and the Committee on Academic Misconduct (COAM) expect that all students have read and understand the University‚Äôs Code of Student Conduct, and that all students will complete all academic and scholarly assignments with fairness and honesty. Students must recognize that failure to follow the rules and guidelines established in the University‚Äôs Code of Student Conduct and this syllabus may constitute Academic Misconduct.\nThe Ohio State University‚Äôs Code of Student Conduct (Section 3335-23-04) defines academic misconduct as: Any activity that tends to compromise the academic integrity of the University or subvert the educational process. Examples of academic misconduct include (but are not limited to) plagiarism, collusion (unauthorized collaboration), copying the work of another student, and possession of unauthorized materials during an examination. Ignorance of the University‚Äôs Code of Student Conduct is never considered an excuse for academic misconduct, so I recommend that you review the Code of Student Conduct and, specifically, the sections dealing with academic misconduct.\nIf I suspect that a student has committed academic misconduct in this course, I am obligated by University Rules to report my suspicions to the Committee on Academic Misconduct. If COAM determines that you have violated the University‚Äôs Code of Student Conduct (i.e., committed academic misconduct), the sanctions for the misconduct could include a failing grade in this course and suspension or dismissal from the University.\nIf you have any questions about the above policy or what constitutes academic misconduct in this course, please contact me.\n\n\nUse of Generative AI (e.g., ChatGPT, Co-Pilot or similar)\nArtificial intelligence tools, like ChatGPT, Microsoft Co-Pilot or similar are not to be used on assignments unless instructions explicitly indicate to do so. Suspected unauthorized use of generative AI on assignments of any type will be reported to COAM.\n\n\nCreating an environment free from harassment, discrimination, and sexual misconduct\nThe Ohio State University is committed to building and maintaining a community to reflect diversity and to improve opportunities for all. All Buckeyes have the right to be free from harassment, discrimination, and sexual misconduct. Ohio State does not discriminate on the basis of age, ancestry, color, disability, ethnicity, gender, gender identity or expression, genetic information, HIV/AIDS status, military status, national origin, pregnancy (childbirth, false pregnancy, termination of pregnancy, or recovery therefrom), race, religion, sex, sexual orientation, or protected veteran status, or any other bases under the law, in its activities, academic programs, admission, and employment. Members of the university community also have the right to be free from all forms of sexual misconduct: sexual harassment, sexual assault, relationship violence, stalking, and sexual exploitation.\nTo report harassment, discrimination, sexual misconduct, or retaliation and/or seek confidential and non-confidential resources and supportive measures, contact the Civil Rights Compliance Office:\nOnline reporting form at civilrights.osu.edu Call 614-247-5838 or TTY 614-688-8605, Or Email civilrights@osu.edu\nThe university is committed to stopping sexual misconduct, preventing its recurrence, eliminating any hostile environment, and remedying its discriminatory effects. All university employees have reporting responsibilities to the Civil Rights Compliance Office to ensure the university can take appropriate action:\n\nAll university employees, except those exempted by legal privilege of confidentiality or expressly identified as a confidential reporter, have an obligation to report incidents of sexual assault immediately.\nThe following employees have an obligation to report all other forms of sexual misconduct as soon as practicable but at most within five workdays of becoming aware of such information: 1. Any human resource professional (HRP); 2. Anyone who supervises faculty, staff, students, or volunteers; 3. Chair/director; and 4. Faculty member.\n\nIn addition, this course adheres to The Principles of Community adopted by the College of Food, Agricultural, and Environmental Sciences. These principles can be found at https://cfaes.osu.edu/about/cfaes-principles-community. If you have been a victim of or a witness to harassment or discrimination or a bias incident, you can report it online and anonymously (if you choose) at https://civilrights.osu.edu/.\n\n\nIntellectual Diversity\nOhio State is committed to fostering a culture of open inquiry and intellectual diversity within the classroom. This course will cover a range of information and may include discussions or debates about controversial issues, beliefs, or policies. Any such discussions and debates are intended to support understanding of the approved curriculum and relevant course objectives rather than promote any specific point of view. Students will be assessed on principles applicable to the field of study and the content covered in the course. Preparing students for citizenship includes helping them develop critical thinking skills that will allow them to reach their own conclusions regarding complex or controversial matters.\n\n\nReligious accommodations\nOhio State has had a longstanding practice of making reasonable academic accommodations for students‚Äô religious beliefs and practices in accordance with applicable law. In 2023, Ohio State updated its practice to align with new state legislation. Under this new provision, students must be in early communication with their instructors regarding any known accommodation requests for religious beliefs and practices, providing notice of specific dates for which they request alternative accommodations within 14 days after the first instructional day of the course. Instructors in turn shall not question the sincerity of a student‚Äôs religious or spiritual belief system in reviewing such requests and shall keep requests for accommodations confidential.\nWith sufficient notice, instructors will provide students with reasonable alternative accommodations with regard to examinations and other academic requirements with respect to students‚Äô sincerely held religious beliefs and practices by allowing up to three absences each semester for the student to attend or participate in religious activities. Examples of religious accommodations can include, but are not limited to, rescheduling an exam, altering the time of a student‚Äôs presentation, allowing make-up assignments to substitute for missed class work, or flexibility in due dates or research responsibilities. If concerns arise about a requested accommodation, instructors are to consult their tenure initiating unit head for assistance.\nA student‚Äôs request for time off shall be provided if the student‚Äôs sincerely held religious belief or practice severely affects the student‚Äôs ability to take an exam or meet an academic requirement and the student has notified their instructor, in writing during the first 14 days after the course begins, of the date of each absence. Although students are required to provide notice within the first 14 days after a course begins, instructors are strongly encouraged to work with the student to provide a reasonable accommodation if a request is made outside the notice period. A student may not be penalized for an absence approved under this policy.\nIf students have questions or disputes related to academic accommodations, they should contact their course instructor, and then their department or college office. For questions or to report discrimination or harassment based on religion, individuals should contact the Civil Rights Compliance Office.\n\n\nWeather or other short-term closing\nAlthough Ohio State strives to remain open to ensure continuity of services to students and the public, extreme conditions can warrant the usage of the university‚Äôs Weather or Other Short-Term Closing Policy. Please visit this webpage to learn more about preparing for potential closings and planning ahead for winter weather.\n\n\nCounseling and Consultation Services/Mental Health\nAs a student you may experience a range of issues that can cause barriers to learning, such as strained relationships, increased anxiety, alcohol/drug problems, feeling down, difficulty concentrating and/or lack of motivation. These mental health concerns or stressful events may lead to diminished academic performance or reduce a student‚Äôs ability to participate in daily activities. The Ohio State University offers services to assist you with addressing these and other concerns you may be experiencing. If you or someone you know are suffering from any of the aforementioned conditions, you can learn more about the broad range of confidential mental health services available on campus via the Office of Student Life Counseling and Consultation Services (CCS) by visiting ccs.osu.edu or calling (614) 292- 5766. CCS is located on the 4th Floor of the Younkin Success Center and 10th Floor of Lincoln Tower. You can reach an on-call counselor when CCS is closed at (614) 292-5766 and 24 hour emergency help is also available through the 24/7 National Suicide Prevention Hotline at 1-(800)-273-TALK or at suicidepreventionlifeline.org\nDavid Wirt, wirt.9@osu.edu, is the CFAES embedded mental health counselor in Columbus. He is available for new consultations and to establish routine care. To schedule with David, please call 614-292-5766. Students should mention their affiliation with CFAES when setting up a phone screening.\nDr.¬†Schaad, schaad.15@osu.edu, is the CFAES embedded mental health counselor in Wooster. She is available for new consultations and to establish routine care. To schedule with\nDr.¬†Schaad, please call 614-292-5766. Students should mention their affiliation with CFAES when setting up a phone screening.\n\n\n\n\nAccessibility accomodations\nThe university strives to maintain a healthy and accessible environment to support student learning in and out of the classroom. If you anticipate or experience academic barriers based on your disability (including mental health, chronic, or temporary medical conditions), please let me know immediately so that we can privately discuss options. To establish reasonable accommodations, I may request that you register with Student Life Disability Services. After registration, make arrangements with me as soon as possible to discuss your accommodations so that they may be implemented in a timely fashion.\nIf you are ill and need to miss class, including if you are staying home and away from others while experiencing symptoms of a viral infection or fever, please let me know immediately. In cases where illness interacts with an underlying medical condition, please consult with Student Life Disability Services to request reasonable accommodations. You can connect with them at slds@osu.edu; 614-292-3307; or slds.osu.edu.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html",
    "href": "rmds/03_Rmd_recitation_solutions.html",
    "title": "R Markdown Recitation Solutions",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html#r-markdown-recitation",
    "href": "rmds/03_Rmd_recitation_solutions.html#r-markdown-recitation",
    "title": "R Markdown Recitation Solutions",
    "section": "",
    "text": "This will be your template RMarkdown document for playing around with RMarkdown. This was opened using the default Rmd settings. You are going to be playing around with the 3 components of an Rmd:\n\ntext\ncode\nthe YAML (aka the header)\n\nTo see if each of your changes has worked, you will need to knit."
  },
  {
    "objectID": "rmds/03_Rmd_recitation_solutions.html#second-biggest-header",
    "href": "rmds/03_Rmd_recitation_solutions.html#second-biggest-header",
    "title": "R Markdown Recitation Solutions",
    "section": "Second biggest header",
    "text": "Second biggest header\n\nThird biggest header\n\n\nFour biggest header\nYou get the idea.\nAdd a hyperlink to our class website\nAdd an image. Note you will do this differently if you are adding an image from internet vs one you have on your local machine. Also remember your working directory is the location of your Rmd and you may want to have a directory called img where images are stored.\n\n\n\nThis is my dog Nacho\n\n\nAdd a block quote\n\nHere is an important block quote\n\nMake a bulleted list\n\nThing 1\nThing 2\nThing 3\nThis also works\nTo make\nA bulleted list\n\n\nYou can also\nMake\nNumbered lists"
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html",
    "title": "Manhattan Plots Recitation",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html#introduction",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html#introduction",
    "title": "Manhattan Plots Recitation",
    "section": "",
    "text": "We are going to practice making Manhattan plots today.\n\nlibrary(tidyverse) # for everything\nlibrary(ggrepel) # for repelling labels\nlibrary(qqman) # for gwas data\n\ngwasResults &lt;- qqman::gwasResults",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html#investigate-your-data.",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html#investigate-your-data.",
    "title": "Manhattan Plots Recitation",
    "section": "Investigate your data.",
    "text": "Investigate your data.\n\nWhat are your columns?\n\n\nHow many markers are there?\n\n\nHow are the markers distributed across the chromosomes?",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/12_manhattan/12_manhattan_recitation.html#make-a-manhattan-plot.",
    "href": "modules/module4/12_manhattan/12_manhattan_recitation.html#make-a-manhattan-plot.",
    "title": "Manhattan Plots Recitation",
    "section": "Make a Manhattan plot.",
    "text": "Make a Manhattan plot.\nColor by chromosome, make sure the x-axis breaks are appropriate, be sure your y-axis is -log10 pvalue. Label the top 3 most significant points with their SNP number.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 12 - Manhattan plots"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html",
    "title": "Leftover tidbits recitation",
    "section": "",
    "text": "Today‚Äôs recitation materials are on a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\nlibrary(gghighlight) # for highlighting\nlibrary(gganimate) # animating plots\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\nlibrary(scales) # for easy scaling\nlibrary(plotly) # for ggplotly\nlibrary(glue) # for easy pasting\n\nlibrary(gapminder) # for data for viz2",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#introduction",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#introduction",
    "title": "Leftover tidbits recitation",
    "section": "",
    "text": "Today‚Äôs recitation materials are on a bunch of stuff I thought was interesting but didn‚Äôt fit specifically into any of the other lessons. This includes some cool ggplot extension packages we haven‚Äôt gone over yet, and heatmaps that utilize base R plotting.\n\n\nLoading the libraries that are for each section. Individual libraries are before each section so you can see which go with what plot types.\n\nlibrary(tidyverse) # for everything\nlibrary(gghighlight) # for highlighting\nlibrary(gganimate) # animating plots\nlibrary(ggrepel) # for text/label repelling\nlibrary(magick) # for gif rendering\nlibrary(scales) # for easy scaling\nlibrary(plotly) # for ggplotly\nlibrary(glue) # for easy pasting\n\nlibrary(gapminder) # for data for viz2",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#really-start-using-an-rproject",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#really-start-using-an-rproject",
    "title": "Leftover tidbits recitation",
    "section": "Really start using an Rproject üìΩÔ∏è",
    "text": "Really start using an Rproject üìΩÔ∏è\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nIf you don‚Äôt have a Rproject for class, set one up.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-1",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-1",
    "title": "Leftover tidbits recitation",
    "section": "Visualization 1",
    "text": "Visualization 1\nWe are going to interrogate a dataset from Gapminder that includes information about Happiness Scores collected across different countries and years.\nCreate a visualization that shows the happiness scores for all countries from 2008 to 2010. Highlight in some way the top 3 countries with the highest happiness scores per continent.\nI‚Äôve put the data on Github so you can easily download it with the code below. Note, the question asks you to make a plot considering continent so I‚Äôve also provided you a key that has each country, and the continent to which it belows for you to join together.\n\nLoad data\n\nhappiness &lt;- read_csv(\"https://github.com/jcooperstone/dataviz-site/raw/master/4_12_leftovers/data/hapiscore_whr.csv\")\n\ncountry_continent &lt;- read_csv(\"https://github.com/jcooperstone/dataviz-site/raw/master/4_12_leftovers/data/country_continent.csv\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-2",
    "href": "modules/module4/13_leftovers/13_leftovers_recitation.html#visualization-2",
    "title": "Leftover tidbits recitation",
    "section": "Visualization 2",
    "text": "Visualization 2\nRecreate a plot in the vein of the one here. You can make the same interactive plot (use the data from 2007, which is slightly older and different from what you see in the online plot), or choose to animate it over year. Or do both.\nUse the data gapminder::gapminder which you can access from R.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 13 - Leftovers"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "",
    "text": "Get familiar with plotly\nDiscuss options for interactive plots",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#installing-plotly",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#installing-plotly",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Installing plotly",
    "text": "Installing plotly\n\n# From CRAN\ninstall.packages(\"plotly\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#ggplotly",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#ggplotly",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "ggplotly()",
    "text": "ggplotly()\n\n\n\n\n\n\n\n\n\nImage retrieved from this blog.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#description",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#description",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Description",
    "text": "Description\nThis function converts a ggplot2::ggplot() object to a plotly object.\nggplotly(\n  p = ggplot2::last_plot(),\n  width = NULL,\n  height = NULL,\n  tooltip = \"all\",\n  dynamicTicks = FALSE,\n  layerData = 1,\n  originalData = TRUE,\n  source = \"A\",\n  ...\n)\nNow, we are going to explore using plotly on ggplot objects using ggplotly().",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#basic-scatter-plot",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#basic-scatter-plot",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Basic scatter plot",
    "text": "Basic scatter plot\n\nggpenguins &lt;- palmerpenguins::penguins |&gt; \n# note that x is the default first argument and y is the second\n# and this works even though we haven't said this explicitly\n  ggplot(aes(bill_length_mm, body_mass_g, color = species )) + \n  geom_point() + \n  scale_color_d3() + # from ggsci\n  theme_bw() +\n  labs(x = \"Bill length, in mm\",\n       y = \"Body mass, in g\",\n       color = \"Species\",\n       title = \"Palmer penguin body mass by bill length\")\n\nggpenguins\n\n\n\n\n\n\n\n\nThings you can do with an interactive plot:\n\nZoom in and out\nTurn off groups (click on the legend)\nDownload plot as a .png (click the camera)\n\n\nggplotly(ggpenguins)",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#pca",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#pca",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "PCA",
    "text": "PCA\n\n# Library here just to make emphasis \nlibrary(ggfortify)\n\nIn this case we are going to explore a new library ggfortify that accepts prcomp results and creates an automatic scores PCA plot.\nAfter you perform PCA analysis, you can use autoplot() to create a ggplot2 object for using in the ggplotly() function.\n\ndf &lt;- iris[1:4] # Extract numeric variables\n\npca_res &lt;- prcomp(df, scale = TRUE) # Run PCA\n\np &lt;- autoplot(pca_res, data = iris, colour = 'Species') + # PCA autoplot\n  scale_color_d3() +\n  labs(title = \"Scores plot of iris data\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at &lt;https://github.com/sinhrks/ggfortify/issues&gt;.\n\nggplotly(p)",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#boxplots",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#boxplots",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Boxplots",
    "text": "Boxplots\nData used here is midwest from ggplot2 which contains demographic information from census data collected in the 2000 US census. The variable percollege includes what percentage of the respondents are college educated.\n\nboxplot &lt;- ggplot(midwest, aes(x = state, y = percollege, color = state) ) + \n  geom_boxplot() +\n  scale_color_d3() +\n  theme_bw() + \n  coord_flip() +\n  labs(title = \"Percentage of college educated respondents\")\n\nggplotly(boxplot)",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#barplots",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#barplots",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Barplots",
    "text": "Barplots\n\nstack_barplot &lt;- ggplot(mpg, aes(x = class))   + \n  geom_bar(aes(fill = drv)) + \n  scale_fill_d3() + \n  theme_bw() +\n  labs(title = \"Class of cars in the mpg datase\")\n\nggplotly(stack_barplot)",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#tooltip",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#tooltip",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Tooltip",
    "text": "Tooltip\nTooltip is an argument that controls the text that is shown when you hover the mouse over data. By default, all aes mapping variables are shown. You can modify the order and the variables that are shown in the tooltip.\nIn the penguins data we have more variables that may want to include in the text shown in our plotly plot such as sex and island.\n\n# dt[seq(10),] subset the ten first row and then use glimpse to shorten the output\nglimpse(palmerpenguins::penguins[seq(10), ]) \n\nRows: 10\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n\n\n\nggplotly(ggpenguins)\n\n\n\n\n\n\n‚Äúcolour‚Äù is required and ‚Äúcolor‚Äù is not supported in ggplotly()\n\n\nggplotly(ggpenguins,\n         tooltip = \"colour\")",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#changing-hover-details",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#changing-hover-details",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Changing hover details",
    "text": "Changing hover details\nYou might not like the default hover text aesthetics, and can change them! You can do this using style and layout and adding these functions using the pipe (|&gt; or |&gt;).\nCode taken from OSU‚Äôs Code Club\n\n# setting fonts for the plot\nfont &lt;- list(\n  family = \"Arial\",\n  size = 15,\n  color = \"white\")\n\n# setting hover label specs\nlabel &lt;- list(\n  bgcolor = \"#3d1b40\",\n  bordercolor = \"transparent\",\n  font = font) # we can do this bc we already set font\n\n# amending our ggplotly call to include new fonts and hover label specs\nggplotly(ggpenguins, tooltip = \"colour\") |&gt;\n  style(hoverlabel = label) |&gt;\n  layout(font = font)",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/11_interactive-plots/11_interactive-plots.html#helpful-resources",
    "href": "modules/module4/11_interactive-plots/11_interactive-plots.html#helpful-resources",
    "title": "Interactive plots with plotly and ggplotly",
    "section": "Helpful resources",
    "text": "Helpful resources\n\nUsing different fonts: link\nggfortify: link\nggsci: link",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 11 - Interactive plots"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html",
    "href": "modules/module4/10_pca/10_pca_recitation.html",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands compiled by f-imp and posted to Github.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#introduction",
    "href": "modules/module4/10_pca/10_pca_recitation.html#introduction",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "",
    "text": "Today is the first recitation for Module 4 where we put together a lot of the material we‚Äôve learned in the first 3 modules of this course. Today‚Äôs material is on conducting principal components analysis (PCA) using R, and visualizing the results with some tools we‚Äôve already learned to use, and some new wrangling and viz tips along the way.\n\n\n\n\n\nSource\n\n\n\n\n\nlibrary(tidyverse) # everything\nlibrary(readxl) # reading in excel sheets\nlibrary(factoextra) # easy PCA plotting\nlibrary(glue) # easy pasting\nlibrary(ggrepel) # repelling labels away from their points\nlibrary(patchwork) # for combining and arranging plots\n\n\n\nWe will be using data about pizza, which includes data collected about the nutritional information of 300 different grocery store pizzas, from 10 brands compiled by f-imp and posted to Github.\n\npizza &lt;- read_csv(file = \"https://raw.githubusercontent.com/f-imp/Principal-Component-Analysis-PCA-over-3-datasets/master/datasets/Pizza.csv\")\n\nRows: 300 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): brand\ndbl (8): id, mois, prot, fat, ash, sodium, carb, cal\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow different are each of the different brands of pizzas analyzed overall?",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#run-a-pca",
    "href": "modules/module4/10_pca/10_pca_recitation.html#run-a-pca",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "1. Run a PCA",
    "text": "1. Run a PCA",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-scree-plot-of-the-percent-variance-explained-by-each-component",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "2. Make a scree plot of the percent variance explained by each component",
    "text": "2. Make a scree plot of the percent variance explained by each component",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-scores-plot-of-samples-coloring-each-sample-by-its-brand",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "3. Make a scores plot of samples, coloring each sample by its brand",
    "text": "3. Make a scores plot of samples, coloring each sample by its brand",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#make-a-loadings-plot-of-samples",
    "href": "modules/module4/10_pca/10_pca_recitation.html#make-a-loadings-plot-of-samples",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "4. Make a loadings plot of samples",
    "text": "4. Make a loadings plot of samples",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module4/10_pca/10_pca_recitation.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "href": "modules/module4/10_pca/10_pca_recitation.html#create-either-a-biplot-or-a-visualization-that-shows-both-your-scores-and-loadings-plot-together.",
    "title": "Principal Components Analysis Recitation üçï",
    "section": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together.",
    "text": "5. Create either a biplot, or a visualization that shows both your scores and loadings plot together.",
    "crumbs": [
      "Module 4 - Putting it together",
      "Week 10 - Principal components analysis"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html",
    "title": "Adding Statistics Recitation Solutions",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\n\nlibrary(tidyverse) # for everything\nlibrary(NHANES) # for data\nlibrary(rstatix) # for pipe friendly statistics functions\nlibrary(ggpubr) # for easy annotating of stats\nlibrary(glue) # for easy pasting\nlibrary(rcompanion) # for creating the comparison table",
    "crumbs": [
      "Recitation solutions",
      "Week 9 - Adding statistics solutions"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#introduction",
    "title": "Adding Statistics Recitation Solutions",
    "section": "",
    "text": "Today you will be practicing what we learned in today‚Äôs class on adding statistics to your plots.\n\n\nWe will be using the NHANES data again from the package NHANES.\n\nlibrary(tidyverse) # for everything\nlibrary(NHANES) # for data\nlibrary(rstatix) # for pipe friendly statistics functions\nlibrary(ggpubr) # for easy annotating of stats\nlibrary(glue) # for easy pasting\nlibrary(rcompanion) # for creating the comparison table",
    "crumbs": [
      "Recitation solutions",
      "Week 9 - Adding statistics solutions"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "href": "modules/module3/09_add-stats/09_add-stats_recitation_solutions.html#is-total-cholesterol-totchol-different-by-age-agedecade",
    "title": "Adding Statistics Recitation Solutions",
    "section": "Is total cholesterol (TotChol) different by age (AgeDecade)?",
    "text": "Is total cholesterol (TotChol) different by age (AgeDecade)?\n\n\n\n\n\n\nNeed a hint? (Click to expand)\n\n\n\n\n\nHint - you want to test your assumptions to see what tests to do. You might need to use different posthoc comparison methods than we did in class.\n\n\n\n\n\n\n\n\n\nNeed another hint? (Click to expand)\n\n\n\n\n\nAnother hint - the function rcompanion::cldList() will convert the resulting comparison table from a posthoc Dunn test to create a column with the letters indicating which groups are significantly different from each other.\n\n\n\n\nBase plot\nPlot to get an overview.\n\n(totchol_age_baseplot &lt;- NHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt;\n  ggplot(aes(x = AgeDecade, y = TotChol, group = AgeDecade)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Age, by Decade\",\n       y = \"Total Cholesterol (mmol/L)\",\n       title = \"Differences in total cholesterol by age in NHANES 2009/2010, and 2011/2012\"))\n\n\n\n\n\n\n\n\nWould a violin plot be better?\n\nNHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt;\n  ggplot(aes(x = AgeDecade, y = TotChol, group = AgeDecade)) +\n  geom_violin() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Age, by Decade\",\n       y = \"Total Cholesterol (mmol/L)\",\n       title = \"Differences in total cholesterol by age in NHANES 2009/2010, and 2011/2012\")\n\n\n\n\n\n\n\n\nEh I think I like the boxplot better.\nUse stat_compare_means()\n\nNHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt;\n  ggplot(aes(x = AgeDecade, y = TotChol, group = AgeDecade)) +\n  geom_boxplot() +\n  stat_compare_means() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Age, by Decade\",\n       y = \"Total Cholesterol (mmol/L)\",\n       title = \"Differences in total cholesterol by age from NHANES 2009/2010, and 2011/2012\")\n\n\n\n\n\n\n\n\n\n\nTesting assumptions\n\nNormality\n\n# testing normality by group\nNHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt; # remove NAs\n  group_by(AgeDecade) |&gt;\n  shapiro_test(TotChol) # test for normality\n\n# A tibble: 8 √ó 4\n  AgeDecade variable statistic        p\n  &lt;fct&gt;     &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 \" 0-9\"    TotChol      0.986 5.23e- 4\n2 \" 10-19\"  TotChol      0.971 5.15e-15\n3 \" 20-29\"  TotChol      0.988 1.85e- 8\n4 \" 30-39\"  TotChol      0.963 1.58e-17\n5 \" 40-49\"  TotChol      0.960 7.88e-19\n6 \" 50-59\"  TotChol      0.987 6.76e- 9\n7 \" 60-69\"  TotChol      0.986 2.11e- 7\n8 \" 70+\"    TotChol      0.983 6.20e- 6\n\n\nNot normal.\n\nNHANES |&gt; \n  ggplot(aes(x = TotChol)) +\n  geom_histogram(bins = 50)\n\nWarning: Removed 1526 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\nConstant variance\n\nNHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt; # remove NAs\n  levene_test(TotChol ~ AgeDecade) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic        p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1     7  8158      48.6 4.39e-68\n\n\nNon constant variance. Non-parametric it is.\n\n\n\nLog transformed tests\nWhat if I log transform my data? Does it look more normal then?\n\nNHANES_log &lt;- NHANES |&gt;\n  mutate(TotChol_log2 = log2(TotChol))\n\n\nNormality\n\n# testing normality by group\nNHANES_log |&gt;\n  drop_na(AgeDecade, TotChol_log2) |&gt; # remove NAs\n  group_by(AgeDecade) |&gt;\n  shapiro_test(TotChol_log2) # test for normality\n\n# A tibble: 8 √ó 4\n  AgeDecade variable     statistic            p\n  &lt;fct&gt;     &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n1 \" 0-9\"    TotChol_log2     0.995 0.219       \n2 \" 10-19\"  TotChol_log2     0.991 0.00000180  \n3 \" 20-29\"  TotChol_log2     0.989 0.0000000306\n4 \" 30-39\"  TotChol_log2     0.995 0.000536    \n5 \" 40-49\"  TotChol_log2     0.993 0.00000977  \n6 \" 50-59\"  TotChol_log2     0.993 0.0000100   \n7 \" 60-69\"  TotChol_log2     0.989 0.00000472  \n8 \" 70+\"    TotChol_log2     0.995 0.0751      \n\n\nStill pretty not normal via Shapiro Test. Let‚Äôs look at the log2 transformed total choletserol distributions across the different age groups.\n\nNHANES_log |&gt;\n  drop_na(TotChol_log2, AgeDecade) |&gt;\n  ggplot(aes(x = TotChol_log2)) +\n  geom_histogram() +\n  facet_wrap(vars(AgeDecade)) +\n  theme_classic() +\n  labs(x = \"Log2 Total Cholesterol\",\n       y = \"Count\",\n       title = \"Distribution of cholesterol levels by age\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nSome age groups look more normal than others.\n\n\nConstant variance\n\nNHANES_log |&gt;\n  drop_na(AgeDecade, TotChol_log2) |&gt; # remove NAs\n  levene_test(TotChol_log2 ~ AgeDecade) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic        p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1     7  8158      16.8 4.03e-22\n\n\nStill no constant variance.\n\n\n\nKruskal Wallis test\n\nkruskal_chol &lt;- NHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt; # remove NAs\n  kruskal_test(TotChol ~ AgeDecade)\n\nOk significant difference exists. Where is it?\n\n\nPost-hoc analysis\nRun Dunn test, which is the posthoc test that goes along the Kruskal-Wallis. In an analogy example, ANOVA is to Tukey as Kruskall-Wallis is to Dunn. I am using the Benjamini Hochberg multiple testing correction, the default in this method is p.adjust.method = \"holm\" which also would be ok to use in this case.\n\nkruskal_chol_posthoc &lt;- NHANES |&gt;\n  drop_na(AgeDecade, TotChol) |&gt; # remove NAs\n  dunn_test(TotChol ~ AgeDecade,\n            p.adjust.method = \"BH\") # Benjamini Hochberg multiple testing correction\n\nknitr::kable(kruskal_chol_posthoc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\np\np.adj\np.adj.signif\n\n\n\n\nTotChol\n0-9\n10-19\n415\n1214\n0.0499933\n0.9601277\n0.9601277\nns\n\n\nTotChol\n0-9\n20-29\n415\n1257\n10.5808362\n0.0000000\n0.0000000\n****\n\n\nTotChol\n0-9\n30-39\n415\n1273\n15.8700258\n0.0000000\n0.0000000\n****\n\n\nTotChol\n0-9\n40-49\n415\n1354\n21.2610617\n0.0000000\n0.0000000\n****\n\n\nTotChol\n0-9\n50-59\n415\n1234\n22.1590557\n0.0000000\n0.0000000\n****\n\n\nTotChol\n0-9\n60-69\n415\n873\n18.7771929\n0.0000000\n0.0000000\n****\n\n\nTotChol\n0-9\n70+\n415\n546\n14.8778795\n0.0000000\n0.0000000\n****\n\n\nTotChol\n10-19\n20-29\n1214\n1257\n14.8156650\n0.0000000\n0.0000000\n****\n\n\nTotChol\n10-19\n30-39\n1214\n1273\n22.2911677\n0.0000000\n0.0000000\n****\n\n\nTotChol\n10-19\n40-49\n1214\n1354\n30.1092884\n0.0000000\n0.0000000\n****\n\n\nTotChol\n10-19\n50-59\n1214\n1234\n31.0354615\n0.0000000\n0.0000000\n****\n\n\nTotChol\n10-19\n60-69\n1214\n873\n25.1656727\n0.0000000\n0.0000000\n****\n\n\nTotChol\n10-19\n70+\n1214\n546\n18.7480286\n0.0000000\n0.0000000\n****\n\n\nTotChol\n20-29\n30-39\n1257\n1273\n7.4954589\n0.0000000\n0.0000000\n****\n\n\nTotChol\n20-29\n40-49\n1257\n1354\n15.1631720\n0.0000000\n0.0000000\n****\n\n\nTotChol\n20-29\n50-59\n1257\n1234\n16.4294388\n0.0000000\n0.0000000\n****\n\n\nTotChol\n20-29\n60-69\n1257\n873\n11.8155734\n0.0000000\n0.0000000\n****\n\n\nTotChol\n20-29\n70+\n1257\n546\n7.2165159\n0.0000000\n0.0000000\n****\n\n\nTotChol\n30-39\n40-49\n1273\n1354\n7.5785143\n0.0000000\n0.0000000\n****\n\n\nTotChol\n30-39\n50-59\n1273\n1234\n9.0202694\n0.0000000\n0.0000000\n****\n\n\nTotChol\n30-39\n60-69\n1273\n873\n5.0637294\n0.0000004\n0.0000005\n****\n\n\nTotChol\n30-39\n70+\n1273\n546\n1.4042843\n0.1602342\n0.1661688\nns\n\n\nTotChol\n40-49\n50-59\n1354\n1234\n1.6385488\n0.1013073\n0.1091001\nns\n\n\nTotChol\n40-49\n60-69\n1354\n873\n-1.6897791\n0.0910702\n0.1019987\nns\n\n\nTotChol\n40-49\n70+\n1354\n546\n-4.4189908\n0.0000099\n0.0000126\n****\n\n\nTotChol\n50-59\n60-69\n1234\n873\n-3.1166301\n0.0018293\n0.0022270\n**\n\n\nTotChol\n50-59\n70+\n1234\n546\n-5.6131492\n0.0000000\n0.0000000\n****\n\n\nTotChol\n60-69\n70+\n873\n546\n-2.7616144\n0.0057516\n0.0067102\n**\n\n\n\n\n\nIn the table above, you can see whether each group comparison is different. But, because we have 8 groups this gets a little bit complicated. For example, the first row says that the groups 0-9 and 10-19 are not significantly different. The second row says that 0-9 is significantly different from 20-29. We could sort out all the differences by looking at all the comparisons and making sure that groups that are different do not share a letter.\nUse rcompanion::cldList() to create the groups for us. Reading the documentation about cldList() helped me learn that:\n\nthere needs to be a formula that compares the p-values (here, p.adj) to a comparison column (here, one I created called comparison)\nthere needs to be a comparison column that is in the form similar to ‚ÄúTreat.A - Treat.B = 0‚Äù where =, 0 are removed by default. The removal of 0 affects our group names but we can fix that later. Since we have hyphens in our group names, I removed them since this column only allows one hyphen between the groups to be compared\nset a threshold for what p-value is considered significant\n\nTo do this, first:\n\nI removed the hyphen from group1 and group2 in new variables called group1_rep and group2_rep\nThen, I made a new column called comparison that ‚Äúglues‚Äù together (i.e., pastes) the values from group1_rep and group2_rep\n\n\n# combine group1 and group2 to make one column called comparison\n# then replace hyphens with something else because cldList can only have one hyphen\nkruskal_chol_posthoc_1 &lt;- kruskal_chol_posthoc |&gt;\n  mutate(group1_rep = str_replace_all(group1, pattern = \"-\", replacement = \"to\"),\n         group2_rep = str_replace_all(group2, pattern = \"-\", replacement = \"to\")) |&gt;\n  mutate(comparison = glue(\"{group1_rep} -{group2_rep}\"))\n\nknitr::kable(head(kruskal_chol_posthoc_1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\np\np.adj\np.adj.signif\ngroup1_rep\ngroup2_rep\ncomparison\n\n\n\n\nTotChol\n0-9\n10-19\n415\n1214\n0.0499933\n0.9601277\n0.9601277\nns\n0to9\n10to19\n0to9 - 10to19\n\n\nTotChol\n0-9\n20-29\n415\n1257\n10.5808362\n0.0000000\n0.0000000\n****\n0to9\n20to29\n0to9 - 20to29\n\n\nTotChol\n0-9\n30-39\n415\n1273\n15.8700258\n0.0000000\n0.0000000\n****\n0to9\n30to39\n0to9 - 30to39\n\n\nTotChol\n0-9\n40-49\n415\n1354\n21.2610617\n0.0000000\n0.0000000\n****\n0to9\n40to49\n0to9 - 40to49\n\n\nTotChol\n0-9\n50-59\n415\n1234\n22.1590557\n0.0000000\n0.0000000\n****\n0to9\n50to59\n0to9 - 50to59\n\n\nTotChol\n0-9\n60-69\n415\n873\n18.7771929\n0.0000000\n0.0000000\n****\n0to9\n60to69\n0to9 - 60to69\n\n\n\n\n\n\n# run cldList()\n(group_cldList &lt;- cldList(p.adj ~ comparison,\n        data = kruskal_chol_posthoc_1,\n        threshold = 0.05))\n\n  Group Letter MonoLetter\n1   to9      a      a    \n2 1to19      a      a    \n3 2to29      b       b   \n4 3to39      c        c  \n5 4to49     de         de\n6 5to59      d         d \n7 6to69      e          e\n8    7+      c        c  \n\n\nThe zeroes of the Group column got lost, let‚Äôs fix that by replacing with the values we are actually going to want to use (those with the hyphen vs.¬†those that have ‚Äúto‚Äù).\n\n# grab a vector that contains the names of the AgeDecade\n# want this without any missimg values\nage_groups &lt;- levels(NHANES$AgeDecade)\n\n# replace the groups with missing 0 with our cleaned up groups\ngroup_cldList$Group &lt;- age_groups\n\n# did it work?\ngroup_cldList\n\n   Group Letter MonoLetter\n1    0-9      a      a    \n2  10-19      a      a    \n3  20-29      b       b   \n4  30-39      c        c  \n5  40-49     de         de\n6  50-59      d         d \n7  60-69      e          e\n8    70+      c        c  \n\n\nOr, you could create groups from kruskal_chol_posthoc results manually.\n\nunique(NHANES$AgeDecade)\n\n[1]  30-39  0-9    40-49  60-69  50-59  10-19  20-29  70+   &lt;NA&gt;  \nLevels:  0-9  10-19  20-29  30-39  40-49  50-59  60-69  70+\n\n(group_manual &lt;- \n    data.frame(group = levels(NHANES$AgeDecade), # use levels to get the right order\n               letter = c(\"a\", \"a\", \"b\", \"c\", \"de\", \"d\", \"e\", \"c\"))) # letters manually\n\n   group letter\n1    0-9      a\n2  10-19      a\n3  20-29      b\n4  30-39      c\n5  40-49     de\n6  50-59      d\n7  60-69      e\n8    70+      c\n\n\nMake a dataframe that has the maximum total cholesterol for each age so that we know where to place the numbers on the plot. I was having some trouble with the summarize() function from dplyr being masked by one from Hmisc so I‚Äôm referring to the one I want explicitly. Another way to try and get around an issue like this would be to load the tidyverse as your last package so it isn‚Äôt the one that gets masked.\n\n(max_chol &lt;- NHANES |&gt;\n  drop_na(TotChol, AgeDecade) |&gt;\n  group_by(AgeDecade) |&gt;\n  dplyr::summarize(max_tot_chol = max(TotChol)))\n\n# A tibble: 8 √ó 2\n  AgeDecade max_tot_chol\n  &lt;fct&gt;            &lt;dbl&gt;\n1 \" 0-9\"            6.34\n2 \" 10-19\"          7.76\n3 \" 20-29\"          8.2 \n4 \" 30-39\"          9.93\n5 \" 40-49\"         13.6 \n6 \" 50-59\"         12.3 \n7 \" 60-69\"         10.3 \n8 \" 70+\"            9.05\n\n\nTo get one df that contains both the values to help place your numbers in the right spot and has your letters, you can either:\n\nbind the two dfs together (this requires them to be in the same order, which they are)\nor use a *_join() function\n\nThe *_join() is a preferred method because it does not rely on your data being ordered in the same way.\nBind the groups to the maximum cholesterol df by selecting just that column with the Letter.\n\n(dunn_for_plotting_bind &lt;- bind_cols(max_chol, group_cldList$Letter) |&gt;\n  rename(Letter = 3)) # rename the third column \"Letter\"\n\nNew names:\n‚Ä¢ `` -&gt; `...3`\n\n\n# A tibble: 8 √ó 3\n  AgeDecade max_tot_chol Letter\n  &lt;fct&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 \" 0-9\"            6.34 a     \n2 \" 10-19\"          7.76 a     \n3 \" 20-29\"          8.2  b     \n4 \" 30-39\"          9.93 c     \n5 \" 40-49\"         13.6  de    \n6 \" 50-59\"         12.3  d     \n7 \" 60-69\"         10.3  e     \n8 \" 70+\"            9.05 c     \n\n\nJoin the groups to the maximum cholesterol df.\n\n# since the two columns hae diff names in the two df\n# i'm indicating which columns should be joined\ndunn_for_plotting &lt;- full_join(max_chol, group_cldList,\n                               join_by(AgeDecade == Group))\n\n# did it work?\ndunn_for_plotting\n\n# A tibble: 8 √ó 4\n  AgeDecade max_tot_chol Letter MonoLetter\n  &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 \" 0-9\"            6.34 a      \"a    \"   \n2 \" 10-19\"          7.76 a      \"a    \"   \n3 \" 20-29\"          8.2  b      \" b   \"   \n4 \" 30-39\"          9.93 c      \"  c  \"   \n5 \" 40-49\"         13.6  de     \"   de\"   \n6 \" 50-59\"         12.3  d      \"   d \"   \n7 \" 60-69\"         10.3  e      \"    e\"   \n8 \" 70+\"            9.05 c      \"  c  \"   \n\n\n\n\nPlot\n\n# using geom_text()\ntotchol_age_baseplot +\n  geom_text(data = dunn_for_plotting,\n            aes(x = AgeDecade, \n                y = max_tot_chol + 1,\n                label = Letter)) +\n  labs(caption = \"Groups with different letters are significant different using the Kruskal Wallis test, \\nand the Dunn test for pairwise comparisons at p &lt; 0.05\")\n\n\n\n\n\n\n\n# using annotate()\ntotchol_age_baseplot +\n  annotate(geom = \"text\",\n           x = seq(1:8),\n           y = dunn_for_plotting$max_tot_chol + 1,\n           label = dunn_for_plotting$Letter) +\n  labs(caption = \"Groups with different letters are significant different using the Kruskal Wallis test, \\nand the Dunn test for pairwise comparisons at p &lt; 0.05\")\n\n\n\n\n\n\n\n\nI also decided to add for context, what the cut-off for normal cholesterol is, so someone can see how these values relate to normal values. A normal cholesterol level is below 200 mg/dL or below 5.17 mmol/L.\n\ntotchol_age_baseplot +\n  expand_limits(x = 0) + # a little more space to add a note\n  geom_hline(yintercept = 5.17, # set the yintercept\n             linetype = \"dashed\", # make the line dashed\n             color = \"red\") + # make the linered\n  # add means comparison letters\n  annotate(geom = \"text\",\n           x = seq(1:8),\n           y = dunn_for_plotting$max_tot_chol + 1,\n           label = dunn_for_plotting$Letter) +\n  # add a lil note about cholesterol\n  annotate(geom = \"text\",\n           x = 1, \n           y = 13, \n           size = 3,\n           label = \"5.17 nmol/L cholesterol \\nis the upper limit \\nfor normal levels\") +\n  # put that note in a box\n  annotate(geom = \"rect\", \n           xmin = 0.1, \n           xmax = 1.85, \n           ymin = 11.9, \n           ymax = 14.1,\n           color = \"black\", \n           alpha = .2) + # transparency\n  # add an arrow from the note to the horizontal line\n  geom_segment(aes(x = 1, y = 11.9, xend = 0.2, yend = 5.17),\n                  arrow = arrow(length = unit(0.15, \"cm\"))) +\n  labs(caption = \"Groups with different letters are significant different using the Kruskal Wallis test, \\nand the Dunn test for pairwise comparisons at p &lt; 0.05\")",
    "crumbs": [
      "Recitation solutions",
      "Week 9 - Adding statistics solutions"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html",
    "href": "modules/module3/09_add-stats/09_add-stats.html",
    "title": "Annotating Statistics onto Plots",
    "section": "",
    "text": "Figure from XKCD",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#introduction",
    "href": "modules/module3/09_add-stats/09_add-stats.html#introduction",
    "title": "Annotating Statistics onto Plots",
    "section": "Introduction",
    "text": "Introduction\nNow that we‚Äôve spent some time going through how to make plots, today we will focus on how to annotate statistics that you‚Äôve calculated to show statistical differences, embedded within your plot. I will go over a few different ways to do this.\nThe purpose of today‚Äôs session is more to give you practical experience with running and retrieving statistical analysis output, than teaching about the assumptions and background of the test itself. If you are looking for a good statistics class, I would recommend Dr.¬†Kristin Mercer‚Äôs HCS 8887 Experimental Design.\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries.\nWe are going to use data that was collection about body characteristics of penguins on Palmer Station in Antarctica. This data is in a dataframe called penguins in the package palmerpenguins which you can download from CRAN.\n\n\n\n\n\nFrom Palmer Penguins\n\n\n\n\n\n# install.packages(palmerpenguins)\nlibrary(tidyverse)\nlibrary(palmerpenguins) # for penguins data\nlibrary(rstatix) # for pipeable stats testing\nlibrary(agricolae) # for posthoc tests \nlibrary(ggpubr) # extension for adding stats to plots\nlibrary(glue) # for easy pasting\n\n\nknitr::kable(head(penguins)) # kable to make a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-t-tests-or-similar",
    "href": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-t-tests-or-similar",
    "title": "Annotating Statistics onto Plots",
    "section": "2 group comparisons (t-tests or similar)",
    "text": "2 group comparisons (t-tests or similar)\n\nOur question: Is there a significant difference in the body_weight_g of male and female penguins?\n\nBefore we run the statistics, let‚Äôs make a plot to see what this data looks like.\n\n# what are the values for sex?\nunique(penguins$sex)\n\n[1] male   female &lt;NA&gt;  \nLevels: female male\n\n# plot\n(penguins_by_sex &lt;- penguins |&gt;\n  drop_na(body_mass_g, sex) |&gt; # remove NAs for body_mass_g and sex\n  ggplot(aes(x = sex, y = body_mass_g, color = sex)) + \n  geom_boxplot(outlier.shape = NA) + # remove outliers\n  geom_jitter(height = 0, width = 0.3) + # jitter only in the x direction\n  scale_x_discrete(labels = c(\"Female\", \"Male\")) + # change x-axis labels\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Sex\",\n       y = \"Body Mass (g)\",\n       title = \"Body mass of penguins by sex\",\n       subtitle = \"Collected from Palmer Station, Antarctica\",\n       caption = \"Data accessed from the R package palmerpenguins\"))\n\n\n\n\n\n\n\n\nIt looks like there is a difference here. Before adding the statistics to our plot, let‚Äôs:\n\ntest that our data is suitable for running the text we want\nrun the statistical test separately from the plot\n\n\nTesting assumptions\nBriefly, in order to use parametric procedures (like a t-test), we need to be sure our data meets the assumptions for 1) normality and 2) constant variance. This is just one way to do these tests, there are others that I am not going to go over.\n\n\n\n\n\nIllustration by Allison Horst\n\n\n\n\n\nNormality\nWe will test normality by the Shapiro-Wilk test using the function rstatix::shapiro_test(). This function is a pipe-friendly wrapper for the function shapiro.test(), which just means you can use it with pipes.\n\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt; # remove NAs\n  group_by(sex) |&gt; # test by sex\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 2 √ó 4\n  sex    variable    statistic            p\n  &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 female body_mass_g     0.919 0.0000000616\n2 male   body_mass_g     0.925 0.000000123 \n\n\nThis data is not normal, which means we need to use non-parametric tests. Since we are not meeting the assumption for nornality, really you don‚Äôt need to test for constant variance, but I‚Äôll show you how to do it anyway.\n\n\nConstant variance\nWe can test for equal variance using Levene‚Äôs test, levene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\n\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt; # remove NAs\n  levene_test(body_mass_g ~ sex) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic      p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1   331      6.06 0.0143\n\n\nNo constant variance. Double Non-parametric.\nCan we visualize normality another way?\n\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt;\n  ggplot(aes(x = body_mass_g, y = sex, fill = sex)) +\n  ggridges::geom_density_ridges(alpha = 0.7) + # density ridgeline plot\n  scale_y_discrete(labels = c(\"Female\", \"Male\")) +\n  theme_classic() +  \n  theme(legend.position = \"none\") +\n  labs(x = \"Body Mass (g)\",\n       y = \"Sex\",\n       title = \"Distribution of body weights for male and female penguins\")\n\nPicking joint bandwidth of 235\n\n\n\n\n\n\n\n\n\nThese two distributions look bimodal(and thus not normal). This is likely because we have 3 different species of penguins here. You can see below that actually each species looks reasonably normal.\n\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt;\n  ggplot(aes(x = body_mass_g, fill = sex)) +\n  geom_histogram() +\n  facet_grid(cols = vars(species), rows = vars(sex)) + # 2 way facet\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Body Mass (g)\",\n       y = \"Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nNon-parametric t-test\nThis means if we want to test for different means, we can use the Wilcoxon rank sun test, or Mann Whitney test. If your data was normal, you could just change wilcox_test() to t_test() and the rest would be the same.\n\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt;\n  wilcox_test(body_mass_g ~ sex,\n              paired = FALSE)\n\n# A tibble: 1 √ó 7\n  .y.         group1 group2    n1    n2 statistic        p\n* &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 body_mass_g female male     165   168     6874. 1.81e-15\n\n\nThis is not surprising, that there is a significant difference in body weight between male and female penguins. We can see this clearly in our plot.\nHow can we add the stats to our plot?\n\n\nPlot\n\nUsing stat_compare_means()\nThe function stat_compare_means() allows mean comparison p-values to be easily added to a ggplot.\nNote, the function should look at your data and test for normality and pick the statistical test accordingly. You can see that is working in the chunk below, but I would recommend that you always do your own statistical test and make sure you plot accordingly.\n\npenguins_by_sex +\n  stat_compare_means()\n\n\n\n\n\n\n\npenguins_by_sex +\n  stat_compare_means(method = \"wilcox.test\") \n\n\n\n\n\n\n\n\n\n\nManually with geom_text() or annotate()\nIn general, plotting using geom_text() is easier, and follows classic geom_() syntax (e.g., includes aes()) but for some reason these don‚Äôt pass as vectorized objects so sometimes it yields low quality images. Using annotate() passes as vectors and thus tends to be higher quality. You can decide which you want to use depending on your purpose.\nIf I‚Äôm being honest, the most common way that I would add statistics to a plot if I was trying to do just a few simple plots at once, would be with annotate() . I like to use annotate() over geom_text() or geom_label() because it is vectorized and don‚Äôt become low quality down the road.\nWith geom_text()\n\npenguins_by_sex +\n  geom_text(aes(x = 2, y = 6500, label = \"*\"), # x, y, and label within aes()\n            color = \"black\", size = 6)\n\nWarning in geom_text(aes(x = 2, y = 6500, label = \"*\"), color = \"black\", : All aesthetics have length 1, but the data has 333 rows.\n‚Ñπ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWith annotate()\n\npenguins_by_sex +\n  annotate(geom = \"text\", # note no aes()\n           x = 2, y = 6500, \n           label = \"*\", \n           size = 6)\n\n\n\n\n\n\n\n\nYou can also add multiple annotation layers. I‚Äôm introducing a new function here, glue() which is amazing for easy syntax pasting of strings with data.\nThe syntax for glue() is like this:\n\nx &lt;- 2 + 3\n\nglue(\"2 + 3 = {x}\")\n\n2 + 3 = 5\n\n\n\n# we did this already, just assigning to object\nby_sex_pval &lt;- penguins |&gt;\n  drop_na(body_mass_g, sex) |&gt;\n  wilcox_test(body_mass_g ~ sex,\n              paired = FALSE)\n\n# plot\npenguins_by_sex +\n  ylim(2500, 7500) + # adjust the y-axis so there's space for the label\n  annotate(geom = \"text\", x = 2, y = 6500, label = \"*\", size = 6) +\n  annotate(geom = \"text\", x = 2, y = 7000,\n           label = glue(\"Wilcoxon signed rank test \\np-value = {by_sex_pval$p}\"))",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-anova-or-similar",
    "href": "modules/module3/09_add-stats/09_add-stats.html#group-comparisons-anova-or-similar",
    "title": "Annotating Statistics onto Plots",
    "section": ">2 group comparisons (ANOVA or similar)",
    "text": "&gt;2 group comparisons (ANOVA or similar)\nWhen we are comparing means between more than 2 samples, we will have to first run a statistical test to see if there are any significant differences among our groups, and then if there are, run a post-hoc test. Before we do that, let‚Äôs plot.\nAre there significant differences in body mass\n\n(penguins_f_massbyspecies &lt;- penguins |&gt;\n  drop_na(body_mass_g, species, sex) |&gt;\n  filter(sex == \"female\") |&gt;\n  ggplot(aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(outlier.shape = NA,\n              draw_quantiles = 0.5) + # add the median by drawing 50% quantile\n  ggdist::geom_dots(side = \"both\", color = \"black\", alpha = 0.8) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Penguin Species\",\n       y = \"Body Mass (g)\",\n       title = \"Body mass of female penguins by species\",\n       subtitle = \"Collected from Palmer Station, Antarctica\",\n       caption = \"Data accessed from the R package palmerpenguins\"))\n\nWarning: The `draw_quantiles` argument of `geom_violin()` is deprecated as of ggplot2\n4.0.0.\n‚Ñπ Please use the `quantiles.linetype` argument instead.\n\n\nWarning in geom_violin(outlier.shape = NA, draw_quantiles = 0.5): Ignoring\nunknown parameters: `outlier.shape`\n\n\n\n\n\n\n\n\n\n\nTesting assumptions\n\nNormality\n\n# testing normality by group\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt; # remove NAs\n  filter(sex == \"female\") |&gt;\n  group_by(species) |&gt; # test by species\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 3 √ó 4\n  species   variable    statistic     p\n  &lt;fct&gt;     &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    body_mass_g     0.977 0.199\n2 Chinstrap body_mass_g     0.963 0.306\n3 Gentoo    body_mass_g     0.981 0.511\n\n# testing normality across all data\npenguins |&gt;\n  drop_na(body_mass_g, sex) |&gt; # remove NAs\n  filter(sex == \"female\") |&gt;\n  shapiro_test(body_mass_g) # test for normality\n\n# A tibble: 1 √ó 3\n  variable    statistic            p\n  &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 body_mass_g     0.919 0.0000000616\n\n\nOk looks like we have normally distributed data among the different species of female penguins.\n\n\nConstant variance\nlevene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\n\npenguins |&gt;\n  drop_na(body_mass_g, sex, species) |&gt; # remove NAs\n  filter(sex == \"female\") |&gt;\n  levene_test(body_mass_g ~ species) # test for constant variance\n\n# A tibble: 1 √ó 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2   162    0.0357 0.965\n\n\nWe have constant variance. Along with normally distributed data, this means that we can use parametric tests. In the case of &gt;2 samples, that would be ANOVA.\n\n\n\nANOVA\nThe most commonly used function to run ANOVA in R is called aov() which is a part of the stats package that is pre-loaded with base R. So no new packages need to be installed here.\nIf we want to learn more about the function aov() we can do so using the code below. The help documentation will show up in the bottom right quadrant of your RStudio.\n\n?aov()\n\nWe can run an ANOVA by indicating our model, and here I‚Äôm also selecting to drop the NAs for our variables of interest, and filtering within the data = argument.\n\naov_female_massbyspecies &lt;- \n  aov(data = penguins |&gt; \n             filter(sex == \"female\") |&gt;\n             drop_na(body_mass_g, species),\n      body_mass_g ~ species)\n\nNow lets look at the aov object.\n\nsummary(aov_female_massbyspecies)\n\n             Df   Sum Sq  Mean Sq F value Pr(&gt;F)    \nspecies       2 60350016 30175008   393.2 &lt;2e-16 ***\nResiduals   162 12430757    76733                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can take the output of our ANOVA and use the function tidy() within the broom package to turn our output into a tidy table. Here, the notation broom::tidy() means I want to use the function tidy() that is a part of the broom package. This works even though I haven‚Äôt called library(broom) at the beginning of my script.\n\ntidy_anova &lt;- broom::tidy(aov_female_massbyspecies)\n\nknitr::kable(tidy_anova)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nspecies\n2\n60350016\n30175008.01\n393.2465\n0\n\n\nResiduals\n162\n12430757\n76733.07\nNA\nNA\n\n\n\n\n\nSee how this is different from just saving the ANOVA summary? Open both anova_summary and tidy_anova and note the differences.\n\nanova_summary &lt;- summary(aov_female_massbyspecies)\n\n\n\nPosthoc group analysis\nNow that we see we have a significant difference somewhere in the body mass of the 3 species of female penguins, we can do a posthoc test to see which groups are significantly different. We will do our post-hoc analysis using Tukey‚Äôs Honestly Significant Difference test and the function HSD.test() which is a part of the useful package agricolae.\n\ntukey_massbyspecies &lt;- HSD.test(aov_female_massbyspecies, \n                      trt = \"species\", \n                      console = TRUE) # prints the results to console\n\n\nStudy: aov_female_massbyspecies ~ \"species\"\n\nHSD Test for body_mass_g \n\nMean Square Error:  76733.07 \n\nspecies,  means\n\n          body_mass_g      std  r       se  Min  Max    Q25  Q50     Q75\nAdelie       3368.836 269.3801 73 32.42126 2850 3900 3175.0 3400 3550.00\nChinstrap    3527.206 285.3339 34 47.50637 2700 4150 3362.5 3550 3693.75\nGentoo       4679.741 281.5783 58 36.37285 3950 5200 4462.5 4700 4875.00\n\nAlpha: 0.05 ; DF Error: 162 \nCritical Value of Studentized Range: 3.345258 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n          body_mass_g groups\nGentoo       4679.741      a\nChinstrap    3527.206      b\nAdelie       3368.836      c\n\n\nLike we did with the aov object, you can also look at the resulting HSD.test object (here, tukey_massbyspecies) in your environment pane.\nHere, instead of using the broom package, you can convert the part of the tukey_bill_length object that contains the post-hoc groupings into a dataframe using as.data.frame().\n\ntidy_tukey &lt;- as.data.frame(tukey_massbyspecies$groups)\n\ntidy_tukey\n\n          body_mass_g groups\nGentoo       4679.741      a\nChinstrap    3527.206      b\nAdelie       3368.836      c\n\n\n\n\nPlot\n\nUsing stat_compare_means()\n\npenguins_f_massbyspecies +\n  stat_compare_means()\n\n\n\n\n\n\n\npenguins_f_massbyspecies +\n  stat_compare_means(method = \"anova\")\n\n\n\n\n\n\n\n\n\n\nManually with geom_text() or annotate()\nIn general, plotting using geom_text() is easier, and follows classic geom_() syntax (e.g., includes aes()) but for some reason these don‚Äôt pass as vectorized objects so sometimes it yields low quality images. Using annotate() passes as vectors and thus tends to be higher quality. You can decide which you want to use depending on your purpose.\nWe want to add the letters to this plot, so we can tell which groups of penguin species are significantly different.\nBefore we can do this, we will need to do some of everyone‚Äôs favorite task, wrangling. We are going to figure out what the maximum body_mass_g for each species is, so it will help us determine where to put our letter labels. Then, we can add our labels to be higher than the largest data point. We will calculate this for each group, so that the letters are always right about our boxplot.\n\nbody_mass_max &lt;- penguins |&gt;\n  filter(sex == \"female\") |&gt;\n  drop_na(body_mass_g, species) |&gt;\n  group_by(species) |&gt;\n  summarize(max_body_mass = max(body_mass_g))\n\nbody_mass_max\n\n# A tibble: 3 √ó 2\n  species   max_body_mass\n  &lt;fct&gt;             &lt;int&gt;\n1 Adelie             3900\n2 Chinstrap          4150\n3 Gentoo             5200\n\n\nLet‚Äôs add our post-hoc group info to body_mass_max, since those two dataframes are not in the same order. Instead of binding the two dataframes together, we are going to join them using one of the dplyr _join() functions, which allows you to combine dataframes based on a specific common column. The join functions work like this:\n\ninner_join(): includes all rows in x and y.\nleft_join(): includes all rows in x.\nright_join(): includes all rows in y.\nfull_join(): includes all rows in x or y.\n\nIn this case, it doesn‚Äôt matter which _join() we use because our dfs all have the exact same rows.\n\ntidier_tukey &lt;- tidy_tukey |&gt;\n  rownames_to_column() |&gt; # converts rownames to columns\n  rename(species = rowname) # renames the column now called rowname to species\n  \n# join\nbody_mass_for_plotting &lt;- full_join(tidier_tukey, body_mass_max,\n                               by = \"species\")\n\nLet‚Äôs plot. First using geom_text()\n\npenguins_f_massbyspecies +\n  geom_text(data = body_mass_for_plotting,\n            aes(x = species,\n                y = 175 + max_body_mass, \n                label = groups))\n\n\n\n\n\n\n\n\nNext using annotate().\n\npenguins_f_massbyspecies +\n  annotate(geom = \"text\",\n           x = c(3,2,1),\n           y = 175 + body_mass_for_plotting$max_body_mass,\n           label = body_mass_for_plotting$groups)",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/09_add-stats/09_add-stats.html#useful-resources",
    "href": "modules/module3/09_add-stats/09_add-stats.html#useful-resources",
    "title": "Annotating Statistics onto Plots",
    "section": "Useful resources",
    "text": "Useful resources\nThere have been previous Code Club sessions about adding statistics to plots:\n\nggpubr to add stats to plots by Daniel Quiroz\nt-tests in R by Mike Sovic\nRunning ANOVA in R and accesing output\nTesting ANOVA assumptions",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 9 - Adding statistics"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n---- Compiling #TidyTuesday Information for 2020-07-07 ----\n--- There is 1 file available ---\n\n\n‚îÄ‚îÄ Downloading files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  1 of 1: \"coffee_ratings.csv\"\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(tidyverse) # for everything\nlibrary(ggridges) # for ridgeline plots\nlibrary(ggdist) # for nice dotplots",
    "crumbs": [
      "Recitation solutions",
      "Week 7 - Data distributions solutions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#introduction",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#introduction",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "",
    "text": "Today you will be investigating some data from the Coffee Quality Database on coffee evaluation by the Coffee Quality Institute. You will look at the distribution of coffee evaluation scores across different characteristics, and include various metadata.\nMore information can be found on the Tidy Tuesday github repo on coffee ratings.\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\n\n---- Compiling #TidyTuesday Information for 2020-07-07 ----\n--- There is 1 file available ---\n\n\n‚îÄ‚îÄ Downloading files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  1 of 1: \"coffee_ratings.csv\"\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n\n\n\n\nlibrary(tidyverse) # for everything\nlibrary(ggridges) # for ridgeline plots\nlibrary(ggdist) # for nice dotplots",
    "crumbs": [
      "Recitation solutions",
      "Week 7 - Data distributions solutions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#total-cupping-score-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#total-cupping-score-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "Total cupping score in Arabica and Robusta",
    "text": "Total cupping score in Arabica and Robusta\nMake 3 different visualizations that shows the distribution of total cupping score (i.e.¬†total_cup_points) across Arabica and Robusta beans. Make the plots so you think they look good.\n\nA histogram\nSince there are so few robusta observations, I decided to make the y-axes on different scales\n\ncoffee_ratings |&gt;\n  ggplot(aes(x = total_cup_points, fill = species)) +\n  geom_histogram(bins = 200) +\n  geom_vline(aes(xintercept = mean(total_cup_points)), color = \"black\") +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(xlim = c(60,100)) + # change x-axis limits\n  facet_wrap(vars(species),\n             nrow = 2, # make two rows so can align histograms top to bottom\n             scales = \"free_y\",\n             strip.position = \"top\") +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       fill = \"Species\",\n       title = \"Distribution of cupping scores across 14,000 coffee samples\",\n       subtitle = \"Note the y-axes are different between plots\",\n       caption = \"Vertical line represents the median total cupping score across all samples\")\n\n\n\n\n\n\n\n\n\n\nDensity plot\nI might like this a bit better than a histogram.\n\ncoffee_ratings |&gt;\n  ggplot(aes(x = total_cup_points, fill = species)) +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(total_cup_points)), color = \"black\") +\n  coord_cartesian(xlim = c(65,92)) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  facet_wrap(vars(species),\n             nrow = 2, # make two rows so can align histograms top to bottom\n             scales = \"free_y\") +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       fill = \"Species\",\n       title = \"Distribution of cupping scores across 14,000 coffee samples\",\n       caption = \"Vertical line represents the median total cupping score across all samples\")\n\n\n\n\n\n\n\n\n\n\nDot plot\nYou can also see here how many fewer robusta observations there are.\n\n# dot plot\ncoffee_ratings |&gt;\n  ggplot(aes(x = total_cup_points, color = species, fill = species)) +\n  geom_dots() +\n  scale_color_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  theme_ggdist() +\n  xlim(50,100) +\n  theme(legend.position = c(.18, .99),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.box.background = element_rect(size = 0.5),\n        legend.box.margin = margin(5, 5, 5, 5)) +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       color = \"Species\",\n       fill = \"Species\")\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_dotsinterval()`).\n\n\n\n\n\n\n\n\n\n\n\nRidgeline plot\n\n# ridgeline plot\ncoffee_ratings |&gt;\n  ggplot(aes(x = total_cup_points, y = species, fill = species)) +\n  stat_density_ridges(quantile_lines = TRUE,\n                      quantiles = 2,\n                      alpha = 0.5) +\n  xlim(55, 100) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  theme_ggdist() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Total cupping score (out of 100)\",\n       y = \"\",\n       title = \"Distribution of cupping scores across 14,000 coffee samples\",\n       subtitle = \"The brown color theme is very coffee-esque\")\n\nPicking joint bandwidth of 0.605\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_density_ridges()`).",
    "crumbs": [
      "Recitation solutions",
      "Week 7 - Data distributions solutions"
    ]
  },
  {
    "objectID": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "href": "modules/module3/07_distributions/07_distributions_recitation_solutions.html#individual-characteristic-cupping-scores-in-arabica-and-robusta",
    "title": "Understanding Data Distributions Recitation Solutions ‚òï",
    "section": "Individual characteristic cupping scores in Arabica and Robusta",
    "text": "Individual characteristic cupping scores in Arabica and Robusta\nMake 3 different visualizations that show the distribution of all the individual contributors (i.e., aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points) to total cupping score across Arabica and Robusta in one plot.\nWrangling:\n\n# go from wide (each coffee attribute in a separate column)\n# to long data (1 column w/ all characteristics, 1 column w/ all ratings)\ncoffee_ratings_tidy &lt;- coffee_ratings |&gt;\n  pivot_longer(cols = aroma:cupper_points, # columns from aroma to cupper_points\n               names_to = \"characteristic\",\n               values_to = \"rating\")\n\nPrepare to clean up facet strip text using the function labeller().\n\n# getting labels ready for plotting\n# what are the coffee characteristics again?\n(coffee_characteristics &lt;- unique(coffee_ratings_tidy$characteristic) |&gt;\n  sort()) # sort alphabetically, arrange won't work here bc not numeric\n\n [1] \"acidity\"       \"aftertaste\"    \"aroma\"         \"balance\"      \n [5] \"body\"          \"clean_cup\"     \"cupper_points\" \"flavor\"       \n [9] \"sweetness\"     \"uniformity\"   \n\n# create a vector of the coffee characteristic names as i want them to appear on the plot\ncoffee_labels &lt;- c(\"Acidity\",\n                   \"Aftertaste\",\n                   \"Aroma\",\n                   \"Balance\",\n                   \"Body\",\n                   \"Clean Cup\",\n                   \"Cupper Points\",\n                   \"Flavor\",\n                   \"Sweetness\",\n                   \"Uniformity\")\n\n# tell coffee_labels which original label to refer to\n# these need to be in the same order (which is why i used sort())\nnames(coffee_labels) &lt;- coffee_characteristics\n\nManaging fonts:\n\n# get fonts not default available in R\nlibrary(sysfonts) # aux packagew here fonts live\nlibrary(showtext) # package that helps use non-standard fonts\n\nLoading required package: showtextdb\n\nlibrary(ragg)\n\n# add the font Atkison Hyperlegible bc i like it\nfont_add_google(\"Atkinson Hyperlegible\")\n\n# what fonts do i have to choose from?\n# remove head() to see them all\nhead(font_info_google())\n\n         family   category num_variants                    variants num_subsets\n1       ABeeZee sans-serif            2             regular, italic           1\n2          Abel sans-serif            1                     regular           1\n3  Abhaya Libre      serif            5 regular, 500, 600, 700, 800           3\n4 Abril Fatface    display            1                     regular           2\n5      Aclonica sans-serif            1                     regular           1\n6          Acme sans-serif            1                     regular           1\n                    subsets version lastModified\n1                     latin     v20   2022-01-27\n2                     latin     v12   2020-09-10\n3 latin, latin-ext, sinhala     v11   2022-01-25\n4          latin, latin-ext     v18   2022-01-27\n5                     latin     v16   2022-01-25\n6                     latin     v17   2022-01-27\n\n# use to indicate that showtext is needed \nshowtext_auto()\n\n\nBoxplots\nThis is just ok.\n\ncoffee_ratings_tidy |&gt;\n  ggplot(aes(x = species, y = rating, fill = species)) +\n  geom_boxplot(color = \"black\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(ylim = c(5,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels), # all that work we did earlier\n             nrow = 2) + # 2 rows in the faceted plot\n  theme_classic() + \n  theme(legend.position = \"none\",\n        text = element_text(family = \"Atkinson Hyperlegible\")) + # changing font\n  labs(x = \"Coffee Species\",\n       y = \"Cupper rating (out of 10)\",\n       title = \"Distribution of cupper scores for individual coffee attributes\",\n       caption = \"Line represents the median rating per species\")\n\n\n\n\n\n\n\n\n\n\nViolin plot\n\n# violin plot\ncoffee_ratings_tidy |&gt;\n  ggplot(aes(x = species, y = rating, fill = species)) +\n  geom_violin(draw_quantiles = 0.5, color = \"black\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(ylim = c(5,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels), # all that work we did earlier\n             nrow = 2) + # 2 rows in the faceted plot\n  theme_classic() + \n  theme(legend.position = \"none\",\n        text = element_text(family = \"Atkinson Hyperlegible\")) + # changing font\n  labs(x = \"Coffee Species\",\n       y = \"Cupper rating (out of 10)\",\n       title = \"Distribution of cupper scores for individual coffee attributes\",\n       caption = \"Line represents the median rating per species\")\n\n\n\n\n\n\n\n\n\n\nDot plots\nThis is just ok.\n\ncoffee_ratings_tidy |&gt;\n  ggplot(aes(x = species, y = rating, color = species)) +\n  geom_dots(side = \"both\", layout = \"swarm\") +\n  scale_color_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(ylim = c(6,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels),\n             nrow = 2) +\n  theme_classic() + \n  theme(legend.position = \"none\",\n        text = element_text(family = \"Atkinson Hyperlegible\")) + # changing font\n  labs(x = \"Cupper rating (out of 10)\",\n       y = \"Coffee Species\",\n       title = \"Distribution of cupper scores for individual coffee attributes\")\n\n\n\n\n\n\n\n\n\n\nDensity plot\nThis one I think is my favorite.\n\ncoffee_ratings_tidy |&gt;\n  ggplot(aes(x = rating, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.8,\n                      quantile_lines = TRUE,\n                      quantiles = 2) +\n  scale_fill_manual(values = c(\"#C19A6B\", \"#5C4033\")) +\n  coord_cartesian(xlim = c(6,10)) +\n  facet_wrap(vars(characteristic),\n             labeller = labeller(characteristic = coffee_labels),\n             nrow = 2) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  labs(x = \"Cupper rating (out of 10)\",\n       y = \"Coffee Species\",\n       title = \"Distribution of cupper scores for individual coffee attributes\")",
    "crumbs": [
      "Recitation solutions",
      "Week 7 - Data distributions solutions"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html",
    "title": "Visualizing Correlations Recitation",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#introduction",
    "title": "Visualizing Correlations Recitation",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-measures-of-blood-pressure",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-measures-of-blood-pressure",
    "title": "Visualizing Correlations Recitation",
    "section": "1. How correlated are different measures of blood pressure?",
    "text": "1. How correlated are different measures of blood pressure?\nIn the NHANES dataset, there are 3 measurements for each systolic (the first/top number) and diastolic blood (the second/bottom number) pressure, and an average for each. How reproducible is each type of blood pressure measurement over the 3 samplings? Make visualizations to convey your findings.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "href": "modules/module3/08_correlations/08_correlations_recitation.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "title": "Visualizing Correlations Recitation",
    "section": "2. How correlated are different physical measurements, health, and lifestyle variables?",
    "text": "2. How correlated are different physical measurements, health, and lifestyle variables?\nIn the NHANES dataset, there are data for subject BMI, Pulse, BPSysAve, BPDiaAve, TotalChol.\nCreate a series of plots/plot to show the relationship between these variables with each other.",
    "crumbs": [
      "Module 3 - Data exploration",
      "Week 8 - Correlations"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nIf you wanted to make a correlation plot for all variables below.\n\nNHANES_trimmed &lt;- NHANES |&gt;\n  select(Age, BMI, Pulse, starts_with(\"BP\"), TotChol) |&gt;\n  drop_na()\n\nNHANES_cor &lt;- cor(NHANES_trimmed)",
    "crumbs": [
      "Recitation solutions",
      "Week 8 - Correlations solutions"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#introduction",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#introduction",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "",
    "text": "We will be using some data collection from the National Health and Nutrition Examination Survey which collects data to assess the health and nutritional status of people in the United States. The data from 2009-2012 has been compiled in an R package called NHANES.\n\n# install.packages(\"NHANES\")\nlibrary(NHANES)\n\n# functionality and correlation packages\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(reshape2)\nlibrary(scales)\n\nknitr::kable(head(NHANES))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nIf you wanted to make a correlation plot for all variables below.\n\nNHANES_trimmed &lt;- NHANES |&gt;\n  select(Age, BMI, Pulse, starts_with(\"BP\"), TotChol) |&gt;\n  drop_na()\n\nNHANES_cor &lt;- cor(NHANES_trimmed)",
    "crumbs": [
      "Recitation solutions",
      "Week 8 - Correlations solutions"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-measures-of-blood-pressure",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-measures-of-blood-pressure",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "1. How correlated are different measures of blood pressure?",
    "text": "1. How correlated are different measures of blood pressure?\nIn the NHANES dataset, there are 3 measurements for each systolic (the first/top number) and diastolic blood (the second/bottom number) pressure. How reproducible is each type of blood pressure measurement over the 3 samplings? Make visualizations to convey your findings.\n\nWrangling, creating two dataframes\n\nIncludes the 4 measures for systolic BP BPSysAve, BPSys1, BPSys2, BPSys3\nIncludes the 4 measures for diastolic BP BPDiaAve, BPDia1, BPDia2, BPDia3\n\n\n# create df with all of the BP measurements\n# remove missing values\nNHANES_BP &lt;- NHANES |&gt;\n  select(starts_with(\"BP\")) |&gt;\n  drop_na()\n\n# create df with all systolic data\nNHANES_systolic &lt;- NHANES_BP |&gt;\n  select(contains(\"Sys\"))\n\n# create df with all diastolic data\nNHANES_diastolic &lt;- NHANES_BP |&gt;\n  select(contains(\"Dia\"))\n\n\n\nLooking at relationships using scatteplots\nWe can look quickly at the relationship betwen all the diastolic BP measurements, and all of the systolic BP measurements using ggpairs().\n\nNHANES_diastolic |&gt;\n  ggpairs(title = \"Diastolic Blood Pressure Relationships\")\n\n\n\n\n\n\n\n\nFrom the diastolic data, we can see some values that are zero. Those are biologically implausible so I am going to elect to remove those observations.\n\nNHANES_diastolic_no0 &lt;- NHANES_diastolic |&gt;\n  filter(BPDiaAve &gt; 0 & BPDia1 &gt; 0 & BPDia2 &gt; 0 & BPDia3 &gt; 0)\n\n# how many observations are there?\nnrow(NHANES_diastolic)\n\n[1] 7971\n\n# how many obesrvations after removing zero diastolic\nnrow(NHANES_diastolic_no0)\n\n[1] 7803\n\n# how many observations did we remove?\nnrow(NHANES_diastolic) - nrow(NHANES_diastolic_no0)\n\n[1] 168\n\n\nTry again now that we‚Äôve removed diastolic BP values that are zero.\n\nNHANES_diastolic_no0 |&gt;\n  ggpairs(title = \"Diastolic Blood Pressure Relationships\")\n\n\n\n\n\n\n\n\nThis looks better.\n\nNHANES_systolic |&gt;\n  ggpairs(title = \"Systolic Blood Pressure Relationships\")\n\n\n\n\n\n\n\n\n\n\nRun correlation analysis with cor() and rcorr()\n\n# run systolic correlation analysis\nNHANES_sys_cor &lt;- cor(NHANES_systolic)\n\n# could also use rcorr()\nNHANES_sys_rcorr &lt;- rcorr(as.matrix(NHANES_systolic))\n\n# run diastolic correlation analysis\nNHANES_dia_cor &lt;- cor(NHANES_diastolic_no0)\n\n# could also use rcorr()\nNHANES_dia_rcorr &lt;- rcorr(as.matrix(NHANES_diastolic_no0))\n\n\n\nPrepare to plot with corrplot()\n\n# create a vector of the systolic names for labeling\nsys_labels &lt;- c(\"Systolic BP, Average\",\n                \"Systolic BP 1\",\n                \"Systolic BP 2\",\n                \"Systolic BP 3\")\n\ndia_labels &lt;- c(\"Diastolic BP, Average\",\n                \"Diastolic BP 1\",\n                \"Diastolic BP 2\",\n                \"Diastolic BP 3\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(NHANES_sys_rcorr$r) &lt;- sys_labels\nrownames(NHANES_sys_rcorr$r) &lt;- sys_labels\ncolnames(NHANES_dia_rcorr$r) &lt;- dia_labels\nrownames(NHANES_dia_rcorr$r) &lt;- dia_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(NHANES_sys_rcorr$P) &lt;- sys_labels\nrownames(NHANES_sys_rcorr$P) &lt;- sys_labels\ncolnames(NHANES_dia_rcorr$P) &lt;- dia_labels\nrownames(NHANES_dia_rcorr$P) &lt;- dia_labels\n\nPlot with corrplot().\n\ncorrplot(NHANES_sys_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = NHANES_sys_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"white\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 1.0, # size of correlation font\n         col = colorRampPalette(c(\"#d8b365\", \"#f5f5f5\", \"#5ab4ac\"))(100)) # change colors to be colorblind friendly\n\n\n\n\n\n\n\n\n\ncorrplot(NHANES_dia_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = NHANES_dia_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"white\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 1.0, # size of correlation font\n         col = colorRampPalette(c(\"#d8b365\", \"#f5f5f5\", \"#5ab4ac\"))(100)) # change colors to be colorblind friendly\n\n\n\n\n\n\n\n\n\n\nPlot with ggcorrplot()\n\nggcorrplot(NHANES_sys_cor)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the ggcorrplot package.\n  Please report the issue at &lt;https://github.com/kassambara/ggcorrplot/issues&gt;.\n\n\n\n\n\n\n\n\n\n\nggcorrplot(NHANES_dia_cor)\n\n\n\n\n\n\n\n\nAll are so highly correlated just looks red.\nCan try adjusting the scale.\n\nggcorrplot(NHANES_sys_cor) +\n  scale_fill_gradient2(limit = c(0.8,1), # set limits for corr range\n                       low = \"#e9a3c9\", mid = \"#f7f7f7\", high =  \"#a1d76a\", # pick colors\n                       midpoint = 0.9) + # set midpoint\n  scale_x_discrete(labels = sys_labels) + # change x-axis labels\n  scale_y_discrete(labels = sys_labels) + # change y-axis labels\n  labs(fill = \"Correlation \\ncoefficient\",\n       title = \"Correlations between measurements of systolic \\nblood pressure in NHANES data\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\nggcorrplot(NHANES_dia_cor) +\n  scale_fill_gradient2(limit = c(0.8,1), # set limits for corr range\n                       low = \"#e9a3c9\", mid = \"#f7f7f7\", high =  \"#a1d76a\", # pick colors\n                       midpoint = 0.9) + # set midpoint\n  scale_x_discrete(labels = dia_labels) + # change x-axis labels\n  scale_y_discrete(labels = dia_labels) + # change y-axis labels\n  labs(fill = \"Correlation \\ncoefficient\",\n       title = \"Correlations between measurements of diastolic \\nblood pressure in NHANES data\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\nPrepare to plot with melt() and ggplot()\nCreate a lower triangle object to plot.\n\n# \"save as\"\nsys_lower &lt;- NHANES_sys_cor\ndia_lower &lt;- NHANES_dia_cor\n\n# use function upper.tri() and set the upper triangle all to NA\n# then we can keep only the lower triangle\nsys_lower[upper.tri(sys_lower)] &lt;- NA\ndia_lower[upper.tri(dia_lower)] &lt;- NA\n\n# melt to go back to long format\nmelted_sys_lower &lt;- melt(sys_lower, na.rm = TRUE)\nmelted_dia_lower &lt;- melt(dia_lower, na.rm = TRUE)\n\n# did it work?\nhead(melted_sys_lower) \n\n      Var1     Var2     value\n1 BPSysAve BPSysAve 1.0000000\n2   BPSys1 BPSysAve 0.9526899\n3   BPSys2 BPSysAve 0.9881045\n4   BPSys3 BPSysAve 0.9876269\n6   BPSys1   BPSys1 1.0000000\n7   BPSys2   BPSys1 0.9468706\n\nhead(melted_dia_lower) \n\n      Var1     Var2     value\n1 BPDiaAve BPDiaAve 1.0000000\n2   BPDia1 BPDiaAve 0.9091983\n3   BPDia2 BPDiaAve 0.9769738\n4   BPDia3 BPDiaAve 0.9770299\n6   BPDia1   BPDia1 1.0000000\n7   BPDia2   BPDia1 0.8979354\n\n\nPlot systolic\n\n# create a vector of the systolic names for labeling\nsys_labels &lt;- c(\"Systolic BP, Average\",\n                \"Systolic BP 1\",\n                \"Systolic BP 2\",\n                \"Systolic BP 3\")\n\nmelted_sys_lower |&gt;\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#ef8a62\",\n                       mid = \"#f7f7f7\",\n                       high = \"#67a9cf\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = sys_labels) +\n  scale_y_discrete(labels = sys_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y = \"\",\n       title = \"Correlation measures of systolic blood pressure at 3 times\",\n       subtitle = \"Data collected from NHANES 2009-2012\",\n       caption = \"Number presents correlation coefficient \\nAll correlations are statistically significant (p &lt; 0.05)\")\n\n\n\n\n\n\n\n\nPlot diastolic\n\n# create a vector of the systolic names for labeling\ndia_labels &lt;- c(\"Diastolic BP, Average\",\n                \"Diastolic BP 1\",\n                \"Diastolic BP 2\",\n                \"Diastolic BP 3\")\n\nmelted_dia_lower |&gt;\n  ggplot(aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"black\") +\n  scale_fill_gradient2(low = \"#f1a340\",\n                       mid = \"#f7f7f7\",\n                       high = \"#998ec3\",\n                       limits = c(-1, 1)) +\n  scale_x_discrete(labels = dia_labels) +\n  scale_y_discrete(labels = dia_labels) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        legend.justification = c(1, 0),\n        legend.position = c(0.5, 0.7),\n        legend.direction = \"horizontal\") +\n  labs(fill = \"Correlation \\ncoefficient\",\n       x = \"\",\n       y =\"\",\n       title = \"Correlation measures of diastolic blood pressure at 3 times\",\n       subtitle = \"Data collected from NHANES 2009-2012\",\n       caption = \"Number presents correlation coefficient \\nAll correlations are statistically significant (p &lt; 0.05)\")",
    "crumbs": [
      "Recitation solutions",
      "Week 8 - Correlations solutions"
    ]
  },
  {
    "objectID": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "href": "modules/module3/08_correlations/08_correlations_recitation_solutions.html#how-correlated-are-different-physical-measurements-health-and-lifestyle-variables",
    "title": "Visualizing Correlations Recitation Solutions",
    "section": "2. How correlated are different physical measurements, health, and lifestyle variables?",
    "text": "2. How correlated are different physical measurements, health, and lifestyle variables?\nIn the NHANES dataset, there are data for subject BMI, Pulse, BPSysAve, BPDiaAve, TotalChol.\nCreate a series of plots to show the relationship between these variables with each other.\n\nWrangle\nCreate a dataframe that includes only the variables we want to correlate, and drop the observations with missing values.\n\nnhanes_trimmed &lt;- NHANES |&gt;\n  select(BMI, Pulse, BPSysAve, BPDiaAve, TotChol) |&gt;\n  drop_na()\n\n\n\nVisualize with ggpairs()\nHere, we don‚Äôt have to specify columns since we‚Äôre using them all.\n\nnhanes_trimmed |&gt;\n    ggpairs(aes(alpha = 0.01), # note alpha inside aes which is weird idk why\n            lower = list(continuous = \"smooth\"),\n            columnLabels = c(\"BMI\", \"Pulse\", \"Systolic BP\", \"Diastolic BP\", \"Total Cholesterol\"))\n\n\n\n\n\n\n\n\n\n\nCreate a correlation plot with corrplot()\nFirst we will make our trimmed df a matrix.\n\n# convert into a matrix as this is what corrplot takes\nnhanes_trimmed_matrix &lt;- nhanes_trimmed |&gt;\n  as.matrix() \n\nnhanes_rcorr &lt;- rcorr(nhanes_trimmed_matrix, type = \"pearson\")\n\n# correlation matrix\nnhanes_rcorr$r\n\n                BMI         Pulse    BPSysAve   BPDiaAve       TotChol\nBMI      1.00000000  1.514764e-02  0.24526330 0.23705522  1.213842e-01\nPulse    0.01514764  1.000000e+00 -0.09670785 0.01217557 -9.486773e-05\nBPSysAve 0.24526330 -9.670785e-02  1.00000000 0.40476431  2.174280e-01\nBPDiaAve 0.23705522  1.217557e-02  0.40476431 1.00000000  2.620035e-01\nTotChol  0.12138419 -9.486773e-05  0.21742801 0.26200353  1.000000e+00\n\n# pvalue matrix\nnhanes_rcorr$P\n\n               BMI     Pulse BPSysAve  BPDiaAve  TotChol\nBMI             NA 0.1772467        0 0.0000000 0.000000\nPulse    0.1772467        NA        0 0.2781342 0.993258\nBPSysAve 0.0000000 0.0000000       NA 0.0000000 0.000000\nBPDiaAve 0.0000000 0.2781342        0        NA 0.000000\nTotChol  0.0000000 0.9932580        0 0.0000000       NA\n\n\nWrangle labels\n\n# create a vector of how i want the labels to look\nnhanes_labels &lt;- c(\"BMI\",\n                   \"Pulse\",\n                   \"Systolic \\nBlood Pressure\",\n                   \"Diastolic \\nBlood Pressure\",\n                   \"Total Cholesterol\")\n\n# change row and column names of the correlation matrix\n# so they are how we want them to be plotted\ncolnames(nhanes_rcorr$r) &lt;- nhanes_labels\nrownames(nhanes_rcorr$r) &lt;- nhanes_labels\n\n# change row and column names of the pvalue matrix\n# so they are how we want them to be plotted\ncolnames(nhanes_rcorr$P) &lt;- nhanes_labels\nrownames(nhanes_rcorr$P) &lt;- nhanes_labels\n\nMake the correlation plot. The numbers are the correlation coefficients for relationships that are significant based on our criteria.\n\ncorrplot(nhanes_rcorr$r, # the correlation matrix\n         type = \"lower\", # lower triangle\n         tl.col = \"black\", # axis labels are black\n         p.mat  = nhanes_rcorr$P, # pvalue matrix\n         sig.level = 0.05, # how sig does a cor need to be to be included\n         insig = \"blank\", # do not display insignificant correlations\n         addCoef.col = \"black\", # display correlations in black\n         diag = FALSE, # don't show the diagonal (because this is all 1)\n         number.cex = 0.6) # size of correlation font",
    "crumbs": [
      "Recitation solutions",
      "Week 8 - Correlations solutions"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\nlibrary(tidyverse)\n\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class.\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years\n\n\n# read in happiness data from your computer\n# mine has the path below since i have a subfolder called data where\n# the happiness data is living\nhappiness &lt;- read_csv(\"data/hapiscore_whr.csv\")\n\nRows: 163 Columns: 19\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (1): country\ndbl (18): 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in life expectancy data from your computer\nlife_expectancy &lt;- read_csv(\"data/life_expectancy.csv\")\n\nRows: 195 Columns: 302\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (1): country\ndbl (301): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Recitation solutions",
      "Week 4 - Wrangling solutions"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#introduction",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#introduction",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "",
    "text": "Today you are going to be practicing what you learned in the wrangling lesson. The more you practice modifying your data the easier it becomes. Remember, there are many ways to accomplish the same outcome. In the recitation solutions, I will show you a few different ways to answer the prompts and you can see how they differ, and use the ones that resonate with you.\n\nlibrary(tidyverse)\n\n\n\nTo practice, we will be using some data I have extracted from Gapminder. I am linking to two files that you can download to your computer, and then read them in like we learned in class.\n\nData on the happiness index for many countries for many years\nData on the life expectancy for many countries for many years\n\n\n# read in happiness data from your computer\n# mine has the path below since i have a subfolder called data where\n# the happiness data is living\nhappiness &lt;- read_csv(\"data/hapiscore_whr.csv\")\n\nRows: 163 Columns: 19\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (1): country\ndbl (18): 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in life expectancy data from your computer\nlife_expectancy &lt;- read_csv(\"data/life_expectancy.csv\")\n\nRows: 195 Columns: 302\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (1): country\ndbl (301): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Recitation solutions",
      "Week 4 - Wrangling solutions"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#explore-your-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#explore-your-data",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Explore your data",
    "text": "Explore your data\nWrite some code that lets you explore that is in these two datasets.\n\n# see data structure with glimpse\nglimpse(happiness)\n\nRows: 163\nColumns: 19\n$ country &lt;chr&gt; \"Afghanistan\", \"Angola\", \"Albania\", \"United Arab Emirates\", \"A‚Ä¶\n$ `2005`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 73.4, NA, NA, NA, 72.6, NA, NA, NA, NA‚Ä¶\n$ `2006`  &lt;dbl&gt; NA, NA, NA, 67.3, 63.1, 42.9, NA, 71.2, 47.3, NA, NA, 33.3, 38‚Ä¶\n$ `2007`  &lt;dbl&gt; NA, NA, 46.3, NA, 60.7, 48.8, 72.8, NA, 45.7, NA, 72.2, NA, 40‚Ä¶\n$ `2008`  &lt;dbl&gt; 37.2, NA, NA, NA, 59.6, 46.5, 72.5, 71.8, 48.2, 35.6, 71.2, 36‚Ä¶\n$ `2009`  &lt;dbl&gt; 44.0, NA, 54.9, 68.7, 64.2, 41.8, NA, NA, 45.7, 37.9, NA, NA, ‚Ä¶\n$ `2010`  &lt;dbl&gt; 47.6, NA, 52.7, 71.0, 64.4, 43.7, 74.5, 73.0, 42.2, NA, 68.5, ‚Ä¶\n$ `2011`  &lt;dbl&gt; 38.3, 55.9, 58.7, 71.2, 67.8, 42.6, 74.1, 74.7, 46.8, 37.1, 71‚Ä¶\n$ `2012`  &lt;dbl&gt; 37.8, 43.6, 55.1, 72.2, 64.7, 43.2, 72.0, 74.0, 49.1, NA, 69.3‚Ä¶\n$ `2013`  &lt;dbl&gt; 35.7, 39.4, 45.5, 66.2, 65.8, 42.8, 73.6, 75.0, 54.8, NA, 71.0‚Ä¶\n$ `2014`  &lt;dbl&gt; 31.3, 38.0, 48.1, 65.4, 66.7, 44.5, 72.9, 69.5, 52.5, 29.1, 68‚Ä¶\n$ `2015`  &lt;dbl&gt; 39.8, NA, 46.1, 65.7, 67.0, 43.5, 73.1, 70.8, 51.5, NA, 69.0, ‚Ä¶\n$ `2016`  &lt;dbl&gt; 42.2, NA, 45.1, 68.3, 64.3, 43.3, 72.5, 70.5, 53.0, NA, 69.5, ‚Ä¶\n$ `2017`  &lt;dbl&gt; 26.6, NA, 46.4, 70.4, 60.4, 42.9, 72.6, 72.9, 51.5, NA, 69.3, ‚Ä¶\n$ `2018`  &lt;dbl&gt; 26.9, NA, 50.0, 66.0, 57.9, 50.6, 71.8, 74.0, 51.7, 37.8, 68.9‚Ä¶\n$ `2019`  &lt;dbl&gt; 23.8, NA, 50.0, 67.1, 60.9, 54.9, 72.3, 72.0, 51.7, NA, 67.7, ‚Ä¶\n$ `2020`  &lt;dbl&gt; NA, NA, 53.6, 64.6, 59.0, NA, 71.4, 72.1, NA, NA, 68.4, 44.1, ‚Ä¶\n$ `2021`  &lt;dbl&gt; 24.4, NA, 52.5, 67.3, 59.1, 53.0, 71.1, 70.8, NA, NA, 68.8, 44‚Ä¶\n$ `2022`  &lt;dbl&gt; 18.6, NA, 52.8, 65.7, 60.2, 53.4, 71.0, 71.0, NA, NA, 68.6, 43‚Ä¶\n\n# look at all columns and first 6 rows with head\nhead(happiness)\n\n# A tibble: 6 √ó 19\n  country  `2005` `2006` `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014`\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghani‚Ä¶     NA   NA     NA     37.2   44     47.6   38.3   37.8   35.7   31.3\n2 Angola       NA   NA     NA     NA     NA     NA     55.9   43.6   39.4   38  \n3 Albania      NA   NA     46.3   NA     54.9   52.7   58.7   55.1   45.5   48.1\n4 United ‚Ä¶     NA   67.3   NA     NA     68.7   71     71.2   72.2   66.2   65.4\n5 Argenti‚Ä¶     NA   63.1   60.7   59.6   64.2   64.4   67.8   64.7   65.8   66.7\n6 Armenia      NA   42.9   48.8   46.5   41.8   43.7   42.6   43.2   42.8   44.5\n# ‚Ñπ 8 more variables: `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n# the code below will open the file for you to look through in R\n# i've commented this out so it doesn't print in my .html\n# but i can easily uncomment if i want to View(happiness)\n# View(happiness)\n\n\n# see data structure with glimpse\nglimpse(life_expectancy)\n\nRows: 195\nColumns: 302\n$ country &lt;chr&gt; \"Afghanistan\", \"Angola\", \"Albania\", \"Andorra\", \"United Arab Em‚Ä¶\n$ `1800`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1801`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1802`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1803`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1804`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1805`  &lt;dbl&gt; 28.2, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1806`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1807`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1808`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1809`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1810`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1811`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1812`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1813`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1814`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1815`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1816`  &lt;dbl&gt; 28.1, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1817`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1818`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1819`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1820`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1821`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1822`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1823`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1824`  &lt;dbl&gt; 28.0, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1825`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1826`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1827`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1828`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1829`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1830`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1831`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1832`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1833`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1834`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1835`  &lt;dbl&gt; 27.9, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1836`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1837`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1838`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1839`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1840`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1841`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1842`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1843`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1844`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1845`  &lt;dbl&gt; 27.8, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1846`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1847`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1848`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1849`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1850`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1851`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1852`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1853`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1854`  &lt;dbl&gt; 27.7, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1855`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1856`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1857`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1858`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1859`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1860`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1861`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1862`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1863`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 34.0, 33.5, 34.0, 34.4, 29.2‚Ä¶\n$ `1864`  &lt;dbl&gt; 27.6, 27.0, 35.4, NA, 30.7, 33.2, 33.5, 33.5, 34.0, 34.4, 28.6‚Ä¶\n$ `1865`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 33.0, 33.5, 34.0, 34.4, 28.1‚Ä¶\n$ `1866`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 32.4, 33.5, 34.0, 34.4, 27.6‚Ä¶\n$ `1867`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.9, 33.5, 34.0, 34.4, 27.1‚Ä¶\n$ `1868`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.4, 33.5, 34.0, 34.4, 26.6‚Ä¶\n$ `1869`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.4, 33.5, 34.0, 34.4, 26.6‚Ä¶\n$ `1870`  &lt;dbl&gt; 27.5, 27.0, 35.4, NA, 30.7, 33.2, 31.5, 33.5, 34.0, 34.4, 26.6‚Ä¶\n$ `1871`  &lt;dbl&gt; 27.7, 27.2, 35.4, NA, 30.9, 33.2, 31.5, 33.5, 34.6, 34.5, 26.6‚Ä¶\n$ `1872`  &lt;dbl&gt; 27.9, 27.4, 35.4, NA, 31.0, 33.2, 31.6, 33.5, 35.1, 34.5, 26.5‚Ä¶\n$ `1873`  &lt;dbl&gt; 28.1, 27.6, 35.4, NA, 31.2, 33.2, 31.6, 33.5, 35.6, 34.6, 26.4‚Ä¶\n$ `1874`  &lt;dbl&gt; 28.3, 27.7, 35.3, NA, 31.3, 33.3, 31.8, 33.5, 36.2, 34.6, 26.5‚Ä¶\n$ `1875`  &lt;dbl&gt; 28.5, 27.9, 35.3, NA, 31.5, 33.3, 32.0, 33.6, 36.7, 34.7, 26.6‚Ä¶\n$ `1876`  &lt;dbl&gt; 28.7, 28.1, 35.3, NA, 31.6, 33.3, 32.2, 33.6, 37.2, 34.7, 26.7‚Ä¶\n$ `1877`  &lt;dbl&gt; 28.9, 28.3, 35.3, NA, 31.8, 33.3, 32.4, 33.6, 37.8, 34.8, 26.8‚Ä¶\n$ `1878`  &lt;dbl&gt; 29.1, 28.5, 35.3, NA, 32.0, 33.2, 32.6, 33.6, 38.3, 34.8, 26.8‚Ä¶\n$ `1879`  &lt;dbl&gt; 29.3, 28.7, 35.3, NA, 32.1, 33.2, 32.8, 33.6, 38.8, 34.9, 26.9‚Ä¶\n$ `1880`  &lt;dbl&gt; 29.4, 28.9, 35.2, NA, 32.3, 33.2, 33.0, 33.6, 39.4, 34.9, 27.0‚Ä¶\n$ `1881`  &lt;dbl&gt; 29.6, 29.1, 35.2, NA, 32.4, 33.1, 33.2, 33.6, 39.9, 35.0, 27.0‚Ä¶\n$ `1882`  &lt;dbl&gt; 29.8, 29.3, 35.2, NA, 32.6, 33.0, 33.4, 33.6, 40.4, 35.2, 27.1‚Ä¶\n$ `1883`  &lt;dbl&gt; 30.0, 29.4, 35.2, NA, 32.8, 33.0, 33.6, 33.6, 41.0, 35.5, 27.2‚Ä¶\n$ `1884`  &lt;dbl&gt; 30.2, 29.6, 35.2, NA, 32.9, 32.9, 33.8, 33.6, 41.5, 35.7, 27.3‚Ä¶\n$ `1885`  &lt;dbl&gt; 30.4, 29.8, 35.2, NA, 33.1, 32.8, 34.0, 33.6, 42.0, 35.9, 27.3‚Ä¶\n$ `1886`  &lt;dbl&gt; 30.6, 30.0, 35.1, NA, 33.2, 33.1, 34.2, 33.6, 42.6, 36.1, 27.4‚Ä¶\n$ `1887`  &lt;dbl&gt; 30.8, 30.2, 35.1, NA, 33.4, 33.4, 34.4, 33.6, 43.1, 36.4, 27.4‚Ä¶\n$ `1888`  &lt;dbl&gt; 31.0, 30.4, 35.1, NA, 33.5, 33.7, 34.6, 33.6, 43.6, 36.6, 27.5‚Ä¶\n$ `1889`  &lt;dbl&gt; 31.2, 30.6, 35.1, NA, 33.7, 34.0, 34.8, 33.6, 44.2, 36.8, 27.6‚Ä¶\n$ `1890`  &lt;dbl&gt; 31.4, 30.8, 35.1, NA, 33.9, 34.3, 35.0, 33.6, 44.7, 37.1, 27.6‚Ä¶\n$ `1891`  &lt;dbl&gt; 31.6, 30.9, 35.1, NA, 34.0, 34.1, 35.2, 33.6, 45.2, 37.3, 27.7‚Ä¶\n$ `1892`  &lt;dbl&gt; 31.8, 31.1, 35.0, NA, 34.2, 34.0, 35.4, 33.6, 45.8, 37.8, 27.7‚Ä¶\n$ `1893`  &lt;dbl&gt; 32.0, 31.3, 35.0, NA, 34.3, 33.9, 35.6, 33.6, 46.3, 38.2, 27.8‚Ä¶\n$ `1894`  &lt;dbl&gt; 32.2, 31.5, 35.0, NA, 34.5, 33.8, 35.8, 33.6, 46.8, 38.7, 27.8‚Ä¶\n$ `1895`  &lt;dbl&gt; 32.4, 31.7, 35.0, NA, 34.6, 33.6, 36.0, 33.6, 47.4, 39.2, 27.9‚Ä¶\n$ `1896`  &lt;dbl&gt; 32.5, 31.9, 35.0, NA, 34.8, 34.3, 36.2, 33.6, 47.9, 39.6, 27.9‚Ä¶\n$ `1897`  &lt;dbl&gt; 32.7, 32.1, 35.0, NA, 35.0, 35.1, 36.4, 33.6, 48.4, 40.1, 28.0‚Ä¶\n$ `1898`  &lt;dbl&gt; 32.9, 32.3, 35.0, NA, 35.1, 35.8, 36.2, 33.7, 49.0, 40.6, 27.7‚Ä¶\n$ `1899`  &lt;dbl&gt; 33.1, 32.5, 34.9, NA, 35.3, 36.5, 36.1, 33.7, 49.5, 41.0, 27.4‚Ä¶\n$ `1900`  &lt;dbl&gt; 33.3, 32.6, 34.9, NA, 35.4, 37.2, 35.9, 33.7, 50.0, 41.5, 27.1‚Ä¶\n$ `1901`  &lt;dbl&gt; 33.5, 32.8, 34.9, NA, 35.6, 37.8, 36.1, 33.7, 50.6, 42.0, 27.2‚Ä¶\n$ `1902`  &lt;dbl&gt; 33.7, 33.0, 34.9, NA, 35.7, 38.4, 36.4, 33.7, 51.1, 41.0, 27.3‚Ä¶\n$ `1903`  &lt;dbl&gt; 33.9, 33.2, 34.9, NA, 35.9, 39.0, 36.6, 33.7, 51.6, 40.1, 27.4‚Ä¶\n$ `1904`  &lt;dbl&gt; 34.1, 33.4, 34.9, NA, 36.1, 39.6, 36.9, 33.7, 52.2, 40.7, 27.4‚Ä¶\n$ `1905`  &lt;dbl&gt; 34.3, 33.6, 34.8, NA, 36.2, 40.2, 37.1, 33.7, 52.7, 41.3, 27.5‚Ä¶\n$ `1906`  &lt;dbl&gt; 34.5, 33.8, 34.8, NA, 36.4, 41.0, 37.4, 33.7, 53.2, 42.0, 27.6‚Ä¶\n$ `1907`  &lt;dbl&gt; 34.7, 34.0, 34.8, NA, 36.5, 41.8, 37.6, 33.7, 53.8, 42.6, 27.7‚Ä¶\n$ `1908`  &lt;dbl&gt; 34.9, 34.2, 34.8, NA, 36.7, 42.6, 37.9, 33.7, 54.3, 43.2, 27.8‚Ä¶\n$ `1909`  &lt;dbl&gt; 35.0, 34.4, 34.8, NA, 36.8, 43.4, 38.1, 33.7, 54.8, 43.8, 27.8‚Ä¶\n$ `1910`  &lt;dbl&gt; 35.2, 34.5, 34.8, NA, 37.0, 44.2, 38.4, 33.7, 55.4, 44.5, 27.9‚Ä¶\n$ `1911`  &lt;dbl&gt; 35.4, 34.7, 34.7, NA, 37.2, 44.7, 41.4, 33.7, 55.9, 45.1, 30.2‚Ä¶\n$ `1912`  &lt;dbl&gt; 35.6, 34.9, 34.7, NA, 37.3, 45.3, 41.8, 33.7, 56.4, 45.6, 30.4‚Ä¶\n$ `1913`  &lt;dbl&gt; 35.8, 35.1, 34.7, NA, 37.5, 45.9, 39.6, 33.7, 57.0, 46.2, 28.4‚Ä¶\n$ `1914`  &lt;dbl&gt; 36.0, 35.3, 34.7, NA, 37.6, 46.4, 39.2, 33.7, 57.5, 46.8, 28.0‚Ä¶\n$ `1915`  &lt;dbl&gt; 36.2, 35.5, 34.7, NA, 37.8, 47.0, 38.8, 33.7, 58.0, 47.3, 27.5‚Ä¶\n$ `1916`  &lt;dbl&gt; 36.4, 35.7, 34.7, NA, 38.0, 47.8, 38.8, 33.7, 58.6, 47.9, 27.4‚Ä¶\n$ `1917`  &lt;dbl&gt; 36.6, 35.9, 34.6, NA, 38.1, 48.7, 35.8, 33.7, 59.1, 48.5, 24.8‚Ä¶\n$ `1918`  &lt;dbl&gt; 9.59, 13.90, 19.00, NA, 31.70, 42.50, 27.00, 21.90, 55.00, 32.‚Ä¶\n$ `1919`  &lt;dbl&gt; 36.9, 36.2, 34.6, NA, 38.4, 50.3, 37.0, 33.8, 60.2, 49.6, 25.4‚Ä¶\n$ `1920`  &lt;dbl&gt; 37.1, 36.4, 34.6, NA, 38.6, 51.2, 28.0, 33.8, 60.7, 50.2, 23.6‚Ä¶\n$ `1921`  &lt;dbl&gt; 37.3, 36.6, 34.6, NA, 38.7, 51.7, 38.0, 33.8, 61.3, 50.7, 25.9‚Ä¶\n$ `1922`  &lt;dbl&gt; 37.5, 36.8, 34.6, NA, 38.9, 52.2, 39.0, 34.6, 63.1, 51.3, 26.5‚Ä¶\n$ `1923`  &lt;dbl&gt; 37.7, 37.0, 34.5, NA, 39.0, 52.7, 39.9, 35.4, 62.0, 51.9, 27.2‚Ä¶\n$ `1924`  &lt;dbl&gt; 37.9, 37.2, 34.5, NA, 39.2, 53.2, 42.4, 36.3, 62.8, 52.4, 29.0‚Ä¶\n$ `1925`  &lt;dbl&gt; 38.1, 37.4, 34.5, NA, 39.4, 53.7, 41.5, 37.1, 63.5, 53.0, 28.1‚Ä¶\n$ `1926`  &lt;dbl&gt; 38.3, 37.6, 34.5, NA, 39.5, 54.1, 44.7, 38.0, 63.2, 53.6, 30.4‚Ä¶\n$ `1927`  &lt;dbl&gt; 38.4, 37.8, 34.5, NA, 39.7, 54.4, 44.0, 38.8, 63.2, 54.1, 29.7‚Ä¶\n$ `1928`  &lt;dbl&gt; 38.6, 37.9, 34.5, NA, 39.8, 54.8, 45.5, 39.6, 63.2, 54.7, 30.7‚Ä¶\n$ `1929`  &lt;dbl&gt; 38.8, 38.1, 34.5, NA, 40.0, 55.2, 44.3, 40.5, 63.4, 55.3, 29.6‚Ä¶\n$ `1930`  &lt;dbl&gt; 39.0, 38.3, 35.3, NA, 40.1, 55.5, 43.6, 41.3, 65.2, 55.8, 28.9‚Ä¶\n$ `1931`  &lt;dbl&gt; 39.2, 38.5, 36.1, NA, 40.3, 55.6, 42.0, 42.1, 65.7, 56.4, 27.4‚Ä¶\n$ `1932`  &lt;dbl&gt; 39.4, 38.7, 37.0, NA, 40.5, 55.6, 39.5, 43.0, 66.0, 56.7, 25.4‚Ä¶\n$ `1933`  &lt;dbl&gt; 39.6, 38.9, 37.9, NA, 40.6, 55.6, 33.0, 43.8, 65.8, 57.0, 19.8‚Ä¶\n$ `1934`  &lt;dbl&gt; 39.8, 39.1, 38.7, NA, 40.8, 55.6, 46.0, 44.7, 65.2, 57.3, 30.0‚Ä¶\n$ `1935`  &lt;dbl&gt; 39.9, 39.3, 39.6, NA, 40.9, 55.6, 47.5, 45.5, 65.4, 57.6, 30.9‚Ä¶\n$ `1936`  &lt;dbl&gt; 40.1, 39.5, 40.4, NA, 41.1, 56.6, 49.2, 46.3, 65.6, 57.9, 32.0‚Ä¶\n$ `1937`  &lt;dbl&gt; 40.3, 39.6, 41.3, NA, 41.3, 57.7, 48.2, 47.2, 66.1, 58.2, 31.1‚Ä¶\n$ `1938`  &lt;dbl&gt; 40.5, 39.8, 42.1, NA, 41.4, 58.7, 49.9, 48.0, 66.2, 58.5, 32.1‚Ä¶\n$ `1939`  &lt;dbl&gt; 40.7, 40.0, 41.6, NA, 41.6, 59.7, 52.3, 48.9, 66.1, 58.0, 33.6‚Ä¶\n$ `1940`  &lt;dbl&gt; 40.9, 40.2, 40.7, NA, 41.7, 60.7, 49.8, 49.7, 66.6, 57.7, 31.6‚Ä¶\n$ `1941`  &lt;dbl&gt; 41.0, 40.7, 40.1, NA, 41.9, 61.3, 27.4, 50.5, 66.5, 56.4, 21.2‚Ä¶\n$ `1942`  &lt;dbl&gt; 41.2, 41.3, 38.7, NA, 42.0, 61.9, 23.5, 51.4, 66.2, 54.0, 18.6‚Ä¶\n$ `1943`  &lt;dbl&gt; 41.4, 41.8, 35.8, NA, 42.2, 62.5, 21.1, 52.2, 66.7, 50.1, 17.1‚Ä¶\n$ `1944`  &lt;dbl&gt; 41.6, 42.3, 32.9, NA, 42.4, 63.2, 27.5, 53.1, 68.4, 39.1, 22.0‚Ä¶\n$ `1945`  &lt;dbl&gt; 41.8, 42.9, 45.4, NA, 42.5, 63.8, 35.4, 53.9, 68.8, 31.4, 28.7‚Ä¶\n$ `1946`  &lt;dbl&gt; 42.0, 43.4, 48.3, NA, 45.6, 63.7, 49.6, 54.8, 68.3, 55.9, 35.3‚Ä¶\n$ `1947`  &lt;dbl&gt; 42.2, 43.9, 49.7, NA, 48.8, 63.6, 41.5, 55.6, 69.0, 61.2, 29.6‚Ä¶\n$ `1948`  &lt;dbl&gt; 42.4, 44.5, 50.5, NA, 52.0, 63.5, 47.0, 56.5, 68.9, 63.2, 34.9‚Ä¶\n$ `1949`  &lt;dbl&gt; 42.5, 45.0, 51.4, NA, 55.2, 63.4, 47.9, 57.3, 69.5, 63.4, 36.5‚Ä¶\n$ `1950`  &lt;dbl&gt; 42.7, 45.6, 52.2, 74.6, 58.4, 63.3, 48.2, 58.1, 69.4, 64.8, 37‚Ä¶\n$ `1951`  &lt;dbl&gt; 42.9, 45.6, 53.6, 74.7, 58.5, 63.5, 49.0, 58.7, 69.2, 65.5, 38‚Ä¶\n$ `1952`  &lt;dbl&gt; 43.1, 45.6, 54.5, 74.8, 58.6, 64.2, 50.0, 59.3, 69.5, 66.7, 39‚Ä¶\n$ `1953`  &lt;dbl&gt; 43.5, 45.6, 55.4, 75.0, 58.7, 64.1, 51.1, 59.8, 69.9, 67.2, 40‚Ä¶\n$ `1954`  &lt;dbl&gt; 43.3, 45.6, 56.1, 75.1, 58.8, 64.7, 52.1, 60.3, 70.2, 67.3, 41‚Ä¶\n$ `1955`  &lt;dbl&gt; 43.9, 45.5, 56.3, 75.2, 58.9, 64.5, 53.3, 60.9, 70.3, 67.7, 42‚Ä¶\n$ `1956`  &lt;dbl&gt; 44.1, 45.7, 58.0, 75.3, 58.8, 65.2, 54.5, 61.5, 70.4, 67.8, 43‚Ä¶\n$ `1957`  &lt;dbl&gt; 44.3, 45.8, 59.3, 75.4, 59.3, 65.2, 55.7, 61.9, 70.6, 67.8, 44‚Ä¶\n$ `1958`  &lt;dbl&gt; 44.5, 45.9, 61.0, 75.5, 59.6, 65.4, 56.5, 62.6, 71.0, 68.4, 45‚Ä¶\n$ `1959`  &lt;dbl&gt; 44.7, 46.1, 61.7, 75.6, 59.7, 65.4, 58.0, 63.3, 70.9, 68.5, 46‚Ä¶\n$ `1960`  &lt;dbl&gt; 45.0, 46.3, 62.5, 75.7, 60.3, 65.3, 59.2, 63.8, 71.1, 69.0, 48‚Ä¶\n$ `1961`  &lt;dbl&gt; 45.3, 44.8, 63.3, 75.8, 60.8, 65.7, 60.4, 64.8, 71.3, 69.6, 50‚Ä¶\n$ `1962`  &lt;dbl&gt; 45.5, 45.0, 63.3, 75.9, 61.3, 65.8, 61.4, 65.5, 71.2, 69.7, 51‚Ä¶\n$ `1963`  &lt;dbl&gt; 45.7, 45.2, 63.8, 76.0, 61.6, 65.8, 62.0, 65.8, 71.2, 69.8, 53‚Ä¶\n$ `1964`  &lt;dbl&gt; 45.9, 45.4, 64.4, 76.2, 62.1, 65.8, 62.9, 66.1, 71.0, 70.0, 54‚Ä¶\n$ `1965`  &lt;dbl&gt; 46.1, 45.6, 64.8, 76.3, 62.6, 66.1, 63.6, 66.7, 71.1, 70.1, 55‚Ä¶\n$ `1966`  &lt;dbl&gt; 46.3, 45.8, 65.5, 76.4, 63.0, 66.6, 64.2, 67.1, 71.1, 70.2, 56‚Ä¶\n$ `1967`  &lt;dbl&gt; 46.5, 46.0, 66.1, 76.5, 63.4, 66.5, 64.7, 67.2, 71.1, 70.2, 57‚Ä¶\n$ `1968`  &lt;dbl&gt; 46.7, 46.2, 66.5, 76.7, 63.8, 66.0, 65.1, 67.6, 71.0, 70.3, 58‚Ä¶\n$ `1969`  &lt;dbl&gt; 46.9, 46.4, 67.1, 76.8, 64.2, 65.9, 65.3, 68.0, 71.2, 70.2, 58‚Ä¶\n$ `1970`  &lt;dbl&gt; 47.1, 46.6, 67.8, 77.0, 64.0, 66.1, 65.9, 68.3, 71.2, 70.2, 59‚Ä¶\n$ `1971`  &lt;dbl&gt; 47.3, 46.8, 68.3, 77.1, 64.9, 66.9, 66.4, 68.9, 71.4, 70.4, 61‚Ä¶\n$ `1972`  &lt;dbl&gt; 47.3, 47.0, 68.8, 77.2, 65.1, 67.3, 66.8, 69.4, 71.8, 70.7, 61‚Ä¶\n$ `1973`  &lt;dbl&gt; 47.3, 47.2, 69.3, 77.4, 65.4, 67.7, 67.2, 69.8, 71.9, 71.1, 62‚Ä¶\n$ `1974`  &lt;dbl&gt; 47.4, 47.4, 69.8, 77.5, 65.7, 67.9, 67.9, 70.0, 72.0, 71.2, 63‚Ä¶\n$ `1975`  &lt;dbl&gt; 47.5, 47.5, 70.2, 77.7, 66.0, 68.0, 68.2, 70.2, 72.5, 71.4, 64‚Ä¶\n$ `1976`  &lt;dbl&gt; 47.7, 47.5, 70.7, 77.8, 66.3, 67.0, 69.0, 70.3, 72.8, 71.8, 65‚Ä¶\n$ `1977`  &lt;dbl&gt; 47.9, 47.7, 71.1, 78.0, 66.6, 67.7, 69.5, 70.6, 73.3, 72.2, 65‚Ä¶\n$ `1978`  &lt;dbl&gt; 46.4, 47.8, 71.7, 78.1, 67.0, 69.0, 69.7, 71.0, 73.8, 72.4, 64‚Ä¶\n$ `1979`  &lt;dbl&gt; 44.7, 48.0, 71.3, 78.2, 67.3, 69.8, 70.2, 71.6, 74.2, 72.6, 65‚Ä¶\n$ `1980`  &lt;dbl&gt; 43.7, 48.1, 71.3, 78.3, 67.6, 70.2, 70.2, 72.1, 74.5, 72.8, 66‚Ä¶\n$ `1981`  &lt;dbl&gt; 44.3, 48.2, 71.3, 78.4, 68.0, 70.3, 70.5, 72.6, 74.8, 73.1, 66‚Ä¶\n$ `1982`  &lt;dbl&gt; 44.1, 48.2, 71.4, 78.5, 68.1, 70.9, 70.8, 73.2, 74.9, 73.3, 66‚Ä¶\n$ `1983`  &lt;dbl&gt; 42.3, 48.2, 71.2, 78.5, 67.9, 70.7, 70.8, 73.8, 75.3, 73.5, 66‚Ä¶\n$ `1984`  &lt;dbl&gt; 39.9, 48.4, 71.4, 78.6, 68.4, 70.8, 71.1, 73.5, 75.5, 73.9, 66‚Ä¶\n$ `1985`  &lt;dbl&gt; 42.0, 48.6, 71.9, 78.7, 68.5, 71.7, 71.3, 73.8, 75.6, 74.3, 66‚Ä¶\n$ `1986`  &lt;dbl&gt; 43.3, 48.6, 72.3, 78.8, 68.5, 72.0, 71.7, 74.0, 76.0, 74.7, 66‚Ä¶\n$ `1987`  &lt;dbl&gt; 45.9, 48.6, 72.2, 78.8, 68.5, 72.1, 71.8, 74.2, 76.2, 75.1, 66‚Ä¶\n$ `1988`  &lt;dbl&gt; 48.5, 48.6, 72.4, 78.9, 68.5, 72.1, 55.3, 74.3, 76.4, 75.5, 66‚Ä¶\n$ `1989`  &lt;dbl&gt; 52.7, 49.4, 72.5, 79.0, 68.6, 72.3, 71.0, 74.3, 76.5, 75.7, 67‚Ä¶\n$ `1990`  &lt;dbl&gt; 53.8, 49.7, 72.8, 79.0, 68.7, 72.5, 70.6, 74.0, 77.0, 76.0, 66‚Ä¶\n$ `1991`  &lt;dbl&gt; 53.8, 50.3, 72.6, 79.1, 68.7, 72.7, 70.3, 74.0, 77.4, 76.1, 66‚Ä¶\n$ `1992`  &lt;dbl&gt; 54.2, 50.3, 73.2, 79.2, 68.8, 72.8, 69.4, 74.0, 77.6, 76.3, 64‚Ä¶\n$ `1993`  &lt;dbl&gt; 54.4, 49.0, 73.8, 79.3, 68.8, 73.0, 69.0, 73.4, 77.9, 76.5, 64‚Ä¶\n$ `1994`  &lt;dbl&gt; 53.9, 50.3, 74.6, 79.5, 68.7, 73.4, 69.5, 73.3, 78.0, 76.7, 63‚Ä¶\n$ `1995`  &lt;dbl&gt; 54.3, 51.2, 74.6, 79.8, 68.8, 73.4, 70.1, 73.3, 78.3, 77.0, 64‚Ä¶\n$ `1996`  &lt;dbl&gt; 54.7, 51.7, 74.5, 80.0, 68.9, 73.5, 70.4, 74.0, 78.5, 77.3, 65‚Ä¶\n$ `1997`  &lt;dbl&gt; 54.5, 51.6, 72.9, 80.2, 69.0, 73.6, 71.1, 73.5, 78.8, 77.7, 65‚Ä¶\n$ `1998`  &lt;dbl&gt; 53.3, 50.6, 74.8, 80.4, 69.2, 73.7, 71.6, 74.5, 79.1, 77.9, 66‚Ä¶\n$ `1999`  &lt;dbl&gt; 54.7, 51.9, 75.1, 80.6, 69.2, 73.8, 71.9, 74.7, 79.4, 78.2, 66‚Ä¶\n$ `2000`  &lt;dbl&gt; 54.7, 52.8, 75.4, 80.8, 69.1, 74.2, 72.4, 74.8, 79.7, 78.5, 66‚Ä¶\n$ `2001`  &lt;dbl&gt; 54.8, 53.4, 76.0, 80.9, 69.2, 74.3, 72.5, 75.1, 80.1, 78.9, 67‚Ä¶\n$ `2002`  &lt;dbl&gt; 55.5, 54.5, 75.9, 81.1, 69.4, 74.3, 72.7, 75.4, 80.3, 79.0, 67‚Ä¶\n$ `2003`  &lt;dbl&gt; 56.5, 55.1, 75.6, 81.2, 69.3, 74.4, 72.8, 75.6, 80.6, 79.1, 67‚Ä¶\n$ `2004`  &lt;dbl&gt; 57.1, 55.5, 75.8, 81.3, 69.1, 74.9, 73.0, 75.7, 80.9, 79.5, 67‚Ä¶\n$ `2005`  &lt;dbl&gt; 57.6, 56.4, 76.2, 81.4, 69.2, 75.3, 73.0, 75.9, 81.2, 79.8, 67‚Ä¶\n$ `2006`  &lt;dbl&gt; 58.0, 57.0, 76.9, 81.5, 69.5, 75.4, 73.1, 75.9, 81.5, 80.1, 67‚Ä¶\n$ `2007`  &lt;dbl&gt; 58.5, 58.0, 77.5, 81.7, 70.0, 75.3, 73.5, 75.1, 81.5, 80.3, 68‚Ä¶\n$ `2008`  &lt;dbl&gt; 59.2, 58.8, 77.6, 81.8, 70.4, 75.7, 73.5, 75.2, 81.7, 80.5, 68‚Ä¶\n$ `2009`  &lt;dbl&gt; 59.9, 59.5, 78.0, 81.8, 70.6, 75.8, 73.6, 75.8, 81.9, 80.5, 68‚Ä¶\n$ `2010`  &lt;dbl&gt; 60.5, 60.2, 78.1, 81.8, 70.8, 75.9, 73.9, 75.9, 82.1, 80.8, 69‚Ä¶\n$ `2011`  &lt;dbl&gt; 61.0, 60.8, 78.1, 81.9, 71.0, 76.0, 74.2, 76.0, 82.3, 81.0, 69‚Ä¶\n$ `2012`  &lt;dbl&gt; 61.4, 61.4, 78.2, 81.9, 71.2, 76.2, 74.6, 76.0, 82.6, 81.2, 69‚Ä¶\n$ `2013`  &lt;dbl&gt; 61.9, 62.1, 78.3, 82.0, 71.6, 76.3, 75.1, 76.1, 82.7, 81.3, 69‚Ä¶\n$ `2014`  &lt;dbl&gt; 61.9, 63.0, 78.2, 82.0, 73.0, 76.5, 75.2, 76.0, 82.7, 81.5, 69‚Ä¶\n$ `2015`  &lt;dbl&gt; 61.9, 63.5, 78.1, 82.0, 73.2, 76.5, 75.1, 76.0, 82.7, 81.6, 70‚Ä¶\n$ `2016`  &lt;dbl&gt; 62.0, 63.9, 78.2, 82.1, 73.4, 76.2, 75.3, 76.0, 83.0, 81.8, 70‚Ä¶\n$ `2017`  &lt;dbl&gt; 62.9, 64.2, 78.3, 82.1, 73.5, 76.3, 75.5, 76.1, 83.0, 82.0, 70‚Ä¶\n$ `2018`  &lt;dbl&gt; 62.7, 64.6, 78.4, 82.1, 73.7, 76.5, 75.6, 76.2, 82.9, 82.1, 70‚Ä¶\n$ `2019`  &lt;dbl&gt; 63.3, 65.1, 78.5, 82.2, 73.9, 76.6, 75.7, 76.3, 82.9, 82.2, 71‚Ä¶\n$ `2020`  &lt;dbl&gt; 63.4, 65.2, 77.9, NA, 74.0, 74.6, 74.0, 76.3, 82.9, 81.5, 70.4‚Ä¶\n$ `2021`  &lt;dbl&gt; 64.0, 65.8, 78.7, NA, 74.2, 76.9, 76.0, 76.5, 83.2, 82.4, 71.2‚Ä¶\n$ `2022`  &lt;dbl&gt; 64.3, 66.1, 78.8, NA, 74.3, 77.0, 76.1, 76.7, 83.3, 82.6, 71.3‚Ä¶\n$ `2023`  &lt;dbl&gt; 64.6, 66.4, 79.0, NA, 74.4, 77.2, 76.3, 76.8, 83.5, 82.8, 71.4‚Ä¶\n$ `2024`  &lt;dbl&gt; 64.9, 66.8, 79.1, NA, 74.6, 77.3, 76.4, 76.9, 83.6, 82.9, 71.5‚Ä¶\n$ `2025`  &lt;dbl&gt; 65.2, 67.1, 79.2, NA, 74.7, 77.5, 76.5, 77.1, 83.7, 83.1, 71.6‚Ä¶\n$ `2026`  &lt;dbl&gt; 65.4, 67.4, 79.4, NA, 74.8, 77.6, 76.7, 77.2, 83.8, 83.2, 71.7‚Ä¶\n$ `2027`  &lt;dbl&gt; 65.7, 67.7, 79.5, NA, 75.0, 77.8, 76.8, 77.3, 84.0, 83.4, 71.8‚Ä¶\n$ `2028`  &lt;dbl&gt; 66.0, 68.0, 79.7, NA, 75.1, 77.9, 77.0, 77.4, 84.1, 83.6, 71.9‚Ä¶\n$ `2029`  &lt;dbl&gt; 66.2, 68.3, 79.8, NA, 75.2, 78.0, 77.1, 77.6, 84.2, 83.7, 72.0‚Ä¶\n$ `2030`  &lt;dbl&gt; 66.4, 68.6, 80.0, NA, 75.4, 78.2, 77.2, 77.7, 84.3, 83.9, 72.1‚Ä¶\n$ `2031`  &lt;dbl&gt; 66.6, 68.9, 80.2, NA, 75.5, 78.3, 77.4, 77.8, 84.4, 84.1, 72.2‚Ä¶\n$ `2032`  &lt;dbl&gt; 66.9, 69.2, 80.3, NA, 75.6, 78.5, 77.5, 78.0, 84.6, 84.2, 72.3‚Ä¶\n$ `2033`  &lt;dbl&gt; 67.1, 69.4, 80.5, NA, 75.8, 78.6, 77.7, 78.1, 84.7, 84.4, 72.4‚Ä¶\n$ `2034`  &lt;dbl&gt; 67.3, 69.7, 80.7, NA, 75.9, 78.8, 77.8, 78.2, 84.8, 84.5, 72.5‚Ä¶\n$ `2035`  &lt;dbl&gt; 67.5, 70.0, 80.8, NA, 76.0, 78.9, 77.9, 78.3, 84.9, 84.7, 72.7‚Ä¶\n$ `2036`  &lt;dbl&gt; 67.7, 70.2, 80.9, NA, 76.2, 79.1, 78.1, 78.5, 85.0, 84.8, 72.8‚Ä¶\n$ `2037`  &lt;dbl&gt; 67.9, 70.5, 81.1, NA, 76.3, 79.2, 78.2, 78.6, 85.2, 84.9, 72.9‚Ä¶\n$ `2038`  &lt;dbl&gt; 68.0, 70.7, 81.2, NA, 76.4, 79.4, 78.3, 78.7, 85.3, 85.0, 73.0‚Ä¶\n$ `2039`  &lt;dbl&gt; 68.2, 70.9, 81.3, NA, 76.6, 79.5, 78.5, 78.8, 85.4, 85.2, 73.1‚Ä¶\n$ `2040`  &lt;dbl&gt; 68.4, 71.1, 81.5, NA, 76.7, 79.7, 78.6, 79.0, 85.5, 85.3, 73.2‚Ä¶\n$ `2041`  &lt;dbl&gt; 68.6, 71.4, 81.6, NA, 76.8, 79.8, 78.7, 79.1, 85.6, 85.4, 73.3‚Ä¶\n$ `2042`  &lt;dbl&gt; 68.8, 71.6, 81.8, NA, 77.0, 80.0, 78.9, 79.2, 85.7, 85.5, 73.5‚Ä¶\n$ `2043`  &lt;dbl&gt; 68.9, 71.8, 81.9, NA, 77.1, 80.1, 79.0, 79.3, 85.8, 85.6, 73.6‚Ä¶\n$ `2044`  &lt;dbl&gt; 69.1, 72.0, 82.0, NA, 77.2, 80.3, 79.1, 79.5, 86.0, 85.8, 73.7‚Ä¶\n$ `2045`  &lt;dbl&gt; 69.2, 72.2, 82.2, NA, 77.3, 80.4, 79.3, 79.6, 86.1, 85.9, 73.8‚Ä¶\n$ `2046`  &lt;dbl&gt; 69.4, 72.4, 82.3, NA, 77.5, 80.6, 79.4, 79.7, 86.2, 86.0, 73.9‚Ä¶\n$ `2047`  &lt;dbl&gt; 69.5, 72.5, 82.4, NA, 77.6, 80.7, 79.5, 79.8, 86.3, 86.1, 74.0‚Ä¶\n$ `2048`  &lt;dbl&gt; 69.7, 72.7, 82.6, NA, 77.7, 80.8, 79.7, 80.0, 86.4, 86.2, 74.2‚Ä¶\n$ `2049`  &lt;dbl&gt; 69.8, 72.9, 82.7, NA, 77.9, 81.0, 79.8, 80.1, 86.5, 86.3, 74.3‚Ä¶\n$ `2050`  &lt;dbl&gt; 70.0, 73.1, 82.8, NA, 78.0, 81.2, 79.9, 80.2, 86.6, 86.5, 74.4‚Ä¶\n$ `2051`  &lt;dbl&gt; 70.2, 73.3, 83.0, NA, 78.1, 81.3, 80.0, 80.3, 86.8, 86.6, 74.5‚Ä¶\n$ `2052`  &lt;dbl&gt; 70.3, 73.4, 83.1, NA, 78.2, 81.4, 80.2, 80.5, 86.9, 86.7, 74.6‚Ä¶\n$ `2053`  &lt;dbl&gt; 70.4, 73.6, 83.3, NA, 78.4, 81.6, 80.3, 80.6, 87.0, 86.8, 74.7‚Ä¶\n$ `2054`  &lt;dbl&gt; 70.6, 73.8, 83.4, NA, 78.5, 81.7, 80.5, 80.7, 87.1, 86.9, 74.9‚Ä¶\n$ `2055`  &lt;dbl&gt; 70.7, 73.9, 83.5, NA, 78.6, 81.9, 80.6, 80.8, 87.2, 87.0, 75.0‚Ä¶\n$ `2056`  &lt;dbl&gt; 70.9, 74.1, 83.7, NA, 78.7, 82.0, 80.7, 81.0, 87.3, 87.1, 75.1‚Ä¶\n$ `2057`  &lt;dbl&gt; 71.0, 74.2, 83.8, NA, 78.8, 82.2, 80.9, 81.1, 87.4, 87.3, 75.2‚Ä¶\n$ `2058`  &lt;dbl&gt; 71.1, 74.4, 83.9, NA, 79.0, 82.3, 81.0, 81.2, 87.5, 87.4, 75.3‚Ä¶\n$ `2059`  &lt;dbl&gt; 71.3, 74.5, 84.0, NA, 79.1, 82.4, 81.1, 81.3, 87.7, 87.5, 75.5‚Ä¶\n$ `2060`  &lt;dbl&gt; 71.4, 74.7, 84.1, NA, 79.2, 82.6, 81.3, 81.5, 87.8, 87.6, 75.6‚Ä¶\n$ `2061`  &lt;dbl&gt; 71.5, 74.8, 84.3, NA, 79.3, 82.7, 81.4, 81.6, 87.9, 87.7, 75.7‚Ä¶\n$ `2062`  &lt;dbl&gt; 71.7, 75.0, 84.4, NA, 79.4, 82.8, 81.5, 81.7, 88.0, 87.8, 75.8‚Ä¶\n$ `2063`  &lt;dbl&gt; 71.8, 75.1, 84.5, NA, 79.5, 83.0, 81.7, 81.8, 88.1, 87.9, 75.9‚Ä¶\n$ `2064`  &lt;dbl&gt; 72.0, 75.3, 84.6, NA, 79.7, 83.1, 81.8, 82.0, 88.2, 88.0, 76.1‚Ä¶\n$ `2065`  &lt;dbl&gt; 72.1, 75.4, 84.7, NA, 79.8, 83.2, 81.9, 82.1, 88.3, 88.1, 76.2‚Ä¶\n$ `2066`  &lt;dbl&gt; 72.2, 75.5, 84.8, NA, 79.9, 83.4, 82.1, 82.2, 88.5, 88.3, 76.3‚Ä¶\n$ `2067`  &lt;dbl&gt; 72.3, 75.7, 84.9, NA, 80.0, 83.5, 82.2, 82.3, 88.6, 88.4, 76.4‚Ä¶\n$ `2068`  &lt;dbl&gt; 72.5, 75.8, 85.0, NA, 80.1, 83.6, 82.3, 82.5, 88.7, 88.5, 76.6‚Ä¶\n$ `2069`  &lt;dbl&gt; 72.6, 76.0, 85.2, NA, 80.2, 83.7, 82.5, 82.6, 88.8, 88.6, 76.7‚Ä¶\n$ `2070`  &lt;dbl&gt; 72.7, 76.1, 85.3, NA, 80.3, 83.9, 82.6, 82.7, 88.9, 88.7, 76.8‚Ä¶\n$ `2071`  &lt;dbl&gt; 72.9, 76.2, 85.4, NA, 80.4, 84.0, 82.7, 82.8, 89.0, 88.8, 76.9‚Ä¶\n$ `2072`  &lt;dbl&gt; 73.0, 76.4, 85.5, NA, 80.5, 84.1, 82.9, 82.9, 89.1, 88.9, 77.1‚Ä¶\n$ `2073`  &lt;dbl&gt; 73.1, 76.5, 85.6, NA, 80.7, 84.2, 83.0, 83.0, 89.2, 89.0, 77.2‚Ä¶\n$ `2074`  &lt;dbl&gt; 73.3, 76.6, 85.7, NA, 80.8, 84.3, 83.1, 83.2, 89.3, 89.1, 77.3‚Ä¶\n$ `2075`  &lt;dbl&gt; 73.4, 76.8, 85.8, NA, 80.9, 84.5, 83.3, 83.3, 89.5, 89.3, 77.5‚Ä¶\n$ `2076`  &lt;dbl&gt; 73.5, 76.9, 85.9, NA, 81.0, 84.6, 83.4, 83.4, 89.6, 89.4, 77.6‚Ä¶\n$ `2077`  &lt;dbl&gt; 73.7, 77.0, 86.0, NA, 81.0, 84.7, 83.5, 83.5, 89.7, 89.5, 77.7‚Ä¶\n$ `2078`  &lt;dbl&gt; 73.8, 77.2, 86.1, NA, 81.2, 84.8, 83.6, 83.6, 89.8, 89.6, 77.8‚Ä¶\n$ `2079`  &lt;dbl&gt; 74.0, 77.3, 86.2, NA, 81.3, 84.9, 83.8, 83.7, 89.9, 89.7, 78.0‚Ä¶\n$ `2080`  &lt;dbl&gt; 74.1, 77.4, 86.3, NA, 81.3, 85.0, 83.9, 83.8, 90.0, 89.8, 78.1‚Ä¶\n$ `2081`  &lt;dbl&gt; 74.2, 77.5, 86.4, NA, 81.4, 85.1, 84.0, 84.0, 90.1, 89.9, 78.3‚Ä¶\n$ `2082`  &lt;dbl&gt; 74.3, 77.7, 86.5, NA, 81.5, 85.3, 84.1, 84.1, 90.2, 90.0, 78.4‚Ä¶\n$ `2083`  &lt;dbl&gt; 74.5, 77.8, 86.6, NA, 81.6, 85.4, 84.3, 84.2, 90.3, 90.1, 78.5‚Ä¶\n$ `2084`  &lt;dbl&gt; 74.6, 77.9, 86.7, NA, 81.7, 85.5, 84.4, 84.3, 90.5, 90.2, 78.7‚Ä¶\n$ `2085`  &lt;dbl&gt; 74.8, 78.0, 86.8, NA, 81.8, 85.6, 84.5, 84.4, 90.6, 90.3, 78.8‚Ä¶\n$ `2086`  &lt;dbl&gt; 74.9, 78.2, 86.9, NA, 81.9, 85.7, 84.6, 84.5, 90.7, 90.5, 78.9‚Ä¶\n$ `2087`  &lt;dbl&gt; 75.0, 78.3, 87.0, NA, 82.0, 85.8, 84.7, 84.6, 90.8, 90.6, 79.1‚Ä¶\n$ `2088`  &lt;dbl&gt; 75.2, 78.4, 87.1, NA, 82.1, 85.9, 84.8, 84.7, 90.9, 90.7, 79.2‚Ä¶\n$ `2089`  &lt;dbl&gt; 75.3, 78.6, 87.2, NA, 82.2, 86.0, 85.0, 84.8, 91.0, 90.8, 79.3‚Ä¶\n$ `2090`  &lt;dbl&gt; 75.4, 78.7, 87.3, NA, 82.3, 86.1, 85.1, 84.9, 91.1, 90.9, 79.5‚Ä¶\n$ `2091`  &lt;dbl&gt; 75.5, 78.8, 87.4, NA, 82.4, 86.2, 85.2, 85.0, 91.3, 91.0, 79.6‚Ä¶\n$ `2092`  &lt;dbl&gt; 75.7, 79.0, 87.5, NA, 82.5, 86.3, 85.3, 85.1, 91.4, 91.1, 79.7‚Ä¶\n$ `2093`  &lt;dbl&gt; 75.8, 79.1, 87.6, NA, 82.6, 86.5, 85.4, 85.2, 91.5, 91.2, 79.9‚Ä¶\n$ `2094`  &lt;dbl&gt; 76.0, 79.2, 87.7, NA, 82.7, 86.5, 85.5, 85.3, 91.6, 91.3, 80.0‚Ä¶\n$ `2095`  &lt;dbl&gt; 76.1, 79.3, 87.8, NA, 82.8, 86.7, 85.6, 85.4, 91.7, 91.5, 80.1‚Ä¶\n$ `2096`  &lt;dbl&gt; 76.2, 79.5, 87.9, NA, 82.9, 86.8, 85.7, 85.5, 91.8, 91.6, 80.3‚Ä¶\n$ `2097`  &lt;dbl&gt; 76.4, 79.6, 88.0, NA, 83.0, 86.9, 85.8, 85.6, 91.9, 91.7, 80.4‚Ä¶\n$ `2098`  &lt;dbl&gt; 76.5, 79.7, 88.2, NA, 83.1, 87.0, 86.0, 85.7, 92.0, 91.8, 80.5‚Ä¶\n$ `2099`  &lt;dbl&gt; 76.6, 79.9, 88.3, NA, 83.2, 87.1, 86.1, 85.8, 92.1, 91.9, 80.7‚Ä¶\n$ `2100`  &lt;dbl&gt; 76.8, 80.0, 88.4, NA, 83.3, 87.2, 86.2, 85.9, 92.3, 92.0, 80.8‚Ä¶\n\n# look at all columns and first 6 rows with head\nhead(life_expectancy)\n\n# A tibble: 6 √ó 302\n  country  `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghani‚Ä¶   28.2   28.2   28.2   28.2   28.2   28.2   28.1   28.1   28.1   28.1\n2 Angola     27     27     27     27     27     27     27     27     27     27  \n3 Albania    35.4   35.4   35.4   35.4   35.4   35.4   35.4   35.4   35.4   35.4\n4 Andorra    NA     NA     NA     NA     NA     NA     NA     NA     NA     NA  \n5 United ‚Ä¶   30.7   30.7   30.7   30.7   30.7   30.7   30.7   30.7   30.7   30.7\n6 Argenti‚Ä¶   33.2   33.2   33.2   33.2   33.2   33.2   33.2   33.2   33.2   33.2\n# ‚Ñπ 291 more variables: `1810` &lt;dbl&gt;, `1811` &lt;dbl&gt;, `1812` &lt;dbl&gt;, `1813` &lt;dbl&gt;,\n#   `1814` &lt;dbl&gt;, `1815` &lt;dbl&gt;, `1816` &lt;dbl&gt;, `1817` &lt;dbl&gt;, `1818` &lt;dbl&gt;,\n#   `1819` &lt;dbl&gt;, `1820` &lt;dbl&gt;, `1821` &lt;dbl&gt;, `1822` &lt;dbl&gt;, `1823` &lt;dbl&gt;,\n#   `1824` &lt;dbl&gt;, `1825` &lt;dbl&gt;, `1826` &lt;dbl&gt;, `1827` &lt;dbl&gt;, `1828` &lt;dbl&gt;,\n#   `1829` &lt;dbl&gt;, `1830` &lt;dbl&gt;, `1831` &lt;dbl&gt;, `1832` &lt;dbl&gt;, `1833` &lt;dbl&gt;,\n#   `1834` &lt;dbl&gt;, `1835` &lt;dbl&gt;, `1836` &lt;dbl&gt;, `1837` &lt;dbl&gt;, `1838` &lt;dbl&gt;,\n#   `1839` &lt;dbl&gt;, `1840` &lt;dbl&gt;, `1841` &lt;dbl&gt;, `1842` &lt;dbl&gt;, `1843` &lt;dbl&gt;, ‚Ä¶\n\n# this will open the file for you to look through in R\n# View(life_expectancy)\n\nHow many observations there in each dataset?\n\ndim(happiness)\n\n[1] 163  19\n\ndim(life_expectancy)\n\n[1] 195 302\n\n\nYou can write in Markdown to code in line line this: There are `r nrow(happiness)` countries in happiness and `r nrow(life_expectancy)` in life_expectancy. There are `r ncol(happiness)-1` years with data in happiness and `r ncol(life_expectancy)-1` in life_expectancy.\nAnd it will render like this: There are 163 countries in happiness and 195 in life_expectancy. There are 18 years with data in happiness and 301 in life_expectancy.\nWhat years do the data contain information for? If you don‚Äôt use the mutate() function to convert your year to numeric (it is a character column after pivoting) this will still work, but calling min() and max() on non-numeric data doesn‚Äôt make too much sense so I think its generally risky and could introduce problems in other situations (but doesn‚Äôt in this one).\nFor happiness:\n\nhappiness_long &lt;- happiness |&gt; \n  pivot_longer(cols = !country, # all columns but country\n               names_to = \"year\", # colnames to new col called \"year\"\n               values_to = \"happy_value\") |&gt; # values from cells to \"happy_value\" \n  mutate(year = as.numeric(year)) # convert year col to be numeric (is character)\n\nhappiness_long |&gt; \n  summarize(min_year = min(year),\n            max_year = max(year))\n\n# A tibble: 1 √ó 2\n  min_year max_year\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     2005     2022\n\n\nFor life expectancy:\n\nlife_expectancy_long &lt;- life_expectancy |&gt; \n  pivot_longer(cols = !country, # all columns but country\n               names_to = \"year\", # colnames to new col called \"year\"\n               values_to = \"life_exp\") |&gt;   # values from cells to \"happy_value\" \n  mutate(year = as.numeric(year)) # convert year col to be numeric (is character)\n  \nlife_expectancy_long |&gt; \n  summarize(min_year = min(year),\n            max_year = max(year))\n\n# A tibble: 1 √ó 2\n  min_year max_year\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     1800     2100",
    "crumbs": [
      "Recitation solutions",
      "Week 4 - Wrangling solutions"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#modifying-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#modifying-data",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Modifying data",
    "text": "Modifying data\nCreate a new dataset for life_expectancy that only includes observed data (i.e., remove the projected data after 2022). If you used 2024 that would also be fine - but this data was pulled in 2022.\n\n# with long data\nlife_expectancy_actual &lt;- life_expectancy_long |&gt; \n  filter(year &lt;= 2022)\n\n# with wide data\nlife_expectancy_actual &lt;- life_expectancy |&gt; \n  select(country, num_range(prefix = \"\", # since there is no prefix here\n                            range = 1800:2022))",
    "crumbs": [
      "Recitation solutions",
      "Week 4 - Wrangling solutions"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#calculating-summaries",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#calculating-summaries",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Calculating summaries",
    "text": "Calculating summaries\nWhat country has the highest average happiness index in 2022?\n\n# highest happiness is 2022\n# note you can have columns that use non-standard R naming convention\n# like here where columns are numbers\n# but you need to refer to them surrounded by backticks\nhappiness |&gt; \n  select(country, `2022`) |&gt; \n  arrange(desc(`2022`))\n\n# A tibble: 163 √ó 2\n   country     `2022`\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Finland       78  \n 2 Denmark       75.9\n 3 Iceland       75.3\n 4 Israel        74.7\n 5 Netherlands   74  \n 6 Sweden        74  \n 7 Norway        73.2\n 8 Switzerland   72.4\n 9 Luxembourg    72.3\n10 New Zealand   71.2\n# ‚Ñπ 153 more rows\n\n# or we could use clean_names\nhappiness |&gt; \n  janitor::clean_names() |&gt; \n  select(country, x2022) |&gt; \n  arrange(desc(x2022))\n\n# A tibble: 163 √ó 2\n   country     x2022\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 Finland      78  \n 2 Denmark      75.9\n 3 Iceland      75.3\n 4 Israel       74.7\n 5 Netherlands  74  \n 6 Sweden       74  \n 7 Norway       73.2\n 8 Switzerland  72.4\n 9 Luxembourg   72.3\n10 New Zealand  71.2\n# ‚Ñπ 153 more rows\n\n\nWhat about overall average highest index?\n\n# if you want to calculate and have missing values\n# you need to remove them as the default is to not\n\n# pivoting and then calculating mean after group_by country\nhappiness_long |&gt; \n  group_by(country) |&gt; \n  summarize(mean_happiness = mean(happy_value, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_happiness))\n\n# A tibble: 163 √ó 2\n   country     mean_happiness\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Denmark               76.8\n 2 Finland               76.2\n 3 Switzerland           75.0\n 4 Norway                74.8\n 5 Iceland               74.7\n 6 Netherlands           74.5\n 7 Sweden                73.8\n 8 Canada                73.3\n 9 New Zealand           72.9\n10 Australia             72.6\n# ‚Ñπ 153 more rows\n\n# using select in rowMeans\nhappiness |&gt; \n  mutate(mean_happiness = rowMeans(select(happiness, -country),\n                                          na.rm = TRUE)) |&gt; \n  arrange(desc(mean_happiness))\n\n# A tibble: 163 √ó 20\n   country `2005` `2006` `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014`\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Denmark   80.2   NA     78.3   79.7   76.8   77.7   77.9   75.2   75.9   75.1\n 2 Finland   NA     76.7   NA     76.7   NA     73.9   73.5   74.2   74.5   73.8\n 3 Switze‚Ä¶   NA     74.7   NA     NA     75.3   NA     NA     77.8   NA     74.9\n 4 Norway    NA     74.2   NA     76.3   NA     NA     NA     76.8   NA     74.4\n 5 Iceland   NA     NA     NA     68.9   NA     NA     NA     75.9   75     NA  \n 6 Nether‚Ä¶   74.6   NA     74.5   76.3   NA     75     75.6   74.7   74.1   73.2\n 7 Sweden    73.8   NA     72.4   75.2   72.7   75     73.8   75.6   74.3   72.4\n 8 Canada    74.2   NA     74.8   74.9   74.9   76.5   74.3   74.2   75.9   73  \n 9 New Ze‚Ä¶   NA     73     76     73.8   NA     72.2   71.9   72.5   72.8   73.1\n10 Austra‚Ä¶   73.4   NA     72.8   72.5   NA     74.5   74.1   72     73.6   72.9\n# ‚Ñπ 153 more rows\n# ‚Ñπ 9 more variables: `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;,\n#   mean_happiness &lt;dbl&gt;\n\nrowMeans(happiness[,-1], na.rm = TRUE)\n\n  [1] 33.87143 44.22500 50.52000 67.80667 62.69412 46.15000 72.60000 72.22000\n  [9] 49.40714 35.50000 69.66250 41.03571 42.58125 46.89412 46.98571 60.15833\n [17] 52.58000 55.71429 62.05000 57.20588 65.56471 51.96667 39.53077 35.16000\n [25] 73.27647 75.04167 63.60000 51.41176 46.88182 46.62353 42.21111 46.09231\n [33] 61.56471 38.88571 70.51765 54.20000 61.18667 66.35000 68.40000 48.25000\n [41] 76.75882 52.84706 53.72727 57.12353 44.79412 65.03529 57.41875 43.69000\n [49] 76.23333 66.68824 45.75833 69.08235 43.67647 47.80588 43.32500 46.20000\n [57] 56.08750 62.47333 59.90000 54.26923 54.77500 57.06667 39.53636 54.10000\n [65] 52.19412 43.94706 70.43125 48.61333 47.25714 74.68000 72.54118 63.02941\n [73] 57.48889 50.74118 60.27647 58.52941 44.12353 52.32353 42.76471 58.92941\n [81] 62.74545 50.05455 45.25294 40.46000 55.43333 43.33125 40.05000 59.42353\n [89] 70.61667 55.35000 50.35833 56.46471 39.84167 52.00000 66.34118 49.78667\n [97] 42.39375 63.90000 43.95455 53.40000 51.54000 47.59000 44.32857 58.76667\n[105] 39.87143 58.38667 45.92500 42.72000 49.69333 57.30588 74.51250 74.83333\n[113] 48.01176 72.87500 68.50000 51.21176 66.49375 55.60588 52.76471 59.54375\n[121] 55.26667 55.68125 46.38000 65.70000 57.35000 55.25294 36.54167 65.29412\n[129] 43.78000 45.12353 65.15333 40.14286 59.90000 51.83333 53.70667 34.02500\n[137] 62.70000 61.18571 61.41333 73.76471 44.93333 40.15714 40.68667 36.52727\n[145] 60.46471 49.81765 56.00000 62.82000 47.18571 51.50588 63.31333 36.96471\n[153] 43.07059 48.16471 62.88824 70.70000 58.32500 59.19412 53.56471 39.13333\n[161] 49.39412 44.23750 38.00000\n\n# using indexing in rowMeans\nhappiness |&gt; \n  mutate(mean_happiness = rowMeans(happiness[-1], na.rm = TRUE)) |&gt; \n  arrange(desc(mean_happiness)) |&gt; \n  select(country, mean_happiness, everything())\n\n# A tibble: 163 √ó 20\n   country     mean_happiness `2005` `2006` `2007` `2008` `2009` `2010` `2011`\n   &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Denmark               76.8   80.2   NA     78.3   79.7   76.8   77.7   77.9\n 2 Finland               76.2   NA     76.7   NA     76.7   NA     73.9   73.5\n 3 Switzerland           75.0   NA     74.7   NA     NA     75.3   NA     NA  \n 4 Norway                74.8   NA     74.2   NA     76.3   NA     NA     NA  \n 5 Iceland               74.7   NA     NA     NA     68.9   NA     NA     NA  \n 6 Netherlands           74.5   74.6   NA     74.5   76.3   NA     75     75.6\n 7 Sweden                73.8   73.8   NA     72.4   75.2   72.7   75     73.8\n 8 Canada                73.3   74.2   NA     74.8   74.9   74.9   76.5   74.3\n 9 New Zealand           72.9   NA     73     76     73.8   NA     72.2   71.9\n10 Australia             72.6   73.4   NA     72.8   72.5   NA     74.5   74.1\n# ‚Ñπ 153 more rows\n# ‚Ñπ 11 more variables: `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;,\n#   `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\nHow many countries had an average life expectancy over 80 years in 2022?\n\n# with long data\n# to see what the countries are\nlife_expectancy_long |&gt; # long data\n  filter(year == 2022) |&gt;  # only 2022\n  filter(life_exp &gt; 80) # filter for over 80 years\n\n# A tibble: 36 √ó 3\n   country      year life_exp\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 Australia    2022     83.3\n 2 Austria      2022     82.6\n 3 Belgium      2022     81.9\n 4 Canada       2022     82.6\n 5 Switzerland  2022     84.4\n 6 Chile        2022     80.6\n 7 Colombia     2022     80.7\n 8 Costa Rica   2022     80.6\n 9 Cyprus       2022     81.3\n10 Germany      2022     81.7\n# ‚Ñπ 26 more rows\n\n# to see how many there are\nlife_expectancy_long |&gt; \n  filter(year == 2022) |&gt; \n  filter(life_exp &gt; 80) |&gt; \n  nrow() # counts rows\n\n[1] 36\n\n# with wide data\n# to see what the countries are\nlife_expectancy |&gt; \n  select(country, `2022`) |&gt; # pick the columns country and 2022\n  filter(`2022` &gt; 80) # filter for 2022 &gt; 80\n\n# A tibble: 36 √ó 2\n   country     `2022`\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Australia     83.3\n 2 Austria       82.6\n 3 Belgium       81.9\n 4 Canada        82.6\n 5 Switzerland   84.4\n 6 Chile         80.6\n 7 Colombia      80.7\n 8 Costa Rica    80.6\n 9 Cyprus        81.3\n10 Germany       81.7\n# ‚Ñπ 26 more rows\n\n# to see how many there are\nlife_expectancy |&gt; \n  select(country, `2022`) |&gt; \n  filter(`2022` &gt; 80) |&gt; \n  nrow() \n\n[1] 36\n\n\nWhat countries are in the top 10 percentile for happiness? What about the bottom? What about for life expectancy? You can calculate this for the most recent data, for the mean, or really for whatever you want. Remember there are lots of ways to do this. Hint - try using the functions in the slice_() family.\n\n# happiness\n# top 10th percentile\nhappiness_long |&gt; \n  group_by(country) |&gt; \n  summarize(mean_happiness = mean(happy_value, na.rm = TRUE)) |&gt; \n  slice_max(order_by = mean_happiness, prop = 0.1) # take the top 10% ordered by mean_happiness\n\n# A tibble: 16 √ó 2\n   country       mean_happiness\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Denmark                 76.8\n 2 Finland                 76.2\n 3 Switzerland             75.0\n 4 Norway                  74.8\n 5 Iceland                 74.7\n 6 Netherlands             74.5\n 7 Sweden                  73.8\n 8 Canada                  73.3\n 9 New Zealand             72.9\n10 Australia               72.6\n11 Israel                  72.5\n12 Austria                 72.2\n13 United States           70.7\n14 Luxembourg              70.6\n15 Costa Rica              70.5\n16 Ireland                 70.4\n\n# how many countries do we have?\nnrow(happiness)\n\n[1] 163\n\n# how many countries are in the each decile?\nnrow(happiness) * 0.1\n\n[1] 16.3\n\n# we want to pick the top 16 countries\nhappiness_long |&gt; \n  group_by(country) |&gt; \n  summarize(mean_happiness = mean(happy_value, na.rm = TRUE)) |&gt; \n  arrange(-mean_happiness) |&gt; \n  top_n(16)\n\nSelecting by mean_happiness\n\n\n# A tibble: 16 √ó 2\n   country       mean_happiness\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Denmark                 76.8\n 2 Finland                 76.2\n 3 Switzerland             75.0\n 4 Norway                  74.8\n 5 Iceland                 74.7\n 6 Netherlands             74.5\n 7 Sweden                  73.8\n 8 Canada                  73.3\n 9 New Zealand             72.9\n10 Australia               72.6\n11 Israel                  72.5\n12 Austria                 72.2\n13 United States           70.7\n14 Luxembourg              70.6\n15 Costa Rica              70.5\n16 Ireland                 70.4\n\n\n\n# life expectancy in 2022\n# top 10th percentile\nlife_expectancy_long |&gt; \n  filter(year == 2022) |&gt; \n  slice_max(order_by = life_exp, prop = 0.1)\n\n# A tibble: 19 √ó 3\n   country           year life_exp\n   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n 1 Singapore         2022     85.3\n 2 Japan             2022     85.2\n 3 Hong Kong, China  2022     84.8\n 4 Iceland           2022     84.5\n 5 Switzerland       2022     84.4\n 6 Spain             2022     83.5\n 7 Israel            2022     83.5\n 8 Italy             2022     83.5\n 9 Luxembourg        2022     83.4\n10 Norway            2022     83.4\n11 Australia         2022     83.3\n12 France            2022     83.3\n13 South Korea       2022     83.3\n14 Sweden            2022     83.3\n15 Malta             2022     83  \n16 Austria           2022     82.6\n17 Canada            2022     82.6\n18 Ireland           2022     82.5\n19 Finland           2022     82.4\n\n# bottom 10th percentile\n# top 10th percentile\nlife_expectancy_long |&gt; \n  filter(year == 2022) |&gt; \n  slice_max(order_by = -life_exp, prop = 0.1)\n\n# A tibble: 19 √ó 3\n   country                   year life_exp\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Lesotho                   2022     53  \n 2 Central African Republic  2022     53.4\n 3 Eswatini                  2022     59.3\n 4 Somalia                   2022     59.4\n 5 Solomon Islands           2022     59.5\n 6 Mozambique                2022     59.6\n 7 Chad                      2022     61.3\n 8 Kiribati                  2022     61.4\n 9 Zimbabwe                  2022     61.7\n10 Guinea-Bissau             2022     61.9\n11 Guinea                    2022     62.2\n12 Botswana                  2022     62.6\n13 Burkina Faso              2022     62.9\n14 Mali                      2022     63  \n15 Sierra Leone              2022     63  \n16 Niger                     2022     63.6\n17 Zambia                    2022     64  \n18 Afghanistan               2022     64.3\n19 Cameroon                  2022     64.3\n\n\nWhich country has had their happiness index increase the most from 2012 to 2022? Which dropped the most?\n\nhappiness |&gt; \n  mutate(change_2022_2012 = `2022` - `2012`) |&gt; \n  select(country, change_2022_2012) |&gt; \n  arrange(desc(change_2022_2012))\n\n# A tibble: 163 √ó 2\n   country     change_2022_2012\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Guinea                  14.2\n 2 Honduras                14.2\n 3 Romania                 14.2\n 4 Hungary                 13.6\n 5 Congo, Rep.             13.5\n 6 Bulgaria                12.5\n 7 Benin                   11.8\n 8 Senegal                 11.8\n 9 Bahrain                 11.4\n10 Nepal                   11.3\n# ‚Ñπ 153 more rows\n\nhappiness |&gt; \n  mutate(change_2022_2012 = `2022` - `2012`) |&gt; \n  select(country, change_2022_2012) |&gt; \n  arrange(change_2022_2012)\n\n# A tibble: 163 √ó 2\n   country          change_2022_2012\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Lebanon                     -21.8\n 2 Afghanistan                 -19.2\n 3 Venezuela                   -18.6\n 4 Zimbabwe                    -17.5\n 5 Congo, Dem. Rep.            -14.3\n 6 Botswana                    -14  \n 7 Zambia                      -10.3\n 8 Jordan                      -10.1\n 9 Mexico                       -9.9\n10 Malawi                       -7.8\n# ‚Ñπ 153 more rows",
    "crumbs": [
      "Recitation solutions",
      "Week 4 - Wrangling solutions"
    ]
  },
  {
    "objectID": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#joining-data",
    "href": "modules/module2/04_wrangling/04_wrangling_recitation_solutions.html#joining-data",
    "title": "Wrangling your data ü§† Recitation Solutions",
    "section": "Joining data",
    "text": "Joining data\nTry joining the happiness and life_expectancy datasets together and use the different *_join() functions so you can see how they differ. Check their dimensions and look at them. Think about how you might want to do different joins in different situations.\n\nleft_joined &lt;- \n  left_join(x = life_expectancy, y = happiness, by = \"country\") \n\nright_joined &lt;- \n  right_join(x = life_expectancy, y = happiness, by = \"country\")\n\ninner_joined &lt;- \n  inner_join(x = life_expectancy, y = happiness, by = \"country\")\n\nfull_joined &lt;- \n  full_join(x = life_expectancy, y = happiness, by = \"country\")\n\n\ndim(left_joined)\n\n[1] 195 320\n\ndim(right_joined)\n\n[1] 163 320\n\ndim(inner_joined)\n\n[1] 163 320\n\ndim(full_joined)\n\n[1] 195 320\n\n\nIf you wanted to create a plot that allowed you to see the correlation between happiness score and life expectancy in 2022, which joined dataset would you use and why?\n\n# with wide data\nfor_correlation_wide &lt;-\n  inner_join(x = life_expectancy |&gt; select(country, `2022`), \n             y = happiness |&gt; select(country, `2022`), \n             by = \"country\") |&gt; \n  rename(life_expectancy_2022 = `2022.x`) |&gt; \n  rename(happy_value_2022 = `2022.y`)\n\n# with long data\nlife_expectancy_2005_2022 &lt;- life_expectancy |&gt; \n  select(country, `2005`:`2022`) |&gt; \n  pivot_longer(cols = `2005`:`2022`,\n               names_to = \"year\",\n               values_to = \"life_expectancy\") |&gt; \n  mutate(year = as.numeric(year))\n\nfor_correlation_long &lt;- \n  inner_join(x = life_expectancy_2005_2022, y = happiness_long,\n             by = c(\"country\", \"year\"))\n\nIn this case, you want a data frame that includes only the values that are in both the life expectancy and the happiness datasets. And, we want to be able to have a column with the life expectancy and a column with the happiness value.\nI am not expecting you to be able to make a plot but I wanted to just give you a sense of the kinds of things you‚Äôll be learning in class.\n\n# create a df with the extreme values for life exp and happiness\nextremes &lt;- for_correlation_wide |&gt; \n  filter(life_expectancy_2022 &gt; 85 | happy_value_2022 &lt; 38)\n\n# create a plot\nfor_correlation_wide |&gt; \n  ggplot(aes(x = life_expectancy_2022, y = happy_value_2022)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggrepel::geom_label_repel(data = extremes,\n                            aes(x = life_expectancy_2022, y = happy_value_2022, \n                                label = country),\n                            size = 3) +\n  theme_minimal() +\n  labs(x = \"Life expectancy\",\n       y = \"Happiness index\",\n       title = \"Relationship between life expectancy and happiness index in 2022\",\n       caption = \"Data from Gapminder\")",
    "crumbs": [
      "Recitation solutions",
      "Week 4 - Wrangling solutions"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "",
    "text": "Nacho (Jess‚Äôs dog, left) along with his friends Petunia (middle) and Inu (right) waiting for dinner",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Introduction",
    "text": "Introduction\nWe will practice what we learned this week in ggplot102 on:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data. Today we will be looking again at some different data from the Tidy Tuesday project (here is the Github repo) about dog breeds.\n\ninstall.packages(\"tidytuesdayR\")\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nWe will be using the data that is from February 1, 2022, so let‚Äôs download it. The readme for this data is here.\n\ntuesdata &lt;- ???\n\nLet‚Äôs look at it. How can you do that?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigating",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigating",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Investigating",
    "text": "Investigating\nWrite code to determine what the 5 most popular dog breeds in 2020 were.\nWhat are the 5 most popular and the 5 least popular dogs across this time frame? There are many ways to do this.\n\n\n\n\n\n\nNeed a hint about how to do this? (Click to expand)\n\n\n\n\n\nCreate a new variable that is a sum of all the ranks from 2013, allowing a composite score of the popularity of each dog breed across this time period.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-1",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-1",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Visualization 1",
    "text": "Visualization 1\nCreate a plot where you take the 12 most popular dogs from 2020, and plot their popularity rank from 2013 to 2020.\n\n\n\n\n\n\nNeed a hint about how to do this? (Click to expand)\n\n\n\n\n\nTo facet, you need to have the variable you want to facet in one column.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-2",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#visualization-2",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Visualization 2",
    "text": "Visualization 2\nAlter the aesthetics of this plot until you think it looks good.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigate-more",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation.html#investigate-more",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation",
    "section": "Investigate more",
    "text": "Investigate more\nWhat dog has jumped in the rankings most from 2013 to 2020? What has dropped the most?",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 6 - ggplot102"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "",
    "text": "Nacho (Jess‚Äôs dog, left) along with his friends Petunia (middle) and Inu (right) waiting for dinner",
    "crumbs": [
      "Recitation solutions",
      "Week 6 - ggplot102 solutions"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#introduction",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#introduction",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Introduction",
    "text": "Introduction\nWe will practice what we learned this week in ggplot102 on:\n\nFacets\nScales\nLabels\nThemes\n\n\nLoad libraries and data\nBefore we get started, let‚Äôs load our libraries and data. Today we will be looking again at some different data from the Tidy Tuesday project (here is the Github repo) about dog breeds.\n\ninstall.packages(\"tidytuesdayR\")\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nWe will be using the data that is from February 1, 2022, so let‚Äôs download it. The readme for this data is here.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-02-01')\n\n---- Compiling #TidyTuesday Information for 2022-02-01 ----\n--- There are 3 files available ---\n\n\n‚îÄ‚îÄ Downloading files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  1 of 3: \"breed_traits.csv\"\n  2 of 3: \"trait_description.csv\"\n  3 of 3: \"breed_rank.csv\"\n\n\n\n\nLet‚Äôs look at it\ntuesdata is a list of 3 dataframes breed_traits, trait_description and breed_rank.\n\nglimpse(tuesdata)\n\nList of 3\n $ breed_traits     : spc_tbl_ [195 √ó 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Breed                     : chr [1:195] \"Retrievers¬†(Labrador)\" \"French¬†Bulldogs\" \"German¬†Shepherd¬†Dogs\" \"Retrievers¬†(Golden)\" ...\n  ..$ Affectionate With Family  : num [1:195] 5 5 5 5 4 5 3 5 5 5 ...\n  ..$ Good With Young Children  : num [1:195] 5 5 5 5 3 5 5 3 5 3 ...\n  ..$ Good With Other Dogs      : num [1:195] 5 4 3 5 3 3 5 3 4 4 ...\n  ..$ Shedding Level            : num [1:195] 4 3 4 4 3 1 3 3 3 2 ...\n  ..$ Coat Grooming Frequency   : num [1:195] 2 1 2 2 3 4 2 1 2 2 ...\n  ..$ Drooling Level            : num [1:195] 2 3 2 2 3 1 1 3 2 2 ...\n  ..$ Coat Type                 : chr [1:195] \"Double\" \"Smooth\" \"Double\" \"Double\" ...\n  ..$ Coat Length               : chr [1:195] \"Short\" \"Short\" \"Medium\" \"Medium\" ...\n  ..$ Openness To Strangers     : num [1:195] 5 5 3 5 4 5 3 3 4 4 ...\n  ..$ Playfulness Level         : num [1:195] 5 5 4 4 4 5 4 4 4 4 ...\n  ..$ Watchdog/Protective Nature: num [1:195] 3 3 5 3 3 5 2 5 4 4 ...\n  ..$ Adaptability Level        : num [1:195] 5 5 5 5 3 4 4 4 4 4 ...\n  ..$ Trainability Level        : num [1:195] 5 4 5 5 4 5 3 5 5 4 ...\n  ..$ Energy Level              : num [1:195] 5 3 5 3 3 4 4 3 5 3 ...\n  ..$ Barking Level             : num [1:195] 3 1 3 1 2 4 4 1 3 5 ...\n  ..$ Mental Stimulation Needs  : num [1:195] 4 3 5 4 3 5 4 5 5 3 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Breed = col_character(),\n  .. ..   `Affectionate With Family` = col_double(),\n  .. ..   `Good With Young Children` = col_double(),\n  .. ..   `Good With Other Dogs` = col_double(),\n  .. ..   `Shedding Level` = col_double(),\n  .. ..   `Coat Grooming Frequency` = col_double(),\n  .. ..   `Drooling Level` = col_double(),\n  .. ..   `Coat Type` = col_character(),\n  .. ..   `Coat Length` = col_character(),\n  .. ..   `Openness To Strangers` = col_double(),\n  .. ..   `Playfulness Level` = col_double(),\n  .. ..   `Watchdog/Protective Nature` = col_double(),\n  .. ..   `Adaptability Level` = col_double(),\n  .. ..   `Trainability Level` = col_double(),\n  .. ..   `Energy Level` = col_double(),\n  .. ..   `Barking Level` = col_double(),\n  .. ..   `Mental Stimulation Needs` = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ trait_description: spc_tbl_ [16 √ó 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Trait      : chr [1:16] \"Affectionate With Family\" \"Good With Young Children\" \"Good With Other Dogs\" \"Shedding Level\" ...\n  ..$ Trait_1    : chr [1:16] \"Independent\" \"Not Recommended\" \"Not Recommended\" \"No Shedding\" ...\n  ..$ Trait_5    : chr [1:16] \"Lovey-Dovey\" \"Good With Children\" \"Good With Other Dogs\" \"Hair Everywhere\" ...\n  ..$ Description: chr [1:16] \"How affectionate a breed is likely to be with family members, or other people he knows well. Some breeds can be\"| __truncated__ \"A breed's level of tolerance and patience with childrens' behavior, and overall family-friendly nature. Dogs sh\"| __truncated__ \"How generally friendly a breed is towards other dogs. Dogs should always be supervised for interactions and int\"| __truncated__ \"How much fur and hair you can expect the breed to leave behind. Breeds with high shedding will need to be brush\"| __truncated__ ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Trait = col_character(),\n  .. ..   Trait_1 = col_character(),\n  .. ..   Trait_5 = col_character(),\n  .. ..   Description = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ breed_rank       : spc_tbl_ [195 √ó 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Breed    : chr [1:195] \"Retrievers (Labrador)\" \"French Bulldogs\" \"German Shepherd Dogs\" \"Retrievers (Golden)\" ...\n  ..$ 2013 Rank: num [1:195] 1 11 2 3 5 8 4 9 13 10 ...\n  ..$ 2014 Rank: num [1:195] 1 9 2 3 4 7 5 10 12 11 ...\n  ..$ 2015 Rank: num [1:195] 1 6 2 3 4 8 5 9 11 13 ...\n  ..$ 2016 Rank: num [1:195] 1 6 2 3 4 7 5 8 11 13 ...\n  ..$ 2017 Rank: num [1:195] 1 4 2 3 5 7 6 8 10 13 ...\n  ..$ 2018 Rank: num [1:195] 1 4 2 3 5 7 6 8 9 12 ...\n  ..$ 2019 Rank: num [1:195] 1 4 2 3 5 6 7 8 9 11 ...\n  ..$ 2020 Rank: num [1:195] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ links    : chr [1:195] \"https://www.akc.org/dog-breeds/labrador-retriever/\" \"https://www.akc.org/dog-breeds/french-bulldog/\" \"https://www.akc.org/dog-breeds/german-shepherd-dog/\" \"https://www.akc.org/dog-breeds/golden-retriever/\" ...\n  ..$ Image    : chr [1:195] \"https://www.akc.org/wp-content/uploads/2017/11/Labrador-Retriever-illustration.jpg\" \"https://www.akc.org/wp-content/uploads/2017/11/French-Bulldog-Illo-2.jpg\" \"https://www.akc.org/wp-content/uploads/2017/11/German-Shepherd-Dog-Illo-2.jpg\" \"https://www.akc.org/wp-content/uploads/2017/11/Golden-Retriever-Illo-2.jpg\" ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Breed = col_character(),\n  .. ..   `2013 Rank` = col_double(),\n  .. ..   `2014 Rank` = col_double(),\n  .. ..   `2015 Rank` = col_double(),\n  .. ..   `2016 Rank` = col_double(),\n  .. ..   `2017 Rank` = col_double(),\n  .. ..   `2018 Rank` = col_double(),\n  .. ..   `2019 Rank` = col_double(),\n  .. ..   `2020 Rank` = col_double(),\n  .. ..   links = col_character(),\n  .. ..   Image = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n - attr(*, \".tt\")= 'tt' chr [1:3] \"breed_traits.csv\" \"trait_description.csv\" \"breed_rank.csv\"\n  ..- attr(*, \".files\")='data.frame':   3 obs. of  3 variables:\n  .. ..$ data_files: chr [1:3] \"breed_traits.csv\" \"trait_description.csv\" \"breed_rank.csv\"\n  .. ..$ data_type : chr [1:3] \"csv\" \"csv\" \"csv\"\n  .. ..$ delim     : chr [1:3] \",\" \",\" \",\"\n  ..- attr(*, \".readme\")=List of 2\n  .. ..$ node:&lt;externalptr&gt; \n  .. ..$ doc :&lt;externalptr&gt; \n  .. ..- attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n  ..- attr(*, \".date\")= Date[1:1], format: \"2022-02-01\"\n - attr(*, \"class\")= chr \"tt_data\"\n\n\nNote tuesdata is a list of 3 dataframes.\nWe will use the $ to see the different dataframes individually.\n\nglimpse(tuesdata$breed_traits)\n\nRows: 195\nColumns: 17\n$ Breed                        &lt;chr&gt; \"Retrievers¬†(Labrador)\", \"French¬†Bulldogs‚Ä¶\n$ `Affectionate With Family`   &lt;dbl&gt; 5, 5, 5, 5, 4, 5, 3, 5, 5, 5, 5, 3, 5, 4,‚Ä¶\n$ `Good With Young Children`   &lt;dbl&gt; 5, 5, 5, 5, 3, 5, 5, 3, 5, 3, 3, 5, 5, 5,‚Ä¶\n$ `Good With Other Dogs`       &lt;dbl&gt; 5, 4, 3, 5, 3, 3, 5, 3, 4, 4, 4, 3, 3, 3,‚Ä¶\n$ `Shedding Level`             &lt;dbl&gt; 4, 3, 4, 4, 3, 1, 3, 3, 3, 2, 4, 3, 1, 2,‚Ä¶\n$ `Coat Grooming Frequency`    &lt;dbl&gt; 2, 1, 2, 2, 3, 4, 2, 1, 2, 2, 2, 2, 5, 2,‚Ä¶\n$ `Drooling Level`             &lt;dbl&gt; 2, 3, 2, 2, 3, 1, 1, 3, 2, 2, 1, 1, 1, 3,‚Ä¶\n$ `Coat Type`                  &lt;chr&gt; \"Double\", \"Smooth\", \"Double\", \"Double\", \"‚Ä¶\n$ `Coat Length`                &lt;chr&gt; \"Short\", \"Short\", \"Medium\", \"Medium\", \"Sh‚Ä¶\n$ `Openness To Strangers`      &lt;dbl&gt; 5, 5, 3, 5, 4, 5, 3, 3, 4, 4, 4, 3, 5, 4,‚Ä¶\n$ `Playfulness Level`          &lt;dbl&gt; 5, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4,‚Ä¶\n$ `Watchdog/Protective Nature` &lt;dbl&gt; 3, 3, 5, 3, 3, 5, 2, 5, 4, 4, 5, 3, 5, 4,‚Ä¶\n$ `Adaptability Level`         &lt;dbl&gt; 5, 5, 5, 5, 3, 4, 4, 4, 4, 4, 4, 3, 5, 3,‚Ä¶\n$ `Trainability Level`         &lt;dbl&gt; 5, 4, 5, 5, 4, 5, 3, 5, 5, 4, 4, 5, 4, 4,‚Ä¶\n$ `Energy Level`               &lt;dbl&gt; 5, 3, 5, 3, 3, 4, 4, 3, 5, 3, 4, 5, 4, 4,‚Ä¶\n$ `Barking Level`              &lt;dbl&gt; 3, 1, 3, 1, 2, 4, 4, 1, 3, 5, 4, 3, 4, 3,‚Ä¶\n$ `Mental Stimulation Needs`   &lt;dbl&gt; 4, 3, 5, 4, 3, 5, 4, 5, 5, 3, 4, 5, 4, 4,‚Ä¶\n\n\n\nglimpse(tuesdata$trait_description)\n\nRows: 16\nColumns: 4\n$ Trait       &lt;chr&gt; \"Affectionate With Family\", \"Good With Young Children\", \"G‚Ä¶\n$ Trait_1     &lt;chr&gt; \"Independent\", \"Not Recommended\", \"Not Recommended\", \"No S‚Ä¶\n$ Trait_5     &lt;chr&gt; \"Lovey-Dovey\", \"Good With Children\", \"Good With Other Dogs‚Ä¶\n$ Description &lt;chr&gt; \"How affectionate a breed is likely to be with family memb‚Ä¶\n\n\n\nglimpse(tuesdata$breed_rank)\n\nRows: 195\nColumns: 11\n$ Breed       &lt;chr&gt; \"Retrievers (Labrador)\", \"French Bulldogs\", \"German Shephe‚Ä¶\n$ `2013 Rank` &lt;dbl&gt; 1, 11, 2, 3, 5, 8, 4, 9, 13, 10, 24, 20, 6, 7, 16, 14, 18,‚Ä¶\n$ `2014 Rank` &lt;dbl&gt; 1, 9, 2, 3, 4, 7, 5, 10, 12, 11, 22, 18, 6, 8, 15, 13, 19,‚Ä¶\n$ `2015 Rank` &lt;dbl&gt; 1, 6, 2, 3, 4, 8, 5, 9, 11, 13, 20, 17, 7, 10, 15, 12, 18,‚Ä¶\n$ `2016 Rank` &lt;dbl&gt; 1, 6, 2, 3, 4, 7, 5, 8, 11, 13, 18, 16, 9, 10, 14, 12, 19,‚Ä¶\n$ `2017 Rank` &lt;dbl&gt; 1, 4, 2, 3, 5, 7, 6, 8, 10, 13, 15, 17, 9, 11, 14, 12, 19,‚Ä¶\n$ `2018 Rank` &lt;dbl&gt; 1, 4, 2, 3, 5, 7, 6, 8, 9, 12, 13, 15, 10, 11, 16, 14, 18,‚Ä¶\n$ `2019 Rank` &lt;dbl&gt; 1, 4, 2, 3, 5, 6, 7, 8, 9, 11, 10, 13, 12, 14, 17, 15, 16,‚Ä¶\n$ `2020 Rank` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,‚Ä¶\n$ links       &lt;chr&gt; \"https://www.akc.org/dog-breeds/labrador-retriever/\", \"htt‚Ä¶\n$ Image       &lt;chr&gt; \"https://www.akc.org/wp-content/uploads/2017/11/Labrador-R‚Ä¶",
    "crumbs": [
      "Recitation solutions",
      "Week 6 - ggplot102 solutions"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-2020",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-2020",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Top 5 breeds 2020",
    "text": "Top 5 breeds 2020\nWrite code to determine what the 5 most popular dog breeds in 2020 were. There are many ways to do this.\n\nclean_names()\nBecause each of these datasets has some non-conventional column names, I am going to run janitor::clean_names() on each df, and save them in my environment so I won‚Äôt have to keep using the dollar sign $ accessor. In this case, you can also access each data frame using the double [[]] brackets syntax.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nbreed_traits &lt;- clean_names(tuesdata$breed_traits)\ntrait_description &lt;- clean_names(tuesdata$trait_description)\nbreed_rank &lt;- clean_names(tuesdata$breed_rank)\n\n# can also use the double brackets\n# takes the first item in the list of tuesdata\n# breed_traits &lt;- tuesdata[[1]]\n\nThen let‚Äôs look at the beginning of each df.\n\nhead(breed_traits)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreed\naffectionate_with_family\ngood_with_young_children\ngood_with_other_dogs\nshedding_level\ncoat_grooming_frequency\ndrooling_level\ncoat_type\ncoat_length\nopenness_to_strangers\nplayfulness_level\nwatchdog_protective_nature\nadaptability_level\ntrainability_level\nenergy_level\nbarking_level\nmental_stimulation_needs\n\n\n\n\nRetrievers¬†(Labrador)\n5\n5\n5\n4\n2\n2\nDouble\nShort\n5\n5\n3\n5\n5\n5\n3\n4\n\n\nFrench¬†Bulldogs\n5\n5\n4\n3\n1\n3\nSmooth\nShort\n5\n5\n3\n5\n4\n3\n1\n3\n\n\nGerman¬†Shepherd¬†Dogs\n5\n5\n3\n4\n2\n2\nDouble\nMedium\n3\n4\n5\n5\n5\n5\n3\n5\n\n\nRetrievers¬†(Golden)\n5\n5\n5\n4\n2\n2\nDouble\nMedium\n5\n4\n3\n5\n5\n3\n1\n4\n\n\nBulldogs\n4\n3\n3\n3\n3\n3\nSmooth\nShort\n4\n4\n3\n3\n4\n3\n2\n3\n\n\nPoodles\n5\n5\n3\n1\n4\n1\nCurly\nLong\n5\n5\n5\n4\n5\n4\n4\n5\n\n\n\n\n\n\nhead(trait_description)\n\n\n\n\n\n\n\n\n\n\n\n\ntrait\ntrait_1\ntrait_5\ndescription\n\n\n\n\nAffectionate With Family\nIndependent\nLovey-Dovey\nHow affectionate a breed is likely to be with family members, or other people he knows well. Some breeds can be aloof with everyone but their owner, while other breeds treat everyone they know like their best friend.\n\n\nGood With Young Children\nNot Recommended\nGood With Children\nA breed‚Äôs level of tolerance and patience with childrens‚Äô behavior, and overall family-friendly nature. Dogs should always be supervised around young children, or children of any age who have little exposure to dogs.\n\n\nGood With Other Dogs\nNot Recommended\nGood With Other Dogs\nHow generally friendly a breed is towards other dogs. Dogs should always be supervised for interactions and introductions with other dogs, but some breeds are innately more likely to get along with other dogs, both at home and in public.\n\n\nShedding Level\nNo Shedding\nHair Everywhere\nHow much fur and hair you can expect the breed to leave behind. Breeds with high shedding will need to be brushed more frequently, are more likely to trigger certain types of allergies, and are more likely to require more consistent vacuuming and lint-rolling.\n\n\nCoat Grooming Frequency\nMonthly\nDaily\nHow frequently a breed requires bathing, brushing, trimming, or other kinds of coat maintenance. Consider how much time, patience, and budget you have for this type of care when looking at the grooming effort needed. All breeds require regular nail trimming.\n\n\nDrooling Level\nLess Likely to Drool\nAlways Have a Towel\nHow drool-prone a breed tends to be. If you‚Äôre a neat freak, dogs that can leave ropes of slobber on your arm or big wet spots on your clothes may not be the right choice for you.\n\n\n\n\n\n\nhead(breed_rank)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreed\nx2013_rank\nx2014_rank\nx2015_rank\nx2016_rank\nx2017_rank\nx2018_rank\nx2019_rank\nx2020_rank\nlinks\nimage\n\n\n\n\nRetrievers (Labrador)\n1\n1\n1\n1\n1\n1\n1\n1\nhttps://www.akc.org/dog-breeds/labrador-retriever/\nhttps://www.akc.org/wp-content/uploads/2017/11/Labrador-Retriever-illustration.jpg\n\n\nFrench Bulldogs\n11\n9\n6\n6\n4\n4\n4\n2\nhttps://www.akc.org/dog-breeds/french-bulldog/\nhttps://www.akc.org/wp-content/uploads/2017/11/French-Bulldog-Illo-2.jpg\n\n\nGerman Shepherd Dogs\n2\n2\n2\n2\n2\n2\n2\n3\nhttps://www.akc.org/dog-breeds/german-shepherd-dog/\nhttps://www.akc.org/wp-content/uploads/2017/11/German-Shepherd-Dog-Illo-2.jpg\n\n\nRetrievers (Golden)\n3\n3\n3\n3\n3\n3\n3\n4\nhttps://www.akc.org/dog-breeds/golden-retriever/\nhttps://www.akc.org/wp-content/uploads/2017/11/Golden-Retriever-Illo-2.jpg\n\n\nBulldogs\n5\n4\n4\n4\n5\n5\n5\n5\nhttps://www.akc.org/dog-breeds/bulldog/\nhttps://www.akc.org/wp-content/uploads/2017/11/Bulldog-Illo-2.jpg\n\n\nPoodles\n8\n7\n8\n7\n7\n7\n6\n6\nhttps://www.akc.org/dog-breeds/poodle-standard/\nhttps://www.akc.org/wp-content/uploads/2017/11/Standard-Poodle-illustration.jpg\n\n\n\n\n\nWhat are the most popular breeds in 2020, three ways.\n\nbreed_rank |&gt;\n  filter(x2020_rank &lt;= 5)\n\n# A tibble: 5 √ó 11\n  breed        x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank x2018_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers ‚Ä¶          1          1          1          1          1          1\n2 French Bull‚Ä¶         11          9          6          6          4          4\n3 German Shep‚Ä¶          2          2          2          2          2          2\n4 Retrievers ‚Ä¶          3          3          3          3          3          3\n5 Bulldogs              5          4          4          4          5          5\n# ‚Ñπ 4 more variables: x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;, links &lt;chr&gt;,\n#   image &lt;chr&gt;\n\nbreed_rank |&gt;\n  arrange(x2020_rank) |&gt;\n  slice(1:5)\n\n# A tibble: 5 √ó 11\n  breed        x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank x2018_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers ‚Ä¶          1          1          1          1          1          1\n2 French Bull‚Ä¶         11          9          6          6          4          4\n3 German Shep‚Ä¶          2          2          2          2          2          2\n4 Retrievers ‚Ä¶          3          3          3          3          3          3\n5 Bulldogs              5          4          4          4          5          5\n# ‚Ñπ 4 more variables: x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;, links &lt;chr&gt;,\n#   image &lt;chr&gt;\n\nbreed_rank |&gt;\n  slice_min(x2020_rank, n = 5) # min because a low rank is \"high\"\n\n# A tibble: 5 √ó 11\n  breed        x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank x2018_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers ‚Ä¶          1          1          1          1          1          1\n2 French Bull‚Ä¶         11          9          6          6          4          4\n3 German Shep‚Ä¶          2          2          2          2          2          2\n4 Retrievers ‚Ä¶          3          3          3          3          3          3\n5 Bulldogs              5          4          4          4          5          5\n# ‚Ñπ 4 more variables: x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;, links &lt;chr&gt;,\n#   image &lt;chr&gt;",
    "crumbs": [
      "Recitation solutions",
      "Week 6 - ggplot102 solutions"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-across-2013-2020",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#top-5-breeds-across-2013-2020",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Top 5 breeds across 2013-2020",
    "text": "Top 5 breeds across 2013-2020\nWhat are the 5 most popular and the 5 least popular dogs across this time frame? There are many ways to do this. Hint: create a new variable that is a sum of all the ranks from 2013, allowing a composite score of the popularity of each dog breed across this time period.\n\n# most popular\nbreed_rank |&gt;\n  rowwise() |&gt; # perform calculations rowwise\n  mutate(rank_sum = sum(across(x2013_rank:x2020_rank))) |&gt;\n  ungroup() |&gt; # remove rowwise\n  select(breed, rank_sum, everything()) |&gt;\n  slice_min(n = 5, order_by = rank_sum) |&gt; # take lowest 5 values by rank_sum\n  knitr::kable() # makes a nice formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreed\nrank_sum\nx2013_rank\nx2014_rank\nx2015_rank\nx2016_rank\nx2017_rank\nx2018_rank\nx2019_rank\nx2020_rank\nlinks\nimage\n\n\n\n\nRetrievers (Labrador)\n8\n1\n1\n1\n1\n1\n1\n1\n1\nhttps://www.akc.org/dog-breeds/labrador-retriever/\nhttps://www.akc.org/wp-content/uploads/2017/11/Labrador-Retriever-illustration.jpg\n\n\nGerman Shepherd Dogs\n17\n2\n2\n2\n2\n2\n2\n2\n3\nhttps://www.akc.org/dog-breeds/german-shepherd-dog/\nhttps://www.akc.org/wp-content/uploads/2017/11/German-Shepherd-Dog-Illo-2.jpg\n\n\nRetrievers (Golden)\n25\n3\n3\n3\n3\n3\n3\n3\n4\nhttps://www.akc.org/dog-breeds/golden-retriever/\nhttps://www.akc.org/wp-content/uploads/2017/11/Golden-Retriever-Illo-2.jpg\n\n\nBulldogs\n37\n5\n4\n4\n4\n5\n5\n5\n5\nhttps://www.akc.org/dog-breeds/bulldog/\nhttps://www.akc.org/wp-content/uploads/2017/11/Bulldog-Illo-2.jpg\n\n\nBeagles\n45\n4\n5\n5\n5\n6\n6\n7\n7\nhttps://www.akc.org/dog-breeds/beagle/\nhttps://www.akc.org/wp-content/uploads/2017/11/Beagle-Illo-2.jpg\n\n\n\n\n# least popular            \nbreed_rank |&gt;\n  rowwise() |&gt;\n  mutate(rank_sum = sum(across(x2013_rank:x2020_rank))) |&gt;\n  ungroup() |&gt;\n  select(breed, rank_sum, everything()) |&gt;\n  arrange(rank_sum) |&gt;\n  slice_max(n = 5, order_by = rank_sum) |&gt; # max bc here higher sum is less popular\n  knitr::kable() # makes a nice formatted table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreed\nrank_sum\nx2013_rank\nx2014_rank\nx2015_rank\nx2016_rank\nx2017_rank\nx2018_rank\nx2019_rank\nx2020_rank\nlinks\nimage\n\n\n\n\nNorwegian Lundehunds\n1499\n175\n184\n182\n188\n190\n191\n194\n195\nhttps://www.akc.org/dog-breeds/norwegian-lundehund/\nhttps://www.akc.org/wp-content/uploads/2017/11/Norwegian-Lundehund-Illo-2.jpg\n\n\nEnglish Foxhounds\n1497\n177\n183\n184\n187\n189\n188\n195\n194\nhttps://www.akc.org/dog-breeds/english-foxhound/\nhttps://www.akc.org/wp-content/uploads/2017/11/English-Foxhound-Illo-2.jpg\n\n\nAmerican Foxhounds\n1482\n176\n180\n181\n189\n187\n186\n191\n192\nhttps://www.akc.org/dog-breeds/american-foxhound/\nhttps://www.akc.org/wp-content/uploads/2017/11/American-Foxhound-Illo-2.jpg\n\n\nHarriers\n1473\n173\n181\n183\n186\n183\n189\n188\n190\nhttps://www.akc.org/dog-breeds/harrier/\nhttps://www.akc.org/wp-content/uploads/2017/11/Harrier-Illo-2.jpg\n\n\nCesky Terriers\n1468\n174\n182\n179\n182\n185\n185\n190\n191\nhttps://www.akc.org/dog-breeds/cesky-terrier/\nhttps://www.akc.org/wp-content/uploads/2017/11/Cesky-Terrier-Illo-2.jpg\n\n\n\n\n\nAnother way to do it still with wide data.\n\n# create a vector with the rowSums for the ranks from 2013 to 2020\nbreed_rank_2013to2020 &lt;- breed_rank |&gt;\n  select(x2013_rank:x2020_rank) |&gt;\n  rowSums() |&gt; # calculate the sum across the rows\n  as.data.frame() |&gt; # convert from vector to dataframe\n  rename(rank_sum = 1) # rename the first and only column to rank_sum \n\n# bind back to the rest of the data\nbreed_rank_sum &lt;- bind_cols(breed_rank, breed_rank_2013to2020) |&gt;\n  select(breed, rank_sum, everything()) # move breed and rank_sum to front\n\n# most popular\nbreed_rank_sum |&gt;\n  arrange(rank_sum) |&gt;\n  slice_min(n = 5, order_by = rank_sum)\n\n# A tibble: 5 √ó 12\n  breed          rank_sum x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Retrievers (L‚Ä¶        8          1          1          1          1          1\n2 German Shephe‚Ä¶       17          2          2          2          2          2\n3 Retrievers (G‚Ä¶       25          3          3          3          3          3\n4 Bulldogs             37          5          4          4          4          5\n5 Beagles              45          4          5          5          5          6\n# ‚Ñπ 5 more variables: x2018_rank &lt;dbl&gt;, x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;,\n#   links &lt;chr&gt;, image &lt;chr&gt;\n\n# least popular\nbreed_rank_sum |&gt;\n  arrange(rank_sum) |&gt;\n  slice_max(n = 5, order_by = rank_sum)\n\n# A tibble: 5 √ó 12\n  breed          rank_sum x2013_rank x2014_rank x2015_rank x2016_rank x2017_rank\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Norwegian Lun‚Ä¶     1499        175        184        182        188        190\n2 English Foxho‚Ä¶     1497        177        183        184        187        189\n3 American Foxh‚Ä¶     1482        176        180        181        189        187\n4 Harriers           1473        173        181        183        186        183\n5 Cesky Terriers     1468        174        182        179        182        185\n# ‚Ñπ 5 more variables: x2018_rank &lt;dbl&gt;, x2019_rank &lt;dbl&gt;, x2020_rank &lt;dbl&gt;,\n#   links &lt;chr&gt;, image &lt;chr&gt;\n\n\nA third way with long data. This is my preferred way especially since we need long data later to plot.\n\n# create long data cleaned\nbreed_rank_sum_long &lt;- breed_rank |&gt;\n  pivot_longer(cols = x2013_rank:x2020_rank, \n               names_to = \"year\", \n               values_to = \"rank\") |&gt;\n  separate(col = year, \n           sep = \"_\", \n           into = c(\"year\", \"extra\")) |&gt; # separate year column parts\n  mutate(year = str_remove(string = year, pattern = \"x\")) |&gt; # remove the \"x\"\n  select(breed, year, rank)\n\nhead(breed_rank_sum_long)\n\n# A tibble: 6 √ó 3\n  breed                 year   rank\n  &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt;\n1 Retrievers (Labrador) 2013      1\n2 Retrievers (Labrador) 2014      1\n3 Retrievers (Labrador) 2015      1\n4 Retrievers (Labrador) 2016      1\n5 Retrievers (Labrador) 2017      1\n6 Retrievers (Labrador) 2018      1\n\n# most popular\nbreed_rank_sum_long |&gt;\n  group_by(breed) |&gt; # do operation by breed\n  summarize(rank_sum = sum(rank)) |&gt; # add all ranks\n  arrange(rank_sum) |&gt; # actually not necessary\n  slice_min(order_by = rank_sum, n = 5)\n\n# A tibble: 5 √ó 2\n  breed                 rank_sum\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Retrievers (Labrador)        8\n2 German Shepherd Dogs        17\n3 Retrievers (Golden)         25\n4 Bulldogs                    37\n5 Beagles                     45\n\n# least popular\nbreed_rank_sum_long |&gt;\n  group_by(breed) |&gt; # do operation by breed\n  summarize(rank_sum = sum(rank)) |&gt; # add all ranks\n  arrange(desc(rank_sum)) |&gt; # actually not necessary\n  slice_max(order_by = rank_sum, n = 5)\n\n# A tibble: 5 √ó 2\n  breed                rank_sum\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Norwegian Lundehunds     1499\n2 English Foxhounds        1497\n3 American Foxhounds       1482\n4 Harriers                 1473\n5 Cesky Terriers           1468",
    "crumbs": [
      "Recitation solutions",
      "Week 6 - ggplot102 solutions"
    ]
  },
  {
    "objectID": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#popularity-ranking-2013-2020",
    "href": "modules/module2/06_ggplot102/06_ggplot102_recitation_solutions.html#popularity-ranking-2013-2020",
    "title": "ggplot 102: Facets, Scales, Labels, and Themes (but now üê∂) recitation solutions",
    "section": "Popularity ranking 2013-2020",
    "text": "Popularity ranking 2013-2020\nCreate a plot where you take the 12 most popular dogs from 2020, and plot their popularity rank from 2013 to 2020. Hint, to facet, you need to have the variable you want to facet in one column.\nI‚Äôm showing you a slightly different way to clean up your long dataframe here.\n\n# go from wide to long data or \"tidy\" data\nbreed_rank_tidy &lt;- breed_rank |&gt;\n  pivot_longer(cols = x2013_rank:x2020_rank,\n              names_to = \"year\",\n              values_to = \"rank\")\n\n# remove extra character in year and assign it back\nbreed_rank_tidy$year &lt;- breed_rank_tidy$year |&gt;\n  str_remove(pattern = \"x\") |&gt;\n  str_remove(pattern = \"_rank\")\n\nThe %in% operator checks to see if a value is within a vector of values. In this case, we are checking to see whether the variable breed is within the 12 breeds we have filtered for (i.e., dogs_to_include$breed). Here is a page with some examples of using %in%.\n\n# what dogs should I include?\n# create a df including only top the 12 dogs in 2020\ndogs_to_include &lt;- breed_rank_tidy |&gt;\n  filter(year == 2020 & rank &lt;= 12) \n\n# plot\nbreed_rank_tidy |&gt;\n  filter(breed %in% dogs_to_include$breed) |&gt;\n  ggplot(aes(x = year, y = rank)) +\n    geom_point() +\n    facet_wrap(vars(breed), \n               labeller = labeller(breed = label_wrap_gen(20))) +\n    theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nAlter the aesthetics of this plot until you think it looks good.\n\nbreed_rank_tidy |&gt;\n  filter(breed %in% dogs_to_include$breed) |&gt;\n  ggplot(aes(x = year, y = rank, group = breed)) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(vars(fct_reorder(breed, rank))) + # orders facets by mean rank so more popular first\n    scale_y_reverse(breaks = seq(1, 25, 4)) + # reverse so that lower number rank is at the top and set the labels to start at 1, increment by 4 until 25 since a zero rank doesn't exist\n    theme_minimal() + \n    theme(axis.text.x = element_text(angle = 90), # make x-axis labels on 90degree angle\n          strip.text = element_text(size = 8), # change strip text size\n          panel.grid.major.x = element_blank()) + # remove some grid lines\n    labs(x = \"Year\",\n         y = \"Popularity rank among all AKC dogs\",\n         title = \"Popularity of 12 Most Popular AKC Dog Breeds in 2020 from 2013-2020\",\n         subtitle = \"Labrador retrievers are so popular!\")\n\n\n\n\n\n\n\n\nWhat dog has jumped in the rankings most from 2013 to 2020? What has dropped the most?\n\nbreed_rank |&gt;\n  mutate(rank_inc = (x2020_rank - x2013_rank)) |&gt;\n  select(breed, rank_inc, everything()) |&gt;\n  slice_min(n = 1, order_by = rank_inc) |&gt;\n  select(breed, rank_inc, x2013_rank, x2020_rank)\n\n# A tibble: 1 √ó 4\n  breed             rank_inc x2013_rank x2020_rank\n  &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Spaniels (Boykin)      -34        121         87\n\nbreed_rank |&gt;\n  mutate(rank_inc = (x2020_rank - x2013_rank)) |&gt;\n  select(breed, rank_inc, everything()) |&gt;\n  slice_max(n = 1, order_by = rank_inc) |&gt;\n  select(breed, rank_inc, x2013_rank, x2020_rank)\n\n# A tibble: 1 √ó 4\n  breed                     rank_inc x2013_rank x2020_rank\n  &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Treeing Walker Coonhounds       52        101        153",
    "crumbs": [
      "Recitation solutions",
      "Week 6 - ggplot102 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "We are going to practice using ggplot2 today, focusing wrangling data, mapping variables to aesthetics, and adding geoms.\nWe are going to use data from the TidyTuesday project. For this recitation, we are going to use the Giant Pumpkins data which is collected from the Great Pumpkin Commonwealth. You can learn more about how the data is structured here.\nToday, you are going to make this plot:\n\n\n\nOur plot for today\n\n\n\n\nFirst we will load the tidyverse. The tidyverse also contains the package lubridate which you might use to help later with dates.\n\nlibrary(tidyverse)\n\n\n\n\nYou can read in the data directly from a url or you can download the file and read it in locally. To get the file locally, you can go to this link, right click on Raw, and save link as pumpkins.txt in your working directory.\n\n# from a url\npumpkins_raw &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-19/pumpkins.csv')\n\nRows: 28065 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (14): id, place, weight_lbs, grower_name, city, state_prov, country, gpc...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# download and read in from your computer\npumpkins_raw &lt;- read_csv(\"pumpkins.txt\")",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#load-packages",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#load-packages",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "First we will load the tidyverse. The tidyverse also contains the package lubridate which you might use to help later with dates.\n\nlibrary(tidyverse)",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#read-in-data",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#read-in-data",
    "title": "ggplot 101 recitation üéÉ",
    "section": "",
    "text": "You can read in the data directly from a url or you can download the file and read it in locally. To get the file locally, you can go to this link, right click on Raw, and save link as pumpkins.txt in your working directory.\n\n# from a url\npumpkins_raw &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-19/pumpkins.csv')\n\nRows: 28065 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (14): id, place, weight_lbs, grower_name, city, state_prov, country, gpc...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# download and read in from your computer\npumpkins_raw &lt;- read_csv(\"pumpkins.txt\")",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#turn-one-character-column-into-two",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#turn-one-character-column-into-two",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Turn one character column into two ‚úÇÔ∏è",
    "text": "Turn one character column into two ‚úÇÔ∏è\nFrom both looking at the data, and reading about the variable id on the documentation page, you see that it contains two type of observations. Since we want to be able to filter for only Giant Pumpkins, and we will want to plot x = year we need to separate them.\nTo use them separately, we need to separate this column into two columns such like year and type.\nTry doing this with the function separate() or separate_wider_delim(). I will show you both ways and then just use separate() for ther rest of the example.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\", # indicate what the separator is\n           remove = FALSE) # set FALSE if you want to keep id, otherwise id will be removed\n\n# A tibble: 28,065 √ó 16\n   id     year  type  place weight_lbs grower_name      city  state_prov country\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;  \n 1 2013-F 2013  F     1     154.50     Ellenbecker, To‚Ä¶ Glea‚Ä¶ Wisconsin  United‚Ä¶\n 2 2013-F 2013  F     2     146.50     Razo, Steve      New ‚Ä¶ Ohio       United‚Ä¶\n 3 2013-F 2013  F     3     145.00     Ellenbecker, To‚Ä¶ Glen‚Ä¶ Wisconsin  United‚Ä¶\n 4 2013-F 2013  F     4     140.80     Martin, Margare‚Ä¶ Comb‚Ä¶ Wisconsin  United‚Ä¶\n 5 2013-F 2013  F     5     139.00     Barlow, John     &lt;NA&gt;  Wisconsin  United‚Ä¶\n 6 2013-F 2013  F     5     139.00     Werner, Quinn    Saeg‚Ä¶ Pennsylva‚Ä¶ United‚Ä¶\n 7 2013-F 2013  F     7     136.50     Treece, Jef      West‚Ä¶ Oregon     United‚Ä¶\n 8 2013-F 2013  F     8     136.00     Werner, Quinn    Saeg‚Ä¶ Pennsylva‚Ä¶ United‚Ä¶\n 9 2013-F 2013  F     9     134.50     Rose, Jerry      Hunt‚Ä¶ Ohio       United‚Ä¶\n10 2013-F 2013  F     10    134.00     Coolen, Russell  Bout‚Ä¶ Nova Scot‚Ä¶ Canada \n# ‚Ñπ 28,055 more rows\n# ‚Ñπ 7 more variables: gpc_site &lt;chr&gt;, seed_mother &lt;chr&gt;,\n#   pollinator_father &lt;chr&gt;, ott &lt;chr&gt;, est_weight &lt;chr&gt;, pct_chart &lt;chr&gt;,\n#   variety &lt;chr&gt;\n\n\n\npumpkins_raw |&gt; \n  separate_wider_delim(cols = id,\n                       delim = \"-\", # delimiter is a hyphen\n                       names = c(\"year\", \"type\"), # combine year and type as a vector\n                       cols_remove = FALSE) # to keep id, not required\n\n# A tibble: 28,065 √ó 16\n   year  type  id     place weight_lbs grower_name      city  state_prov country\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;  \n 1 2013  F     2013-F 1     154.50     Ellenbecker, To‚Ä¶ Glea‚Ä¶ Wisconsin  United‚Ä¶\n 2 2013  F     2013-F 2     146.50     Razo, Steve      New ‚Ä¶ Ohio       United‚Ä¶\n 3 2013  F     2013-F 3     145.00     Ellenbecker, To‚Ä¶ Glen‚Ä¶ Wisconsin  United‚Ä¶\n 4 2013  F     2013-F 4     140.80     Martin, Margare‚Ä¶ Comb‚Ä¶ Wisconsin  United‚Ä¶\n 5 2013  F     2013-F 5     139.00     Barlow, John     &lt;NA&gt;  Wisconsin  United‚Ä¶\n 6 2013  F     2013-F 5     139.00     Werner, Quinn    Saeg‚Ä¶ Pennsylva‚Ä¶ United‚Ä¶\n 7 2013  F     2013-F 7     136.50     Treece, Jef      West‚Ä¶ Oregon     United‚Ä¶\n 8 2013  F     2013-F 8     136.00     Werner, Quinn    Saeg‚Ä¶ Pennsylva‚Ä¶ United‚Ä¶\n 9 2013  F     2013-F 9     134.50     Rose, Jerry      Hunt‚Ä¶ Ohio       United‚Ä¶\n10 2013  F     2013-F 10    134.00     Coolen, Russell  Bout‚Ä¶ Nova Scot‚Ä¶ Canada \n# ‚Ñπ 28,055 more rows\n# ‚Ñπ 7 more variables: gpc_site &lt;chr&gt;, seed_mother &lt;chr&gt;,\n#   pollinator_father &lt;chr&gt;, ott &lt;chr&gt;, est_weight &lt;chr&gt;, pct_chart &lt;chr&gt;,\n#   variety &lt;chr&gt;",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#select-observations-by-their-values",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#select-observations-by-their-values",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Select observations by their values üéÉ",
    "text": "Select observations by their values üéÉ\nNow that you separated id into year and type we want to keep only the data for Giant Pumpkins, and only for the first place pumpkins. Giant Pumpkins have the code ‚ÄúP‚Äù and we can tell that from the documentation about this dataset that was linked at the top of the recitation.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") # only keep type P AND first place\n\n# A tibble: 9 √ó 16\n  id     year  type  place weight_lbs grower_name       city  state_prov country\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;  \n1 2013-P 2013  P     1     2,032.00   Mathison, Tim     Napa  California United‚Ä¶\n2 2014-P 2014  P     1     2,323.70   Meier, Beni       Pfun‚Ä¶ Other      Switze‚Ä¶\n3 2015-P 2015  P     1     2,230.50   Wallace, Ron      Gree‚Ä¶ Rhode Isl‚Ä¶ United‚Ä¶\n4 2016-P 2016  P     1     2,624.60   Willemijns, Math‚Ä¶ Deur‚Ä¶ East Flan‚Ä¶ Belgium\n5 2017-P 2017  P     1     2,363.00   Holland, Joel     Sumn‚Ä¶ Washington United‚Ä¶\n6 2018-P 2018  P     1     2,528.00   Geddes, Steve     Bosc‚Ä¶ New Hamps‚Ä¶ United‚Ä¶\n7 2019-P 2019  P     1     2,517.00   Haist, Karl & Be‚Ä¶ Clar‚Ä¶ New York   United‚Ä¶\n8 2020-P 2020  P     1     2,593.70   Paton, Ian & Stu‚Ä¶ Ever‚Ä¶ England    United‚Ä¶\n9 2021-P 2021  P     1     2,702.90   Cutrupi, Stefano  Radd‚Ä¶ Tuscany    Italy  \n# ‚Ñπ 7 more variables: gpc_site &lt;chr&gt;, seed_mother &lt;chr&gt;,\n#   pollinator_father &lt;chr&gt;, ott &lt;chr&gt;, est_weight &lt;chr&gt;, pct_chart &lt;chr&gt;,\n#   variety &lt;chr&gt;",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#remove-pesky-strings",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#remove-pesky-strings",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Remove pesky strings üòë",
    "text": "Remove pesky strings üòë\nLet‚Äôs see what plotting would look like now:\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  ggplot(aes(x = year, y = weight_lbs)) +\n    geom_point()\n\n\n\n\n\n\n\n\nIf we try and make a line plot it just simply gives us no plot.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  ggplot(aes(x = year, y = weight_lbs)) +\n    geom_line()\n\n`geom_line()`: Each group consists of only one observation.\n‚Ñπ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nIf we look at our data, we see that both year and weight_lbs are actually character strings and weight_lbs has a comma that is preventing R from viewing it as a number. To plot them to x and y, they need to be numeric so let‚Äôs fix that.\nIf we simply just trying and convert weight_lbs to be numeric using as.numeric() this does not work. We just end up with a column on NAs. Wow this comma gets in the way!\n\n# doesn't convert weight_lbs to numeric\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = as.numeric(weight_lbs))\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `weight_lbs = as.numeric(weight_lbs)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 9 √ó 16\n  id     year  type  place weight_lbs grower_name       city  state_prov country\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;  \n1 2013-P 2013  P     1             NA Mathison, Tim     Napa  California United‚Ä¶\n2 2014-P 2014  P     1             NA Meier, Beni       Pfun‚Ä¶ Other      Switze‚Ä¶\n3 2015-P 2015  P     1             NA Wallace, Ron      Gree‚Ä¶ Rhode Isl‚Ä¶ United‚Ä¶\n4 2016-P 2016  P     1             NA Willemijns, Math‚Ä¶ Deur‚Ä¶ East Flan‚Ä¶ Belgium\n5 2017-P 2017  P     1             NA Holland, Joel     Sumn‚Ä¶ Washington United‚Ä¶\n6 2018-P 2018  P     1             NA Geddes, Steve     Bosc‚Ä¶ New Hamps‚Ä¶ United‚Ä¶\n7 2019-P 2019  P     1             NA Haist, Karl & Be‚Ä¶ Clar‚Ä¶ New York   United‚Ä¶\n8 2020-P 2020  P     1             NA Paton, Ian & Stu‚Ä¶ Ever‚Ä¶ England    United‚Ä¶\n9 2021-P 2021  P     1             NA Cutrupi, Stefano  Radd‚Ä¶ Tuscany    Italy  \n# ‚Ñπ 7 more variables: gpc_site &lt;chr&gt;, seed_mother &lt;chr&gt;,\n#   pollinator_father &lt;chr&gt;, ott &lt;chr&gt;, est_weight &lt;chr&gt;, pct_chart &lt;chr&gt;,\n#   variety &lt;chr&gt;\n\n\nWe can remove the comma by using the function str_remove() nested within a mutate() call to actually change the column. Remember, we don‚Äôt want to just remove the thousands place comma in one number, we want to edit the dataset to remove the comma.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) # remove comma\n\n# A tibble: 9 √ó 16\n  id     year  type  place weight_lbs grower_name       city  state_prov country\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;  \n1 2013-P 2013  P     1     2032.00    Mathison, Tim     Napa  California United‚Ä¶\n2 2014-P 2014  P     1     2323.70    Meier, Beni       Pfun‚Ä¶ Other      Switze‚Ä¶\n3 2015-P 2015  P     1     2230.50    Wallace, Ron      Gree‚Ä¶ Rhode Isl‚Ä¶ United‚Ä¶\n4 2016-P 2016  P     1     2624.60    Willemijns, Math‚Ä¶ Deur‚Ä¶ East Flan‚Ä¶ Belgium\n5 2017-P 2017  P     1     2363.00    Holland, Joel     Sumn‚Ä¶ Washington United‚Ä¶\n6 2018-P 2018  P     1     2528.00    Geddes, Steve     Bosc‚Ä¶ New Hamps‚Ä¶ United‚Ä¶\n7 2019-P 2019  P     1     2517.00    Haist, Karl & Be‚Ä¶ Clar‚Ä¶ New York   United‚Ä¶\n8 2020-P 2020  P     1     2593.70    Paton, Ian & Stu‚Ä¶ Ever‚Ä¶ England    United‚Ä¶\n9 2021-P 2021  P     1     2702.90    Cutrupi, Stefano  Radd‚Ä¶ Tuscany    Italy  \n# ‚Ñπ 7 more variables: gpc_site &lt;chr&gt;, seed_mother &lt;chr&gt;,\n#   pollinator_father &lt;chr&gt;, ott &lt;chr&gt;, est_weight &lt;chr&gt;, pct_chart &lt;chr&gt;,\n#   variety &lt;chr&gt;\n\n\nCommas, gone! üëèüëèüëè",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#convert-character-to-numeric",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#convert-character-to-numeric",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Convert character to numeric üî¢",
    "text": "Convert character to numeric üî¢\nNow the comma is gone, you can simply change the variable weight_lbs from a character to numeric, so it can be plotted like a number. To change the column type, we are going to use the as.numeric() function. Here‚Äôs some example about how to use as.numeric().\nLet‚Äôs add this to our growing pipe.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) # convert weight_lbs to numeric\n\n# A tibble: 9 √ó 16\n  id     year  type  place weight_lbs grower_name       city  state_prov country\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;  \n1 2013-P 2013  P     1          2032  Mathison, Tim     Napa  California United‚Ä¶\n2 2014-P 2014  P     1          2324. Meier, Beni       Pfun‚Ä¶ Other      Switze‚Ä¶\n3 2015-P 2015  P     1          2230. Wallace, Ron      Gree‚Ä¶ Rhode Isl‚Ä¶ United‚Ä¶\n4 2016-P 2016  P     1          2625. Willemijns, Math‚Ä¶ Deur‚Ä¶ East Flan‚Ä¶ Belgium\n5 2017-P 2017  P     1          2363  Holland, Joel     Sumn‚Ä¶ Washington United‚Ä¶\n6 2018-P 2018  P     1          2528  Geddes, Steve     Bosc‚Ä¶ New Hamps‚Ä¶ United‚Ä¶\n7 2019-P 2019  P     1          2517  Haist, Karl & Be‚Ä¶ Clar‚Ä¶ New York   United‚Ä¶\n8 2020-P 2020  P     1          2594. Paton, Ian & Stu‚Ä¶ Ever‚Ä¶ England    United‚Ä¶\n9 2021-P 2021  P     1          2703. Cutrupi, Stefano  Radd‚Ä¶ Tuscany    Italy  \n# ‚Ñπ 7 more variables: gpc_site &lt;chr&gt;, seed_mother &lt;chr&gt;,\n#   pollinator_father &lt;chr&gt;, ott &lt;chr&gt;, est_weight &lt;chr&gt;, pct_chart &lt;chr&gt;,\n#   variety &lt;chr&gt;",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#convert-year-to-numeric-or-a-date",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#convert-year-to-numeric-or-a-date",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Convert year to numeric or a date",
    "text": "Convert year to numeric or a date\nThe lines aren‚Äôt showing up because year is a character variable. We can change year to be either a number or a date.\nConverting year to numeric.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.numeric(year)) |&gt; # convert year to numeric\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point() + # add points\n    geom_line() # add lines\n\n\n\n\n\n\n\n\nThis creates some issues with the x-axis tick labels - since 2017.5 isn‚Äôt a meaningful year. You can change that with scale_*() by setting the axis breaks but we haven‚Äôt learned how to do that yet.\n\n# scale_x_continous lets you scale how year is mapped to x\n# seq gives you a sequence, here from 2013 to 2021, by 2s\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.numeric(year)) |&gt; # convert year to numeric\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point() + # add points\n    geom_line() + # add lines\n    scale_x_continuous(breaks = seq(2013, 2021, 2)) # adjust x-axis breaks\n\n\n\n\n\n\n\n\nCan convert to date using as.Date() with some arguments.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point() + # add points\n    geom_line()  # add lines\n\n\n\n\n\n\n\n\nYou can also convert year to be a date using functions from the package lubridate and the function ymd().\n\n# ymd() converts characters to dates\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = ymd(year, truncated = 2L)) |&gt; # truncated allows for incomplete dates\n  ggplot(aes(year, weight_lbs)) + \n    geom_point() + \n    geom_line()",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#playing-around",
    "href": "modules/module2/05_ggplot101/05_ggplot101_recitation_solutions.html#playing-around",
    "title": "ggplot 101 recitation üéÉ",
    "section": "Playing around",
    "text": "Playing around\n\nOther plot types\nTry using different geoms besides geom_point() and geom_line(). Which ones might make sense in this situation?\nA bar chart. Note that you can pass color also as hex codes and R is smart enough to highlight those colors with that color if you use RStudio and RMarkdown/Quarto.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_col(color = \"black\", fill = \"#d36e0e\")\n\n\n\n\n\n\n\n\nA lollypop chart. shape = 21 is my favorite shape, its the circle where you can control the color the outline with color and the color of the inside with fill.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_segment(aes(x = year, xend = year, y = 0, yend = weight_lbs)) + # create lines\n    geom_point(fill = \"#d36e0e\", color = \"black\", size = 3, shape = 21)  # add points\n\n\n\n\n\n\n\n\n\n\nBlue lines\nCan you color all the lines blue?\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point() + # points stay default\n    geom_line(color = \"blue\") # lines blue\n\n\n\n\n\n\n\n\n\n\nColor by year\nCan you color the data based on year?\nIf you color a continuous variable by color, you will get a continuous color scale.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point(aes(fill = year), shape = 21) + # color points by year\n    geom_line()\n\n\n\n\n\n\n\n\nIf you want that color scale to be discrete, you can either convert year to a factor in your dataset, or just convert it to a factor for your plot.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point(aes(fill = as.factor(year)), shape = 21) + # year as factor for discrete colors\n    geom_line()\n\n\n\n\n\n\n\n\n\n\nColor and shape by country\nCan you change color and change shape based on country?\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" & place == \"1\") |&gt;  # only type P AND first place\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x = year, y = weight_lbs)) + # map year to x, weight_lbs to y in aes()\n    geom_point(aes(color = as.factor(country), shape = country), size = 3) + # year as factor for discrete colors\n    geom_line()\n\n\n\n\n\n\n\n\n\n\nDistribution of all giant pumpkins in 2021\nCan you make a plot showing the distribution of weights of all giant pumpkins entered in 2021?\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\") |&gt;  # only type P \n  filter(year == 2021) |&gt; # only pumpkins from 2021\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x =  weight_lbs)) + # map weight_lbs to x within aes()\n    geom_density()\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `weight_lbs = as.numeric(weight_lbs)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nCould also do a histogram instead.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\") |&gt;  # only type P \n  filter(year == 2021) |&gt; # only pumpkins from 2021\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x =  weight_lbs)) + # map weight_lbs to x within aes()\n    geom_histogram(color = \"black\", fill = \"orange\") # can set number of bins or binwidth too\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `weight_lbs = as.numeric(weight_lbs)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\nBoxplot of weights for all years\nCan you make a boxplot showing the distribution of weights of all giant pumpkins across all years? Also can you add all the datapoints on top of the boxplot? Is this a good idea? Might there be a better geom to use than a boxplot?\nNote that we are removed the filtering for only first place pumpkins.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" ) |&gt;  # only type P - but all pumpkins now\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x =  year, y = weight_lbs, group = year)) + # group = year to boxplot by year\n    geom_boxplot(outlier.shape = NA) + # don't plot outliers on boxplot layer\n    geom_jitter(alpha = 0.1) # lighten points to avoid overplotting\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `weight_lbs = as.numeric(weight_lbs)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: Removed 9 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nA violin plot instead.\n\npumpkins_raw |&gt; \n  separate(col = id,\n           into = c(\"year\", \"type\"), # combine year and type as a vector\n           sep = \"-\",\n           remove = FALSE) |&gt;  # if you want to keep id, not required\n  filter(type == \"P\" ) |&gt;  # only type P - but all pumpkins now\n  mutate(weight_lbs = str_remove(weight_lbs, \",\")) |&gt; # remove comma\n  mutate(weight_lbs = as.numeric(weight_lbs)) |&gt; # convert weight_lbs to numeric\n  mutate(year = as.Date(year, \"%Y\")) |&gt; # convert to date in format year\n  ggplot(aes(x =  year, y = weight_lbs, group = year)) + # group = year to boxplot by year\n    geom_violin(draw_quantiles = 0.5) # violin plot showing the median\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `weight_lbs = as.numeric(weight_lbs)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: The `draw_quantiles` argument of `geom_violin()` is deprecated as of ggplot2\n4.0.0.\n‚Ñπ Please use the `quantiles.linetype` argument instead.\n\n\nWarning: Removed 9 rows containing non-finite outside the scale range\n(`stat_ydensity()`).",
    "crumbs": [
      "Recitation solutions",
      "Week 5 - ggplot101 solutions"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html",
    "title": "R Markdown for Reproducible Research",
    "section": "",
    "text": "Figure from Allison Horst",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#setting-future-you-up-for-success",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#setting-future-you-up-for-success",
    "title": "R Markdown for Reproducible Research",
    "section": "Setting future you up for success",
    "text": "Setting future you up for success\nHow often do you conduct some kind of data analysis, get some results, ignore the project for 6 months, then return back to your data and realize you can‚Äôt figure out exactly what you did?\nThis does not need to happen to you. Be kind to your future self and take steps to avoid this avoidable problem.\nJust like a lab notebook helps you document all of the steps you take in your wet lab work, R Markdown can function like a lab notebook for all your data analyses.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#what-is-r-markdown",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#what-is-r-markdown",
    "title": "R Markdown for Reproducible Research",
    "section": "What is R Markdown?",
    "text": "What is R Markdown?\nRMarkdown provides a framework for saving and executing code, and sharing your results. R Markdown files have the file format .Rmd.\nYou can do so many things in R Markdown, from making reports that include text, code, code annotations, figures, tables etc., to creating this course website!\nIf you‚Äôve never used R Markdown before, you can download it using the chunk below. Unlike other R packages, you don‚Äôt need to use library(rmarkdown) to load the package each time you want to use R Markdown.\n\ninstall.packages(\"rmarkdown\")",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#why-i-love-r-markdown",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#why-i-love-r-markdown",
    "title": "R Markdown for Reproducible Research",
    "section": "Why I love R Markdown",
    "text": "Why I love R Markdown\nBasically everything I do in R uses R Markdown. I really value to ability to easily add text and annotate code so that future me, my team, or collaborators can understand what I‚Äôve done and why. I try to write my code in such a way that it could be read by anyone, and is ready to be pushed to our lab‚Äôs Github repositories to act as supplementary materials for our publications. It helps others to be able to truly see what we‚Äôve done, and I think makes science more reproducible and open.\n\n\n\n\n\nFigure from Allison Horst",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#open-an-r-markdown-document",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#open-an-r-markdown-document",
    "title": "R Markdown for Reproducible Research",
    "section": "Open an R Markdown document",
    "text": "Open an R Markdown document\nOpen up RStudio, go to File &gt; New File &gt; R Markdown. Change the name of the title to something meaningful to you, mine will be called ‚ÄúTrying R Markdown‚Äù.\n\n\n\n\n\n\n\n\n\n\nSaving our file\nWe gave our file a title, but if you look at the top left corner of our new document, you‚Äôll see it‚Äôs called ‚ÄúUntitled1‚Äù. Let‚Äôs change the name to something easier for our future selves to recognize.\n\n\n\n\n\n\n\n\n\nYou can go to File &gt; Save as and place this new R Markdown with your other course materials, and save it with a meaningful name.\n\nAlways having issues with setting your working directory? R Markdown solves this problem! The default working directory is the location of the saved R Markdown file. Voila!\n\n\n\nAn example\nYou‚Äôll note when you create your template document, it is not blank. So you get a sense of what these documents will look like when they are ‚Äúrun,‚Äù let‚Äôs do that with the template doc.\nIn the taskbar of your R Markdown document you will see a button called Knit in your task bar (there is a little ball of yarn with knitting needles next to it). If you click it, R will run all of the code in your R Markdown file, and default compile it to a .html file (though you can select to compile to other file formats).\n\n\n\n\n\n\n\n\n\nLet‚Äôs compare what our document looks like when viewing it in RStudio (left), and after it is knitted (right).\n\n\n\n\n\n\n\n\n\nIf you have a little bit of R experience, you can begin to see how (some of) the content on the left related to the knitted document on the right. We see text, code chunks (but not all of them), and the output of code.\nNow that we have seen a template R Markdown and have 10,000 foot view as to what it is, we can start going through what the different pieces of the document are.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#yaml-header",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#yaml-header",
    "title": "R Markdown for Reproducible Research",
    "section": "YAML Header",
    "text": "YAML Header\nThe YAML (Yet Another Markdown Language, or YAML ain‚Äôt markup language, if you want to learn more about this name and its origins, you can read about it at this stack overflow post) is at the top of your document and is surrounded by ---.\nThe YAML is where you can set the content that will show up on the top of your knitted document.\n\ntitle: ‚ÄúYour title but put it in quotes‚Äù\nauthor: ‚ÄúThe author and still in quotes‚Äù\ndate: the date you want at the top of your doc in quotes. If you want this to be today‚Äôs date (whatever that is) you can use ‚ÄúSys.Date()‚Äù\noutput: will indicate the format of your compiled document. I would recommend for this class you use html_document as it is the richest format. Your output will be a .html file, which you can save or share.\n\nHere‚Äôs a simple example.\n\n---\ntitle: \"This is my descriptive title\"\nauthor: \"Jess\"\ndate: \"May 10, 2022\"\noutput: html_document\n---\n\nIn the YAML, you can also set options that govern how your document will be compiled within output. For example, you can add a table of contents, make that toc float, add a theme, number your sections, and add a button that allows someone to click and access your .Rmd from your knitted .html file. This last one is especially nice because it allows you to send one viewable document, and if someone wants to edit it, they can download and do so easily. This is how I will ask you to submit your class assignments.\nHere‚Äôs an example of what a more customized YAML could look like.\n\n---\ntitle: \"This is my descriptive title\"\nauthor: \"Jess\"\ndate: \"August 9, 2024\"\noutput: \n  html_document: # knit to a .html doc\n    toc: true # creates a table of contents\n    toc_float: true # has that TOC float so you can see it even when you scroll\n    number_sections: true # number your sections\n    theme: flatly # set a global theme\n    code_download: true # insert the code download button\n---\n\nBe sure you pay attention to the indents (which are 2 or 4 spaces, and not tabs), as the YAML is picky here. If your indents are not correct, you will get an error when you knit. Also, if you are missing a colon, your document will knit weirdly or not at all.\n\n\n\n\n\n\nPro tip to avoid a tab/space debacle\n\n\n\nYou can set in RStudio to insert spaces when you click tab by going to Preferences &gt; Code &gt; Use spaces for tab (and indicate 2).\n\n\nAbove are just some of the options that I like to put in my YAML, but there are tons more. Additional output options that are explained on the second page of the RStudio R Markdown cheatsheet.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#text",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#text",
    "title": "R Markdown for Reproducible Research",
    "section": "Text",
    "text": "Text\nUnlike an R script (.R), where R by default interprets anything as code (and material that isn‚Äôt code needed to be commented out by using #), in an R Markdown, the default is text (and code exists only within code chunks or backticks).\nThe text portion of the document is written in a language called Markdown (which is why this format is called R Markdown). The philosophy of Markdown is that it is easy to both write and read. If you want to learn more about markup languages I‚Äôd recommend the this brief explanation by Michael Broe from a past Code Club Session and the Markup language wikipedia page.\nIf we look back to our template R Markdown, we can see there is text written in the same way that we would write in Word document, or an email, and we recognize immediately as text (i.e., the sentence at line 24).\n\n\n\n\n\n\n\n\n\nBut we can also see markup that is perhaps not immediately, recognizable, for example, the **Knit** on line 16. In this case, two asterisks around a word will make it compile to be bolded (second paragraph in the right photo).\nBelow I‚Äôm compiling some commonly used markdown syntax.\n\n\n\n\n\nFigure from R Markdown Reference Guide\n\n\n\n\nNote, the headers are useful and will indicate the levels in your table of contents. You want to use them, and make them meaningful for your document.\nYou can use Markdown to insert tables, images, mathematical formulas, block quotes, and almost anything else you‚Äôd like. You can even write your papers and dissertation in R Markdown. The old version of this course website is made with distill and R Markdown. My lab website is made with R Markdown and the hugo Aper√≥ theme. Quarto and .qmd documents are a slightly updated version of RMarkdown with some new functionality (here you can find some discussion about the different between the two). Here are some links where you can find lots of other Markdown syntactical information:\n\nMarkdown Guide\nR Markdown reference guide\nR Studio R Markdown Cheatsheet\njust google for what you want\n\nSo how is this useful for this course and making your own data analyses more reproducible? You can embed text along with your code, where you provide introductory information, your rationale for data analysis decision making, links and more information about interpreting your code and its output, provide context as to your results, and anything else that would aid your data‚Äôs interpretation.\nAnd, when you want to make a small change, you can do so, knit, and everything else automatically updates.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module2/03_rmarkdown/03_Rmd.html#code",
    "href": "modules/module2/03_rmarkdown/03_Rmd.html#code",
    "title": "R Markdown for Reproducible Research",
    "section": "Code",
    "text": "Code\nCode chunks are the parts of your R Markdown document where code lives. You can insert a new code chunk by:\n\nusing the keyboard shortcut Cmd + Option + I (Mac) or Ctrl + Alt + I (Windows)\ntyping ```{r} and ``` (and your code goes in between)\nusing the Add Chunk command in the editor toolbar and select R\n\nCode chunks look like this:\n\n\n\n\n\n\n\n\n\nThe code goes in the empty line, and there can be more than 1 bit of code per chunk though I would say if you start having to scroll in your chunk its probably too long.\n\nthe gear allows you to modify the chunk options (we are going to talk more about this)\nthe triangle with the line below it runs all code chunks that come previous to this chunk\nthe play button runs the current chunk\n\nYou can still add comments within a code chunk, but you need to comment them out using #.\n# here is my in chunk annotation\nsome_function()\nWhen you knit your R Markdown, this process will run all of the code in your document. This means if you have code that throws errors or doesn‚Äôt work, your document will not knit. This is some of why I am asking you to knit for your final assignments - all your code needs to work!\nYou can also embed code inline (i.e., within your text).\n\n\n\n\n\n\n\nRaw\nRendered\n\n\n\n\nThere are `r 365*24` hours in a year\nThere are 8760 hours in a year\n\n\nThere are `r nrow(cars)` observations (i.e.¬†rows) in the cars dataset\nThere are 50 observations (i.e.¬†rows) in the cars dataset\n\n\n\nThink about how you could use this ‚Äì embed information from your data analysis (e.g, p-values) within your narrative text without having to hard-code/type it in manually.\n\nAdding options to your code chunks\nYou add options to your code chunks between the {}. This gives R additional instructions regarding running your code and compiling your document. Here are some common examples:\n\necho = FALSE runs your code chunk, displays output, but does not display code in your final doc (this is useful if you want to show a figure but not the code used to create it)\neval = FALSE does not run your code, but does display it in your final doc\ninclude = FALSE runs your code but does not display the code or its output in your final doc\nmessage = FALSE prevents messages from showing up in your final doc\nwarning = FALSE prevents earnings from showing up in your final doc\nfig.height = X and fig.width = Y will allow you to specify the dimensions of your figures (in inches)\nfig.align = can be set to ‚Äúleft‚Äù, ‚Äúright‚Äù, or ‚Äúcenter‚Äù\nfig.cap = \"Your figure caption\" will allow you to set a figure caption\nfig.alt = \"Your alt text\" will allow you to set alt text for screen readers\ncache = TRUE will cache results, meaning if you have a chunk that takes a long time to run, if you haven‚Äôt changed anything and you knit again, the code won‚Äôt run again but access the cache.",
    "crumbs": [
      "Module 2 - Coding fundamentals",
      "Week 3 - R Markdown"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#introductions",
    "href": "modules/module1/01_principles/01_principles.html#introductions",
    "title": "1 - Principles of Data Visualization",
    "section": "Introductions üëã",
    "text": "Introductions üëã\n\n\nName\nProgram\nWhy you decided to take this class\nOne thing you hope to learn",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#teaching-team",
    "href": "modules/module1/01_principles/01_principles.html#teaching-team",
    "title": "1 - Principles of Data Visualization",
    "section": "Teaching Team",
    "text": "Teaching Team\nInstructor: Jessica Cooperstone\n‚úâÔ∏è cooperstone.1@osu.edu\n\n\nTA: Daniel Quiroz Moreno\n‚úâÔ∏è quirozmoreno.1@osu.edu\n\n\nOffice hours: go.osu.edu/dataviz-times",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#website",
    "href": "modules/module1/01_principles/01_principles.html#website",
    "title": "1 - Principles of Data Visualization",
    "section": "Website",
    "text": "Website\nIf you have found these slides, you‚Äôve made it to the website! (Good job.)\n\n\nAll course materials will be posted to, or linked to from www.rdataviz.com",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#syllabus",
    "href": "modules/module1/01_principles/01_principles.html#syllabus",
    "title": "1 - Principles of Data Visualization",
    "section": "Syllabus",
    "text": "Syllabus\n\nA full version of the syllabus can be found on Carmen\nA trimmed version of the syllabus can be found on our course site",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#attendance",
    "href": "modules/module1/01_principles/01_principles.html#attendance",
    "title": "1 - Principles of Data Visualization",
    "section": "Attendance",
    "text": "Attendance\n\n\n\nClass will taught in a hybrid, synchronous manner, meaning I expect you to attend class during class time. This attendance can happen in person, or virtually via Zoom I have found that students who attend in person are more engaged, and tend to master material more quickly. But, it is up to you how you want to attend.\n\nI will record class time for those who want to 1) revisit material or 2) can‚Äôt attend (this should be uncommon). These recordings are not to replace coming to class.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#how-class-will-be",
    "href": "modules/module1/01_principles/01_principles.html#how-class-will-be",
    "title": "1 - Principles of Data Visualization",
    "section": "How class will be?",
    "text": "How class will be?\n\nA combination of lecture, code run-throughs, live coding, and hands-on exercises.\nBring a laptop (not tablet) to class with R and RStudio downloaded (instructions)\nCome with your questions!\nEngage as much as you can!",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#how-can-you-get-help",
    "href": "modules/module1/01_principles/01_principles.html#how-can-you-get-help",
    "title": "1 - Principles of Data Visualization",
    "section": "How can you get help?",
    "text": "How can you get help?\nWe are here to help you and will match the enthusiasm that you put into this course. We expect that you will run into issues for which you would like help. You can ask questions:\n\nDuring class time (both lecture and recitation)\nDuring office hours\nBefore or after class\nBy email (least preferred) - be sure to provide relevant information so we can help you!",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#previous-programming-experience",
    "href": "modules/module1/01_principles/01_principles.html#previous-programming-experience",
    "title": "1 - Principles of Data Visualization",
    "section": "Previous programming experience",
    "text": "Previous programming experience\nYou do not need to be an R expert for this class, but I will assume working-level knowledge of R programming. If you have no experience with R, but would still like to take this class, you can. I ask then you get yourself up to speed by taking this free online class https://www.edx.org/course/data-science-r-basics (audit only) before the start of the 3rd week of class.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#assigments",
    "href": "modules/module1/01_principles/01_principles.html#assigments",
    "title": "1 - Principles of Data Visualization",
    "section": "Assigments",
    "text": "Assigments\n\nModule assignments: After each module, there will be an assignment to provide practice for the techniques learned in class.\nClass reflections: After 10 of the 15 weeks, you will write a 1 paragraph reflection on the material that was presented in class. This can include your thoughts on how you will use these lessons in your own research and data visualizations, ways in which you have investigated this topic (or expect to) on your own, or what else you‚Äôd like to learn in this area. The purpose of this assignment is not to be burdensome, but to keep you engaged in the course material, and providing feedback to me on what parts you‚Äôve found useful, what you‚Äôve struggled with, and what you‚Äôd like to see more of in the future.\nRecitation submissions: I ask you submit 8 of 11 recitations to Carmen to show you have made a good faith effort to engage with the course material. I will mark these at 0 or 1 points, with 1 point given for completion of at least 70% of the assignment.\nCapstone assignment: At the end of the semester, you will complete a capstone assignment where you create a series of visualizations based on your research data, data coming from your lab, or other data that is publicly available. I expect this assignment to be completed in R Markdown, annotated, and knitted into an easy-to-read .html file. I also expect your code to be fully commented such that I can understand what you are doing with each step, and why.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#late-assignments",
    "href": "modules/module1/01_principles/01_principles.html#late-assignments",
    "title": "1 - Principles of Data Visualization",
    "section": "Late assignments",
    "text": "Late assignments\n\nI expect you will turn assignments in on time. Late assignments are not accepted. If there are extenuating circumstances that prevent you from turning in an assignment on time, please connect with me as soon as possible after such a situation arises for discussion about a possible deadline extension.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule",
    "href": "modules/module1/01_principles/01_principles.html#schedule",
    "title": "1 - Principles of Data Visualization",
    "section": "üóì Schedule",
    "text": "üóì Schedule\nThis is our tentative class schedule - but subject to change depending on our pacing, and your interests!",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-1",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-1",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 1)",
    "text": "üóìÔ∏è Schedule (part 1)\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-08-26\n1: Principles\nPrinciples of data visualization\n\n\n2025-09-02\n1: Principles\nGood and bad visualizations",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-2",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-2",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 2)",
    "text": "üóìÔ∏è Schedule (part 2)\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-09-09\n2: Coding fundamentals\nR Markdown for reproducible research\n\n\n2025-09-16\n2: Coding fundamentals\nWrangling, the basics\n\n\n2025-09-23\n2: Coding fundamentals\nggplot 101\n\n\n2025-09-30\n2: Coding fundamentals\nThemes, labels, facets (ggplot 102)",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-3",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-3",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 3)",
    "text": "üóìÔ∏è Schedule (part 3)\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-10-07\n3: Data exploration\nData distributions\n\n\n2025-10-14\n3: Data exploration\nCorrelations\n\n\n2025-10-28\n3: Data exploration\nAnnotating statistics",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#schedule-part-4",
    "href": "modules/module1/01_principles/01_principles.html#schedule-part-4",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Schedule (part 4)",
    "text": "üóìÔ∏è Schedule (part 4)\nNovember 11 is Veterans Day (no class)  November 25 will be asynchronous\n\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-11-04\n4: Putting it together\nPrincipal components analysis\n\n\n2025-11-18\n4: Putting it together\nInteractive plots\n\n\n2025-11-25\n4: Putting it together\nManhattan plots and making lots of plots at once (asynchronous)\n\n\n2025-12-02\n4: Putting it together\nggplot extension packages",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#capstone-prep",
    "href": "modules/module1/01_principles/01_principles.html#capstone-prep",
    "title": "1 - Principles of Data Visualization",
    "section": "üóìÔ∏è Capstone prep",
    "text": "üóìÔ∏è Capstone prep\n\n\n\n\n\nDate\nModule\nTopic\n\n\n\n\n2025-10-21\nCapstone prep\nCapstone plan prep, open session\n\n\n2025-12-09\nCapstone prep\nCapstone assignment, open session",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#there-may-be-a-data-dinosaur",
    "href": "modules/module1/01_principles/01_principles.html#there-may-be-a-data-dinosaur",
    "title": "1 - Principles of Data Visualization",
    "section": "There may be a data dinosaur ü¶ñ",
    "text": "There may be a data dinosaur ü¶ñ\n\nFigure by Alberto Cairo",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-understand-distribution",
    "href": "modules/module1/01_principles/01_principles.html#to-understand-distribution",
    "title": "1 - Principles of Data Visualization",
    "section": "To understand distribution",
    "text": "To understand distribution\nAnscombe‚Äôs quartet üéª",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-discover-data-secrets",
    "href": "modules/module1/01_principles/01_principles.html#to-discover-data-secrets",
    "title": "1 - Principles of Data Visualization",
    "section": "To discover data secrets",
    "text": "To discover data secrets\n\nFigures from Justin Matejka and George Fitzmaurice",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#to-convey-our-message",
    "href": "modules/module1/01_principles/01_principles.html#to-convey-our-message",
    "title": "1 - Principles of Data Visualization",
    "section": "To convey our message",
    "text": "To convey our message\n\nBilbrey et al., New Phytologist, 2021",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#the-data-visualization-process",
    "href": "modules/module1/01_principles/01_principles.html#the-data-visualization-process",
    "title": "1 - Principles of Data Visualization",
    "section": "The data visualization process",
    "text": "The data visualization process\n\n\nFigure adapted from one by Rick Scavetta",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability",
    "href": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability",
    "title": "1 - Principles of Data Visualization",
    "section": "Simple changes improve interpretability",
    "text": "Simple changes improve interpretability",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability-1",
    "href": "modules/module1/01_principles/01_principles.html#simple-changes-improve-interpretability-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Simple changes improve interpretability",
    "text": "Simple changes improve interpretability",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues",
    "href": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues",
    "title": "1 - Principles of Data Visualization",
    "section": "Encoding data with easy-to-process visual clues",
    "text": "Encoding data with easy-to-process visual clues\nLength is easier to see than angles or areas.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues-1",
    "href": "modules/module1/01_principles/01_principles.html#encoding-data-with-easy-to-process-visual-clues-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Encoding data with easy-to-process visual clues",
    "text": "Encoding data with easy-to-process visual clues\nLength is easier to see than angles or areas.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#color-scales-should-be-intuitive-and-accessible",
    "href": "modules/module1/01_principles/01_principles.html#color-scales-should-be-intuitive-and-accessible",
    "title": "1 - Principles of Data Visualization",
    "section": "Color scales should be intuitive and accessible",
    "text": "Color scales should be intuitive and accessible\n\n\nThese are not.",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-1",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-2",
    "href": "modules/module1/01_principles/01_principles.html#show-your-data-if-you-can-2",
    "title": "1 - Principles of Data Visualization",
    "section": "Show your data if you can",
    "text": "Show your data if you can\n#barbarplots",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-1",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-2",
    "href": "modules/module1/01_principles/01_principles.html#cut-your-axes-with-care-2",
    "title": "1 - Principles of Data Visualization",
    "section": "Cut your axes with care",
    "text": "Cut your axes with care",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti",
    "href": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti",
    "title": "1 - Principles of Data Visualization",
    "section": "Avoid figure spaghetti üçù",
    "text": "Avoid figure spaghetti üçù",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti-1",
    "href": "modules/module1/01_principles/01_principles.html#avoid-figure-spaghetti-1",
    "title": "1 - Principles of Data Visualization",
    "section": "Avoid figure spaghetti üçù",
    "text": "Avoid figure spaghetti üçù",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#be-consistent-among-figures",
    "href": "modules/module1/01_principles/01_principles.html#be-consistent-among-figures",
    "title": "1 - Principles of Data Visualization",
    "section": "Be consistent among figures",
    "text": "Be consistent among figures\n\nUse the same color schemes/shapes across figures\nIf you‚Äôre ordering/grouping, do so in the same manner",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#make-sure-your-plot-has-a-clear-message",
    "href": "modules/module1/01_principles/01_principles.html#make-sure-your-plot-has-a-clear-message",
    "title": "1 - Principles of Data Visualization",
    "section": "Make sure your plot has a clear message üçï",
    "text": "Make sure your plot has a clear message üçï",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#marie-kondo-your-plots",
    "href": "modules/module1/01_principles/01_principles.html#marie-kondo-your-plots",
    "title": "1 - Principles of Data Visualization",
    "section": "Marie Kondo your plots",
    "text": "Marie Kondo your plots\nDeclutter, and keep only parts that are informative (and spark joy) üòª\n\nFrom https://socviz.co/lookatdata.html",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#oral-presentation-and-publication-figures-might-not-be-the-same",
    "href": "modules/module1/01_principles/01_principles.html#oral-presentation-and-publication-figures-might-not-be-the-same",
    "title": "1 - Principles of Data Visualization",
    "section": "Oral presentation and publication figures might not be the same",
    "text": "Oral presentation and publication figures might not be the same",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#what-should-you-think-about-when-making-visualizations",
    "href": "modules/module1/01_principles/01_principles.html#what-should-you-think-about-when-making-visualizations",
    "title": "1 - Principles of Data Visualization",
    "section": "What should you think about when making visualizations?",
    "text": "What should you think about when making visualizations?\n\nWho are you talking to? üì¢\nWhat are you trying to convey? üìù\nHow can you fairly represent your data? üöØ",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  },
  {
    "objectID": "modules/module1/01_principles/01_principles.html#next-class",
    "href": "modules/module1/01_principles/01_principles.html#next-class",
    "title": "1 - Principles of Data Visualization",
    "section": "Next class",
    "text": "Next class\nSubmit (through Carmen) by Monday 9/1/2025 at 11:59pm:\n\n1 good visualization (and a paragraph on why its good)\n1 bad visualization (and a paragraph on why its bad)\n\n\nWe will go through these next week. Daniel will pick the best good and the best bad visualizations and there will be prizes! üéâ",
    "crumbs": [
      "Module 1 - Principles",
      "Week 1 - Principles"
    ]
  }
]